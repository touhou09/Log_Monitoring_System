2025-04-24 15:05:54.422 | 25/04/24 06:05:54 INFO Utils: Successfully started service 'sparkDriver' on port 40627.
2025-04-24 15:05:54.440 | 25/04/24 06:05:54 INFO SparkEnv: Registering MapOutputTracker
2025-04-24 15:05:54.472 | 25/04/24 06:05:54 INFO SparkEnv: Registering BlockManagerMaster
2025-04-24 15:05:54.480 | 25/04/24 06:05:54 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2025-04-24 15:05:54.481 | 25/04/24 06:05:54 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
2025-04-24 15:05:54.483 | 25/04/24 06:05:54 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
2025-04-24 15:05:54.497 | 25/04/24 06:05:54 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-401cb546-d515-4aa4-becf-211221693ec6
2025-04-24 15:05:54.507 | 25/04/24 06:05:54 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB
2025-04-24 15:05:54.518 | 25/04/24 06:05:54 INFO SparkEnv: Registering OutputCommitCoordinator
2025-04-24 15:05:54.589 | 25/04/24 06:05:54 INFO JettyUtils: Start Jetty 0.0.0.0:4040 for SparkUI
2025-04-24 15:05:54.623 | 25/04/24 06:05:54 INFO Utils: Successfully started service 'SparkUI' on port 4040.
2025-04-24 15:05:54.647 | 25/04/24 06:05:54 INFO SparkContext: Added JAR file:///opt/bitnami/spark/.ivy2/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.0.jar at spark://2904d747298a:40627/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.0.jar with timestamp 1745474754210
2025-04-24 15:05:54.647 | 25/04/24 06:05:54 INFO SparkContext: Added JAR file:///opt/bitnami/spark/.ivy2/jars/com.clickhouse_clickhouse-jdbc-0.4.6.jar at spark://2904d747298a:40627/jars/com.clickhouse_clickhouse-jdbc-0.4.6.jar with timestamp 1745474754210
2025-04-24 15:05:54.647 | 25/04/24 06:05:54 INFO SparkContext: Added JAR file:///opt/bitnami/spark/.ivy2/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.0.jar at spark://2904d747298a:40627/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.0.jar with timestamp 1745474754210
2025-04-24 15:05:54.648 | 25/04/24 06:05:54 INFO SparkContext: Added JAR file:///opt/bitnami/spark/.ivy2/jars/org.apache.kafka_kafka-clients-3.4.1.jar at spark://2904d747298a:40627/jars/org.apache.kafka_kafka-clients-3.4.1.jar with timestamp 1745474754210
2025-04-24 15:05:54.648 | 25/04/24 06:05:54 INFO SparkContext: Added JAR file:///opt/bitnami/spark/.ivy2/jars/com.google.code.findbugs_jsr305-3.0.0.jar at spark://2904d747298a:40627/jars/com.google.code.findbugs_jsr305-3.0.0.jar with timestamp 1745474754210
2025-04-24 15:05:54.648 | 25/04/24 06:05:54 INFO SparkContext: Added JAR file:///opt/bitnami/spark/.ivy2/jars/org.apache.commons_commons-pool2-2.11.1.jar at spark://2904d747298a:40627/jars/org.apache.commons_commons-pool2-2.11.1.jar with timestamp 1745474754210
2025-04-24 15:05:54.648 | 25/04/24 06:05:54 INFO SparkContext: Added JAR file:///opt/bitnami/spark/.ivy2/jars/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar at spark://2904d747298a:40627/jars/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar with timestamp 1745474754210
2025-04-24 15:05:54.648 | 25/04/24 06:05:54 INFO SparkContext: Added JAR file:///opt/bitnami/spark/.ivy2/jars/org.lz4_lz4-java-1.8.0.jar at spark://2904d747298a:40627/jars/org.lz4_lz4-java-1.8.0.jar with timestamp 1745474754210
2025-04-24 15:05:54.648 | 25/04/24 06:05:54 INFO SparkContext: Added JAR file:///opt/bitnami/spark/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.10.3.jar at spark://2904d747298a:40627/jars/org.xerial.snappy_snappy-java-1.1.10.3.jar with timestamp 1745474754210
2025-04-24 15:05:54.648 | 25/04/24 06:05:54 INFO SparkContext: Added JAR file:///opt/bitnami/spark/.ivy2/jars/org.slf4j_slf4j-api-2.0.7.jar at spark://2904d747298a:40627/jars/org.slf4j_slf4j-api-2.0.7.jar with timestamp 1745474754210
2025-04-24 15:05:54.649 | 25/04/24 06:05:54 INFO SparkContext: Added JAR file:///opt/bitnami/spark/.ivy2/jars/org.apache.hadoop_hadoop-client-api-3.3.4.jar at spark://2904d747298a:40627/jars/org.apache.hadoop_hadoop-client-api-3.3.4.jar with timestamp 1745474754210
2025-04-24 15:05:54.649 | 25/04/24 06:05:54 INFO SparkContext: Added JAR file:///opt/bitnami/spark/.ivy2/jars/commons-logging_commons-logging-1.1.3.jar at spark://2904d747298a:40627/jars/commons-logging_commons-logging-1.1.3.jar with timestamp 1745474754210
2025-04-24 15:05:54.651 | 25/04/24 06:05:54 INFO SparkContext: Added file file:///opt/bitnami/spark/.ivy2/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.0.jar at spark://2904d747298a:40627/files/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.0.jar with timestamp 1745474754210
2025-04-24 15:05:54.652 | 25/04/24 06:05:54 INFO Utils: Copying /opt/bitnami/spark/.ivy2/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.0.jar to /tmp/spark-6d03b30f-0726-4720-ba3e-bc44a6c0a10e/userFiles-9b48d15a-bb46-4ce6-a032-0ba043cb9948/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.0.jar
2025-04-24 15:05:54.657 | 25/04/24 06:05:54 INFO SparkContext: Added file file:///opt/bitnami/spark/.ivy2/jars/com.clickhouse_clickhouse-jdbc-0.4.6.jar at spark://2904d747298a:40627/files/com.clickhouse_clickhouse-jdbc-0.4.6.jar with timestamp 1745474754210
2025-04-24 15:05:54.657 | 25/04/24 06:05:54 INFO Utils: Copying /opt/bitnami/spark/.ivy2/jars/com.clickhouse_clickhouse-jdbc-0.4.6.jar to /tmp/spark-6d03b30f-0726-4720-ba3e-bc44a6c0a10e/userFiles-9b48d15a-bb46-4ce6-a032-0ba043cb9948/com.clickhouse_clickhouse-jdbc-0.4.6.jar
2025-04-24 15:05:54.660 | 25/04/24 06:05:54 INFO SparkContext: Added file file:///opt/bitnami/spark/.ivy2/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.0.jar at spark://2904d747298a:40627/files/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.0.jar with timestamp 1745474754210
2025-04-24 15:05:54.660 | 25/04/24 06:05:54 INFO Utils: Copying /opt/bitnami/spark/.ivy2/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.0.jar to /tmp/spark-6d03b30f-0726-4720-ba3e-bc44a6c0a10e/userFiles-9b48d15a-bb46-4ce6-a032-0ba043cb9948/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.0.jar
2025-04-24 15:05:54.662 | 25/04/24 06:05:54 INFO SparkContext: Added file file:///opt/bitnami/spark/.ivy2/jars/org.apache.kafka_kafka-clients-3.4.1.jar at spark://2904d747298a:40627/files/org.apache.kafka_kafka-clients-3.4.1.jar with timestamp 1745474754210
2025-04-24 15:05:54.662 | 25/04/24 06:05:54 INFO Utils: Copying /opt/bitnami/spark/.ivy2/jars/org.apache.kafka_kafka-clients-3.4.1.jar to /tmp/spark-6d03b30f-0726-4720-ba3e-bc44a6c0a10e/userFiles-9b48d15a-bb46-4ce6-a032-0ba043cb9948/org.apache.kafka_kafka-clients-3.4.1.jar
2025-04-24 15:05:54.666 | 25/04/24 06:05:54 INFO SparkContext: Added file file:///opt/bitnami/spark/.ivy2/jars/com.google.code.findbugs_jsr305-3.0.0.jar at spark://2904d747298a:40627/files/com.google.code.findbugs_jsr305-3.0.0.jar with timestamp 1745474754210
2025-04-24 15:05:54.666 | 25/04/24 06:05:54 INFO Utils: Copying /opt/bitnami/spark/.ivy2/jars/com.google.code.findbugs_jsr305-3.0.0.jar to /tmp/spark-6d03b30f-0726-4720-ba3e-bc44a6c0a10e/userFiles-9b48d15a-bb46-4ce6-a032-0ba043cb9948/com.google.code.findbugs_jsr305-3.0.0.jar
2025-04-24 15:05:54.668 | 25/04/24 06:05:54 INFO SparkContext: Added file file:///opt/bitnami/spark/.ivy2/jars/org.apache.commons_commons-pool2-2.11.1.jar at spark://2904d747298a:40627/files/org.apache.commons_commons-pool2-2.11.1.jar with timestamp 1745474754210
2025-04-24 15:05:54.668 | 25/04/24 06:05:54 INFO Utils: Copying /opt/bitnami/spark/.ivy2/jars/org.apache.commons_commons-pool2-2.11.1.jar to /tmp/spark-6d03b30f-0726-4720-ba3e-bc44a6c0a10e/userFiles-9b48d15a-bb46-4ce6-a032-0ba043cb9948/org.apache.commons_commons-pool2-2.11.1.jar
2025-04-24 15:05:54.670 | 25/04/24 06:05:54 INFO SparkContext: Added file file:///opt/bitnami/spark/.ivy2/jars/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar at spark://2904d747298a:40627/files/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar with timestamp 1745474754210
2025-04-24 15:05:54.670 | 25/04/24 06:05:54 INFO Utils: Copying /opt/bitnami/spark/.ivy2/jars/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar to /tmp/spark-6d03b30f-0726-4720-ba3e-bc44a6c0a10e/userFiles-9b48d15a-bb46-4ce6-a032-0ba043cb9948/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar
2025-04-24 15:05:54.683 | 25/04/24 06:05:54 INFO SparkContext: Added file file:///opt/bitnami/spark/.ivy2/jars/org.lz4_lz4-java-1.8.0.jar at spark://2904d747298a:40627/files/org.lz4_lz4-java-1.8.0.jar with timestamp 1745474754210
2025-04-24 15:05:54.683 | 25/04/24 06:05:54 INFO Utils: Copying /opt/bitnami/spark/.ivy2/jars/org.lz4_lz4-java-1.8.0.jar to /tmp/spark-6d03b30f-0726-4720-ba3e-bc44a6c0a10e/userFiles-9b48d15a-bb46-4ce6-a032-0ba043cb9948/org.lz4_lz4-java-1.8.0.jar
2025-04-24 15:05:54.685 | 25/04/24 06:05:54 INFO SparkContext: Added file file:///opt/bitnami/spark/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.10.3.jar at spark://2904d747298a:40627/files/org.xerial.snappy_snappy-java-1.1.10.3.jar with timestamp 1745474754210
2025-04-24 15:05:54.685 | 25/04/24 06:05:54 INFO Utils: Copying /opt/bitnami/spark/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.10.3.jar to /tmp/spark-6d03b30f-0726-4720-ba3e-bc44a6c0a10e/userFiles-9b48d15a-bb46-4ce6-a032-0ba043cb9948/org.xerial.snappy_snappy-java-1.1.10.3.jar
2025-04-24 15:05:54.688 | 25/04/24 06:05:54 INFO SparkContext: Added file file:///opt/bitnami/spark/.ivy2/jars/org.slf4j_slf4j-api-2.0.7.jar at spark://2904d747298a:40627/files/org.slf4j_slf4j-api-2.0.7.jar with timestamp 1745474754210
2025-04-24 15:05:54.688 | 25/04/24 06:05:54 INFO Utils: Copying /opt/bitnami/spark/.ivy2/jars/org.slf4j_slf4j-api-2.0.7.jar to /tmp/spark-6d03b30f-0726-4720-ba3e-bc44a6c0a10e/userFiles-9b48d15a-bb46-4ce6-a032-0ba043cb9948/org.slf4j_slf4j-api-2.0.7.jar
2025-04-24 15:05:54.690 | 25/04/24 06:05:54 INFO SparkContext: Added file file:///opt/bitnami/spark/.ivy2/jars/org.apache.hadoop_hadoop-client-api-3.3.4.jar at spark://2904d747298a:40627/files/org.apache.hadoop_hadoop-client-api-3.3.4.jar with timestamp 1745474754210
2025-04-24 15:05:54.690 | 25/04/24 06:05:54 INFO Utils: Copying /opt/bitnami/spark/.ivy2/jars/org.apache.hadoop_hadoop-client-api-3.3.4.jar to /tmp/spark-6d03b30f-0726-4720-ba3e-bc44a6c0a10e/userFiles-9b48d15a-bb46-4ce6-a032-0ba043cb9948/org.apache.hadoop_hadoop-client-api-3.3.4.jar
2025-04-24 15:05:54.699 | 25/04/24 06:05:54 INFO SparkContext: Added file file:///opt/bitnami/spark/.ivy2/jars/commons-logging_commons-logging-1.1.3.jar at spark://2904d747298a:40627/files/commons-logging_commons-logging-1.1.3.jar with timestamp 1745474754210
2025-04-24 15:05:54.699 | 25/04/24 06:05:54 INFO Utils: Copying /opt/bitnami/spark/.ivy2/jars/commons-logging_commons-logging-1.1.3.jar to /tmp/spark-6d03b30f-0726-4720-ba3e-bc44a6c0a10e/userFiles-9b48d15a-bb46-4ce6-a032-0ba043cb9948/commons-logging_commons-logging-1.1.3.jar
2025-04-24 15:05:54.751 | 25/04/24 06:05:54 INFO StandaloneAppClient$ClientEndpoint: Connecting to master spark://spark-master:7077...
2025-04-24 15:05:54.776 | 25/04/24 06:05:54 INFO TransportClientFactory: Successfully created connection to spark-master/172.18.0.5:7077 after 14 ms (0 ms spent in bootstraps)
2025-04-24 15:05:54.831 | 25/04/24 06:05:54 INFO StandaloneSchedulerBackend: Connected to Spark cluster with app ID app-20250424060554-0001
2025-04-24 15:05:54.832 | 25/04/24 06:05:54 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20250424060554-0001/0 on worker-20250424060503-172.18.0.9-36847 (172.18.0.9:36847) with 16 core(s)
2025-04-24 15:05:54.834 | 25/04/24 06:05:54 INFO StandaloneSchedulerBackend: Granted executor ID app-20250424060554-0001/0 on hostPort 172.18.0.9:36847 with 16 core(s), 1024.0 MiB RAM
2025-04-24 15:05:54.838 | 25/04/24 06:05:54 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 35373.
2025-04-24 15:05:54.838 | 25/04/24 06:05:54 INFO NettyBlockTransferService: Server created on 2904d747298a:35373
2025-04-24 15:05:54.839 | 25/04/24 06:05:54 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2025-04-24 15:05:54.843 | 25/04/24 06:05:54 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 2904d747298a, 35373, None)
2025-04-24 15:05:54.845 | 25/04/24 06:05:54 INFO BlockManagerMasterEndpoint: Registering block manager 2904d747298a:35373 with 434.4 MiB RAM, BlockManagerId(driver, 2904d747298a, 35373, None)
2025-04-24 15:05:54.847 | 25/04/24 06:05:54 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 2904d747298a, 35373, None)
2025-04-24 15:05:54.849 | 25/04/24 06:05:54 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 2904d747298a, 35373, None)
2025-04-24 15:05:54.862 | 25/04/24 06:05:54 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20250424060554-0001/0 is now RUNNING
2025-04-24 15:05:54.969 | 25/04/24 06:05:54 INFO StandaloneSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0
2025-04-24 15:05:55.101 | 25/04/24 06:05:55 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
2025-04-24 15:05:55.102 | 25/04/24 06:05:55 INFO SharedState: Warehouse path is 'file:/app/spark-warehouse'.
2025-04-24 15:05:56.285 | 25/04/24 06:05:56 INFO StandaloneSchedulerBackend$StandaloneDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.18.0.9:46074) with ID 0,  ResourceProfileId 0
2025-04-24 15:05:56.338 | 25/04/24 06:05:56 INFO BlockManagerMasterEndpoint: Registering block manager 172.18.0.9:33879 with 434.4 MiB RAM, BlockManagerId(0, 172.18.0.9, 33879, None)
2025-04-24 15:05:56.927 | 25/04/24 06:05:56 INFO StateStoreCoordinatorRef: Registered StateStoreCoordinator endpoint
2025-04-24 15:05:56.944 | 25/04/24 06:05:56 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-2cacd18c-322e-4cfa-8e0d-c1008c859651. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.
2025-04-24 15:05:56.956 | 25/04/24 06:05:56 INFO ResolveWriteToStream: Checkpoint root file:///tmp/temporary-2cacd18c-322e-4cfa-8e0d-c1008c859651 resolved to file:/tmp/temporary-2cacd18c-322e-4cfa-8e0d-c1008c859651.
2025-04-24 15:05:56.956 | 25/04/24 06:05:56 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.
2025-04-24 15:05:56.989 | 25/04/24 06:05:56 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-2cacd18c-322e-4cfa-8e0d-c1008c859651/metadata using temp file file:/tmp/temporary-2cacd18c-322e-4cfa-8e0d-c1008c859651/.metadata.5ca9673f-2dc8-475b-8a53-aa7a99a23939.tmp
2025-04-24 15:05:57.027 | 25/04/24 06:05:57 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-2cacd18c-322e-4cfa-8e0d-c1008c859651/.metadata.5ca9673f-2dc8-475b-8a53-aa7a99a23939.tmp to file:/tmp/temporary-2cacd18c-322e-4cfa-8e0d-c1008c859651/metadata
2025-04-24 15:05:57.050 | 25/04/24 06:05:57 INFO MicroBatchExecution: Starting [id = dd378de2-a37c-4818-995d-91e5aec4c185, runId = 57157b6a-9d62-409d-a827-3ef872726f83]. Use file:/tmp/temporary-2cacd18c-322e-4cfa-8e0d-c1008c859651 to store the query checkpoint.
2025-04-24 15:05:57.055 | 25/04/24 06:05:57 INFO MicroBatchExecution: Reading table [org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@4d193687] from DataSourceV2 named 'kafka' [org.apache.spark.sql.kafka010.KafkaSourceProvider@203330cb]
2025-04-24 15:05:57.058 | 25/04/24 06:05:57 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-33d80dac-a68a-4d9e-9955-2288fc90154c. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.
2025-04-24 15:05:57.059 | 25/04/24 06:05:57 INFO ResolveWriteToStream: Checkpoint root file:///tmp/temporary-33d80dac-a68a-4d9e-9955-2288fc90154c resolved to file:/tmp/temporary-33d80dac-a68a-4d9e-9955-2288fc90154c.
2025-04-24 15:05:57.059 | 25/04/24 06:05:57 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.
2025-04-24 15:05:57.066 | 25/04/24 06:05:57 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-33d80dac-a68a-4d9e-9955-2288fc90154c/metadata using temp file file:/tmp/temporary-33d80dac-a68a-4d9e-9955-2288fc90154c/.metadata.c762d17a-4243-467c-a0af-f4bd16e3cf54.tmp
2025-04-24 15:05:57.076 | 25/04/24 06:05:57 INFO OffsetSeqLog: BatchIds found from listing: 
2025-04-24 15:05:57.077 | 25/04/24 06:05:57 INFO OffsetSeqLog: BatchIds found from listing: 
2025-04-24 15:05:57.077 | 25/04/24 06:05:57 INFO MicroBatchExecution: Starting new streaming query.
2025-04-24 15:05:57.081 | 25/04/24 06:05:57 INFO MicroBatchExecution: Stream started from {}
2025-04-24 15:05:57.081 | 25/04/24 06:05:57 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-33d80dac-a68a-4d9e-9955-2288fc90154c/.metadata.c762d17a-4243-467c-a0af-f4bd16e3cf54.tmp to file:/tmp/temporary-33d80dac-a68a-4d9e-9955-2288fc90154c/metadata
2025-04-24 15:05:57.091 | 25/04/24 06:05:57 INFO MicroBatchExecution: Starting [id = aff125db-ad22-45d3-8d65-cae02357b9c3, runId = 46804a04-de66-4b1e-b791-2fd2cee06947]. Use file:/tmp/temporary-33d80dac-a68a-4d9e-9955-2288fc90154c to store the query checkpoint.
2025-04-24 15:05:57.092 | 25/04/24 06:05:57 INFO MicroBatchExecution: Reading table [org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@4d193687] from DataSourceV2 named 'kafka' [org.apache.spark.sql.kafka010.KafkaSourceProvider@203330cb]
2025-04-24 15:05:57.094 | 25/04/24 06:05:57 INFO OffsetSeqLog: BatchIds found from listing: 
2025-04-24 15:05:57.094 | 25/04/24 06:05:57 INFO OffsetSeqLog: BatchIds found from listing: 
2025-04-24 15:05:57.094 | 25/04/24 06:05:57 INFO MicroBatchExecution: Starting new streaming query.
2025-04-24 15:05:57.094 | 25/04/24 06:05:57 INFO MicroBatchExecution: Stream started from {}
2025-04-24 15:05:57.099 | 25/04/24 06:05:57 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-c78571f4-8fa3-463d-a680-ad63781afb6f. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.
2025-04-24 15:05:57.099 | 25/04/24 06:05:57 INFO ResolveWriteToStream: Checkpoint root file:///tmp/temporary-c78571f4-8fa3-463d-a680-ad63781afb6f resolved to file:/tmp/temporary-c78571f4-8fa3-463d-a680-ad63781afb6f.
2025-04-24 15:05:57.099 | 25/04/24 06:05:57 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.
2025-04-24 15:05:57.106 | 25/04/24 06:05:57 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-c78571f4-8fa3-463d-a680-ad63781afb6f/metadata using temp file file:/tmp/temporary-c78571f4-8fa3-463d-a680-ad63781afb6f/.metadata.53b6cf59-ef20-4645-9d82-da9ee0190f5f.tmp
2025-04-24 15:05:57.121 | 25/04/24 06:05:57 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-c78571f4-8fa3-463d-a680-ad63781afb6f/.metadata.53b6cf59-ef20-4645-9d82-da9ee0190f5f.tmp to file:/tmp/temporary-c78571f4-8fa3-463d-a680-ad63781afb6f/metadata
2025-04-24 15:05:57.132 | 25/04/24 06:05:57 INFO MicroBatchExecution: Starting [id = 6f1a8887-49c1-41df-b38c-99a40b9eb6fa, runId = b22d2d34-fa2f-4234-94db-afe323ff5e0d]. Use file:/tmp/temporary-c78571f4-8fa3-463d-a680-ad63781afb6f to store the query checkpoint.
2025-04-24 15:05:57.134 | 25/04/24 06:05:57 INFO MicroBatchExecution: Reading table [org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@4d193687] from DataSourceV2 named 'kafka' [org.apache.spark.sql.kafka010.KafkaSourceProvider@203330cb]
2025-04-24 15:05:57.136 | 25/04/24 06:05:57 INFO OffsetSeqLog: BatchIds found from listing: 
2025-04-24 15:05:57.137 | 25/04/24 06:05:57 INFO OffsetSeqLog: BatchIds found from listing: 
2025-04-24 15:05:57.137 | 25/04/24 06:05:57 INFO MicroBatchExecution: Starting new streaming query.
2025-04-24 15:05:57.137 | 25/04/24 06:05:57 INFO MicroBatchExecution: Stream started from {}
2025-04-24 15:05:57.139 | 25/04/24 06:05:57 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-ef85e29e-f0bd-4d21-8c8c-96b8720dae2d. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.
2025-04-24 15:05:57.140 | 25/04/24 06:05:57 INFO ResolveWriteToStream: Checkpoint root file:///tmp/temporary-ef85e29e-f0bd-4d21-8c8c-96b8720dae2d resolved to file:/tmp/temporary-ef85e29e-f0bd-4d21-8c8c-96b8720dae2d.
2025-04-24 15:05:57.140 | 25/04/24 06:05:57 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.
2025-04-24 15:05:57.148 | 25/04/24 06:05:57 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-ef85e29e-f0bd-4d21-8c8c-96b8720dae2d/metadata using temp file file:/tmp/temporary-ef85e29e-f0bd-4d21-8c8c-96b8720dae2d/.metadata.4336e4c4-c865-49d7-9b06-fb803ac865f7.tmp
2025-04-24 15:05:57.162 | 25/04/24 06:05:57 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-ef85e29e-f0bd-4d21-8c8c-96b8720dae2d/.metadata.4336e4c4-c865-49d7-9b06-fb803ac865f7.tmp to file:/tmp/temporary-ef85e29e-f0bd-4d21-8c8c-96b8720dae2d/metadata
2025-04-24 15:05:57.168 | 25/04/24 06:05:57 INFO MicroBatchExecution: Starting [id = f3e9493a-b283-448c-8c2e-f81a3cccf91b, runId = 7027e2f9-367f-47c4-b287-66d27ec2882b]. Use file:/tmp/temporary-ef85e29e-f0bd-4d21-8c8c-96b8720dae2d to store the query checkpoint.
2025-04-24 15:05:57.170 | 25/04/24 06:05:57 INFO MicroBatchExecution: Reading table [org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@4d193687] from DataSourceV2 named 'kafka' [org.apache.spark.sql.kafka010.KafkaSourceProvider@203330cb]
2025-04-24 15:05:57.171 | 25/04/24 06:05:57 INFO OffsetSeqLog: BatchIds found from listing: 
2025-04-24 15:05:57.171 | 25/04/24 06:05:57 INFO OffsetSeqLog: BatchIds found from listing: 
2025-04-24 15:05:57.171 | 25/04/24 06:05:57 INFO MicroBatchExecution: Starting new streaming query.
2025-04-24 15:05:57.171 | 25/04/24 06:05:57 INFO MicroBatchExecution: Stream started from {}
2025-04-24 15:05:57.174 | 25/04/24 06:05:57 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-6b4dd807-0b15-4df4-b97a-87fbdf0b3438. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.
2025-04-24 15:05:57.176 | 25/04/24 06:05:57 INFO ResolveWriteToStream: Checkpoint root file:///tmp/temporary-6b4dd807-0b15-4df4-b97a-87fbdf0b3438 resolved to file:/tmp/temporary-6b4dd807-0b15-4df4-b97a-87fbdf0b3438.
2025-04-24 15:05:57.176 | 25/04/24 06:05:57 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.
2025-04-24 15:05:57.183 | 25/04/24 06:05:57 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-6b4dd807-0b15-4df4-b97a-87fbdf0b3438/metadata using temp file file:/tmp/temporary-6b4dd807-0b15-4df4-b97a-87fbdf0b3438/.metadata.7e5e1c60-a8f1-4077-b070-80a87c65acc5.tmp
2025-04-24 15:05:57.197 | 25/04/24 06:05:57 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-6b4dd807-0b15-4df4-b97a-87fbdf0b3438/.metadata.7e5e1c60-a8f1-4077-b070-80a87c65acc5.tmp to file:/tmp/temporary-6b4dd807-0b15-4df4-b97a-87fbdf0b3438/metadata
2025-04-24 15:05:57.204 | 25/04/24 06:05:57 INFO MicroBatchExecution: Starting [id = 00653682-fa1a-49ed-863e-fa929f908a51, runId = efd8353c-1172-4f58-8d98-3b8ff2db3c7d]. Use file:/tmp/temporary-6b4dd807-0b15-4df4-b97a-87fbdf0b3438 to store the query checkpoint.
2025-04-24 15:05:57.205 | 25/04/24 06:05:57 INFO MicroBatchExecution: Reading table [org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@4d193687] from DataSourceV2 named 'kafka' [org.apache.spark.sql.kafka010.KafkaSourceProvider@203330cb]
2025-04-24 15:05:57.207 | 25/04/24 06:05:57 INFO OffsetSeqLog: BatchIds found from listing: 
2025-04-24 15:05:57.207 | 25/04/24 06:05:57 INFO OffsetSeqLog: BatchIds found from listing: 
2025-04-24 15:05:57.207 | 25/04/24 06:05:57 INFO MicroBatchExecution: Starting new streaming query.
2025-04-24 15:05:57.208 | 25/04/24 06:05:57 INFO MicroBatchExecution: Stream started from {}
2025-04-24 15:05:57.210 | 25/04/24 06:05:57 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-3c1df5f2-e1da-4beb-8c35-3451ca7efd9d. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.
2025-04-24 15:05:57.210 | 25/04/24 06:05:57 INFO ResolveWriteToStream: Checkpoint root file:///tmp/temporary-3c1df5f2-e1da-4beb-8c35-3451ca7efd9d resolved to file:/tmp/temporary-3c1df5f2-e1da-4beb-8c35-3451ca7efd9d.
2025-04-24 15:05:57.210 | 25/04/24 06:05:57 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.
2025-04-24 15:05:57.217 | 25/04/24 06:05:57 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-3c1df5f2-e1da-4beb-8c35-3451ca7efd9d/metadata using temp file file:/tmp/temporary-3c1df5f2-e1da-4beb-8c35-3451ca7efd9d/.metadata.1c093176-85db-45d1-adfa-5be613f07379.tmp
2025-04-24 15:05:57.230 | 25/04/24 06:05:57 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-3c1df5f2-e1da-4beb-8c35-3451ca7efd9d/.metadata.1c093176-85db-45d1-adfa-5be613f07379.tmp to file:/tmp/temporary-3c1df5f2-e1da-4beb-8c35-3451ca7efd9d/metadata
2025-04-24 15:05:57.235 | 25/04/24 06:05:57 INFO MicroBatchExecution: Starting [id = b981a279-063c-4d07-953e-5000a8a9c86c, runId = 4afe7425-0e6c-4ad7-9410-03a4942cca19]. Use file:/tmp/temporary-3c1df5f2-e1da-4beb-8c35-3451ca7efd9d to store the query checkpoint.
2025-04-24 15:05:57.237 | 25/04/24 06:05:57 INFO MicroBatchExecution: Reading table [org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@4d193687] from DataSourceV2 named 'kafka' [org.apache.spark.sql.kafka010.KafkaSourceProvider@203330cb]
2025-04-24 15:05:57.238 | 25/04/24 06:05:57 INFO OffsetSeqLog: BatchIds found from listing: 
2025-04-24 15:05:57.238 | 25/04/24 06:05:57 INFO OffsetSeqLog: BatchIds found from listing: 
2025-04-24 15:05:57.239 | 25/04/24 06:05:57 INFO MicroBatchExecution: Starting new streaming query.
2025-04-24 15:05:57.239 | 25/04/24 06:05:57 INFO MicroBatchExecution: Stream started from {}
2025-04-24 15:05:57.243 | 25/04/24 06:05:57 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-18018b60-cd91-4654-b55d-89552988ee35. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.
2025-04-24 15:05:57.244 | 25/04/24 06:05:57 INFO ResolveWriteToStream: Checkpoint root file:///tmp/temporary-18018b60-cd91-4654-b55d-89552988ee35 resolved to file:/tmp/temporary-18018b60-cd91-4654-b55d-89552988ee35.
2025-04-24 15:05:57.244 | 25/04/24 06:05:57 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.
2025-04-24 15:05:57.251 | 25/04/24 06:05:57 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-18018b60-cd91-4654-b55d-89552988ee35/metadata using temp file file:/tmp/temporary-18018b60-cd91-4654-b55d-89552988ee35/.metadata.a50bd8b4-d243-497d-aff5-a401b0acd2a5.tmp
2025-04-24 15:05:57.262 | 25/04/24 06:05:57 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-18018b60-cd91-4654-b55d-89552988ee35/.metadata.a50bd8b4-d243-497d-aff5-a401b0acd2a5.tmp to file:/tmp/temporary-18018b60-cd91-4654-b55d-89552988ee35/metadata
2025-04-24 15:05:57.269 | 25/04/24 06:05:57 INFO MicroBatchExecution: Starting [id = 25d3d7ed-535e-4772-9fc0-63351b9b5ef9, runId = f71c82ed-061e-41c6-9580-6396f33635a9]. Use file:/tmp/temporary-18018b60-cd91-4654-b55d-89552988ee35 to store the query checkpoint.
2025-04-24 15:05:57.271 | 25/04/24 06:05:57 INFO MicroBatchExecution: Reading table [org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@4d193687] from DataSourceV2 named 'kafka' [org.apache.spark.sql.kafka010.KafkaSourceProvider@203330cb]
2025-04-24 15:05:57.272 | 25/04/24 06:05:57 INFO OffsetSeqLog: BatchIds found from listing: 
2025-04-24 15:05:57.272 | 25/04/24 06:05:57 INFO OffsetSeqLog: BatchIds found from listing: 
2025-04-24 15:05:57.272 | 25/04/24 06:05:57 INFO MicroBatchExecution: Starting new streaming query.
2025-04-24 15:05:57.273 | 25/04/24 06:05:57 INFO MicroBatchExecution: Stream started from {}
2025-04-24 15:05:57.276 | 25/04/24 06:05:57 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-2ce6649d-5b58-4ef3-98a8-fb21f300afef. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.
2025-04-24 15:05:57.276 | 25/04/24 06:05:57 INFO ResolveWriteToStream: Checkpoint root file:///tmp/temporary-2ce6649d-5b58-4ef3-98a8-fb21f300afef resolved to file:/tmp/temporary-2ce6649d-5b58-4ef3-98a8-fb21f300afef.
2025-04-24 15:05:57.276 | 25/04/24 06:05:57 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.
2025-04-24 15:05:57.285 | 25/04/24 06:05:57 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-2ce6649d-5b58-4ef3-98a8-fb21f300afef/metadata using temp file file:/tmp/temporary-2ce6649d-5b58-4ef3-98a8-fb21f300afef/.metadata.d6379f44-a827-47ea-97f6-04a325bafdd8.tmp
2025-04-24 15:05:57.300 | 25/04/24 06:05:57 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-2ce6649d-5b58-4ef3-98a8-fb21f300afef/.metadata.d6379f44-a827-47ea-97f6-04a325bafdd8.tmp to file:/tmp/temporary-2ce6649d-5b58-4ef3-98a8-fb21f300afef/metadata
2025-04-24 15:05:57.307 | 25/04/24 06:05:57 INFO MicroBatchExecution: Starting [id = 2e8bba16-c696-4b17-8e4b-d77e80407107, runId = 56258db2-de72-41de-91ae-411133733965]. Use file:/tmp/temporary-2ce6649d-5b58-4ef3-98a8-fb21f300afef to store the query checkpoint.
2025-04-24 15:05:57.308 | 25/04/24 06:05:57 INFO MicroBatchExecution: Reading table [org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@4d193687] from DataSourceV2 named 'kafka' [org.apache.spark.sql.kafka010.KafkaSourceProvider@203330cb]
2025-04-24 15:05:57.310 | 25/04/24 06:05:57 INFO OffsetSeqLog: BatchIds found from listing: 
2025-04-24 15:05:57.310 | 25/04/24 06:05:57 INFO AdminClientConfig: AdminClientConfig values: 
2025-04-24 15:05:57.310 | 	auto.include.jmx.reporter = true
2025-04-24 15:05:57.310 | 	bootstrap.servers = [kafka:9092]
2025-04-24 15:05:57.310 | 	client.dns.lookup = use_all_dns_ips
2025-04-24 15:05:57.310 | 	client.id = 
2025-04-24 15:05:57.310 | 	connections.max.idle.ms = 300000
2025-04-24 15:05:57.310 | 	default.api.timeout.ms = 60000
2025-04-24 15:05:57.310 | 	metadata.max.age.ms = 300000
2025-04-24 15:05:57.310 | 	metric.reporters = []
2025-04-24 15:05:57.310 | 	metrics.num.samples = 2
2025-04-24 15:05:57.310 | 	metrics.recording.level = INFO
2025-04-24 15:05:57.310 | 	metrics.sample.window.ms = 30000
2025-04-24 15:05:57.310 | 	receive.buffer.bytes = 65536
2025-04-24 15:05:57.310 | 	reconnect.backoff.max.ms = 1000
2025-04-24 15:05:57.310 | 	reconnect.backoff.ms = 50
2025-04-24 15:05:57.310 | 	request.timeout.ms = 30000
2025-04-24 15:05:57.310 | 	retries = 2147483647
2025-04-24 15:05:57.310 | 	retry.backoff.ms = 100
2025-04-24 15:05:57.310 | 	sasl.client.callback.handler.class = null
2025-04-24 15:05:57.310 | 	sasl.jaas.config = null
2025-04-24 15:05:57.310 | 	sasl.kerberos.kinit.cmd = /usr/bin/kinit
2025-04-24 15:05:57.310 | 	sasl.kerberos.min.time.before.relogin = 60000
2025-04-24 15:05:57.310 | 	sasl.kerberos.service.name = null
2025-04-24 15:05:57.310 | 	sasl.kerberos.ticket.renew.jitter = 0.05
2025-04-24 15:05:57.310 | 	sasl.kerberos.ticket.renew.window.factor = 0.8
2025-04-24 15:05:57.310 | 	sasl.login.callback.handler.class = null
2025-04-24 15:05:57.310 | 	sasl.login.class = null
2025-04-24 15:05:57.310 | 	sasl.login.connect.timeout.ms = null
2025-04-24 15:05:57.310 | 	sasl.login.read.timeout.ms = null
2025-04-24 15:05:57.310 | 	sasl.login.refresh.buffer.seconds = 300
2025-04-24 15:05:57.310 | 	sasl.login.refresh.min.period.seconds = 60
2025-04-24 15:05:57.310 | 	sasl.login.refresh.window.factor = 0.8
2025-04-24 15:05:57.310 | 	sasl.login.refresh.window.jitter = 0.05
2025-04-24 15:05:57.310 | 	sasl.login.retry.backoff.max.ms = 10000
2025-04-24 15:05:57.310 | 	sasl.login.retry.backoff.ms = 100
2025-04-24 15:05:57.310 | 	sasl.mechanism = GSSAPI
2025-04-24 15:05:57.310 | 	sasl.oauthbearer.clock.skew.seconds = 30
2025-04-24 15:05:57.310 | 	sasl.oauthbearer.expected.audience = null
2025-04-24 15:05:57.310 | 	sasl.oauthbearer.expected.issuer = null
2025-04-24 15:05:57.310 | 	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
2025-04-24 15:05:57.310 | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
2025-04-24 15:05:57.310 | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
2025-04-24 15:05:57.310 | 	sasl.oauthbearer.jwks.endpoint.url = null
2025-04-24 15:05:57.310 | 	sasl.oauthbearer.scope.claim.name = scope
2025-04-24 15:05:57.310 | 	sasl.oauthbearer.sub.claim.name = sub
2025-04-24 15:05:57.310 | 	sasl.oauthbearer.token.endpoint.url = null
2025-04-24 15:05:57.310 | 	security.protocol = PLAINTEXT
2025-04-24 15:05:57.310 | 	security.providers = null
2025-04-24 15:05:57.310 | 	send.buffer.bytes = 131072
2025-04-24 15:05:57.310 | 	socket.connection.setup.timeout.max.ms = 30000
2025-04-24 15:05:57.310 | 	socket.connection.setup.timeout.ms = 10000
2025-04-24 15:05:57.310 | 	ssl.cipher.suites = null
2025-04-24 15:05:57.310 | 	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
2025-04-24 15:05:57.310 | 	ssl.endpoint.identification.algorithm = https
2025-04-24 15:05:57.310 | 	ssl.engine.factory.class = null
2025-04-24 15:05:57.310 | 	ssl.key.password = null
2025-04-24 15:05:57.310 | 	ssl.keymanager.algorithm = SunX509
2025-04-24 15:05:57.310 | 	ssl.keystore.certificate.chain = null
2025-04-24 15:05:57.310 | 	ssl.keystore.key = null
2025-04-24 15:05:57.310 | 	ssl.keystore.location = null
2025-04-24 15:05:57.310 | 	ssl.keystore.password = null
2025-04-24 15:05:57.310 | 	ssl.keystore.type = JKS
2025-04-24 15:05:57.310 | 	ssl.protocol = TLSv1.3
2025-04-24 15:05:57.310 | 	ssl.provider = null
2025-04-24 15:05:57.310 | 	ssl.secure.random.implementation = null
2025-04-24 15:05:57.310 | 	ssl.trustmanager.algorithm = PKIX
2025-04-24 15:05:57.310 | 	ssl.truststore.certificates = null
2025-04-24 15:05:57.310 | 	ssl.truststore.location = null
2025-04-24 15:05:57.310 | 	ssl.truststore.password = null
2025-04-24 15:05:57.310 | 	ssl.truststore.type = JKS
2025-04-24 15:05:57.310 | 
2025-04-24 15:05:57.310 | 25/04/24 06:05:57 INFO AdminClientConfig: AdminClientConfig values: 
2025-04-24 15:05:57.310 | 	auto.include.jmx.reporter = true
2025-04-24 15:05:57.310 | 	bootstrap.servers = [kafka:9092]
2025-04-24 15:05:57.310 | 	client.dns.lookup = use_all_dns_ips
2025-04-24 15:05:57.310 | 	client.id = 
2025-04-24 15:05:57.310 | 	connections.max.idle.ms = 300000
2025-04-24 15:05:57.310 | 	default.api.timeout.ms = 60000
2025-04-24 15:05:57.310 | 	metadata.max.age.ms = 300000
2025-04-24 15:05:57.310 | 	metric.reporters = []
2025-04-24 15:05:57.310 | 	metrics.num.samples = 2
2025-04-24 15:05:57.310 | 	metrics.recording.level = INFO
2025-04-24 15:05:57.310 | 	metrics.sample.window.ms = 30000
2025-04-24 15:05:57.310 | 	receive.buffer.bytes = 65536
2025-04-24 15:05:57.310 | 	reconnect.backoff.max.ms = 1000
2025-04-24 15:05:57.310 | 	reconnect.backoff.ms = 50
2025-04-24 15:05:57.310 | 	request.timeout.ms = 30000
2025-04-24 15:05:57.310 | 	retries = 2147483647
2025-04-24 15:05:57.310 | 	retry.backoff.ms = 100
2025-04-24 15:05:57.310 | 	sasl.client.callback.handler.class = null
2025-04-24 15:05:57.310 | 	sasl.jaas.config = null
2025-04-24 15:05:57.310 | 	sasl.kerberos.kinit.cmd = /usr/bin/kinit
2025-04-24 15:05:57.310 | 	sasl.kerberos.min.time.before.relogin = 60000
2025-04-24 15:05:57.310 | 	sasl.kerberos.service.name = null
2025-04-24 15:05:57.310 | 	sasl.kerberos.ticket.renew.jitter = 0.05
2025-04-24 15:05:57.310 | 	sasl.kerberos.ticket.renew.window.factor = 0.8
2025-04-24 15:05:57.310 | 	sasl.login.callback.handler.class = null
2025-04-24 15:05:57.310 | 	sasl.login.class = null
2025-04-24 15:05:57.310 | 	sasl.login.connect.timeout.ms = null
2025-04-24 15:05:57.310 | 	sasl.login.read.timeout.ms = null
2025-04-24 15:05:57.310 | 	sasl.login.refresh.buffer.seconds = 300
2025-04-24 15:05:57.310 | 	sasl.login.refresh.min.period.seconds = 60
2025-04-24 15:05:57.310 | 	sasl.login.refresh.window.factor = 0.8
2025-04-24 15:05:57.310 | 	sasl.login.refresh.window.jitter = 0.05
2025-04-24 15:05:57.310 | 	sasl.login.retry.backoff.max.ms = 10000
2025-04-24 15:05:57.310 | 	sasl.login.retry.backoff.ms = 100
2025-04-24 15:05:57.310 | 	sasl.mechanism = GSSAPI
2025-04-24 15:05:57.310 | 	sasl.oauthbearer.clock.skew.seconds = 30
2025-04-24 15:05:57.310 | 	sasl.oauthbearer.expected.audience = null
2025-04-24 15:05:57.310 | 	sasl.oauthbearer.expected.issuer = null
2025-04-24 15:05:57.310 | 	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
2025-04-24 15:05:57.310 | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
2025-04-24 15:05:57.310 | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
2025-04-24 15:05:57.310 | 	sasl.oauthbearer.jwks.endpoint.url = null
2025-04-24 15:05:57.310 | 	sasl.oauthbearer.scope.claim.name = scope
2025-04-24 15:05:57.310 | 	sasl.oauthbearer.sub.claim.name = sub
2025-04-24 15:05:57.310 | 	sasl.oauthbearer.token.endpoint.url = null
2025-04-24 15:05:57.310 | 	security.protocol = PLAINTEXT
2025-04-24 15:05:57.310 | 	security.providers = null
2025-04-24 15:05:57.310 | 	send.buffer.bytes = 131072
2025-04-24 15:05:57.310 | 	socket.connection.setup.timeout.max.ms = 30000
2025-04-24 15:05:57.310 | 	socket.connection.setup.timeout.ms = 10000
2025-04-24 15:05:57.310 | 	ssl.cipher.suites = null
2025-04-24 15:05:57.310 | 	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
2025-04-24 15:05:57.310 | 	ssl.endpoint.identification.algorithm = https
2025-04-24 15:05:57.310 | 	ssl.engine.factory.class = null
2025-04-24 15:05:57.310 | 	ssl.key.password = null
2025-04-24 15:05:57.310 | 	ssl.keymanager.algorithm = SunX509
2025-04-24 15:05:57.310 | 	ssl.keystore.certificate.chain = null
2025-04-24 15:05:57.310 | 	ssl.keystore.key = null
2025-04-24 15:05:57.310 | 	ssl.keystore.location = null
2025-04-24 15:05:57.310 | 	ssl.keystore.password = null
2025-04-24 15:05:57.310 | 	ssl.keystore.type = JKS
2025-04-24 15:05:57.310 | 	ssl.protocol = TLSv1.3
2025-04-24 15:05:57.310 | 	ssl.provider = null
2025-04-24 15:05:57.310 | 	ssl.secure.random.implementation = null
2025-04-24 15:05:57.310 | 	ssl.trustmanager.algorithm = PKIX
2025-04-24 15:05:57.310 | 	ssl.truststore.certificates = null
2025-04-24 15:05:57.310 | 	ssl.truststore.location = null
2025-04-24 15:05:57.310 | 	ssl.truststore.password = null
2025-04-24 15:05:57.310 | 	ssl.truststore.type = JKS
2025-04-24 15:05:57.310 | 
2025-04-24 15:05:57.310 | 25/04/24 06:05:57 INFO AdminClientConfig: AdminClientConfig values: 
2025-04-24 15:05:57.310 | 	auto.include.jmx.reporter = true
2025-04-24 15:05:57.310 | 	bootstrap.servers = [kafka:9092]
2025-04-24 15:05:57.310 | 	client.dns.lookup = use_all_dns_ips
2025-04-24 15:05:57.310 | 	client.id = 
2025-04-24 15:05:57.310 | 	connections.max.idle.ms = 300000
2025-04-24 15:05:57.310 | 	default.api.timeout.ms = 60000
2025-04-24 15:05:57.310 | 	metadata.max.age.ms = 300000
2025-04-24 15:05:57.310 | 	metric.reporters = []
2025-04-24 15:05:57.310 | 	metrics.num.samples = 2
2025-04-24 15:05:57.310 | 	metrics.recording.level = INFO
2025-04-24 15:05:57.310 | 	metrics.sample.window.ms = 30000
2025-04-24 15:05:57.310 | 	receive.buffer.bytes = 65536
2025-04-24 15:05:57.310 | 	reconnect.backoff.max.ms = 1000
2025-04-24 15:05:57.310 | 	reconnect.backoff.ms = 50
2025-04-24 15:05:57.310 | 	request.timeout.ms = 30000
2025-04-24 15:05:57.310 | 	retries = 2147483647
2025-04-24 15:05:57.310 | 	retry.backoff.ms = 100
2025-04-24 15:05:57.310 | 	sasl.client.callback.handler.class = null
2025-04-24 15:05:57.310 | 	sasl.jaas.config = null
2025-04-24 15:05:57.310 | 	sasl.kerberos.kinit.cmd = /usr/bin/kinit
2025-04-24 15:05:57.310 | 	sasl.kerberos.min.time.before.relogin = 60000
2025-04-24 15:05:57.310 | 	sasl.kerberos.service.name = null
2025-04-24 15:05:57.310 | 	sasl.kerberos.ticket.renew.jitter = 0.05
2025-04-24 15:05:57.310 | 	sasl.kerberos.ticket.renew.window.factor = 0.8
2025-04-24 15:05:57.310 | 	sasl.login.callback.handler.class = null
2025-04-24 15:05:57.310 | 	sasl.login.class = null
2025-04-24 15:05:57.310 | 	sasl.login.connect.timeout.ms = null
2025-04-24 15:05:57.310 | 	sasl.login.read.timeout.ms = null
2025-04-24 15:05:57.310 | 	sasl.login.refresh.buffer.seconds = 300
2025-04-24 15:05:57.310 | 	sasl.login.refresh.min.period.seconds = 60
2025-04-24 15:05:57.310 | 	sasl.login.refresh.window.factor = 0.8
2025-04-24 15:05:57.310 | 	sasl.login.refresh.window.jitter = 0.05
2025-04-24 15:05:57.310 | 	sasl.login.retry.backoff.max.ms = 10000
2025-04-24 15:05:57.310 | 	sasl.login.retry.backoff.ms = 100
2025-04-24 15:05:57.310 | 	sasl.mechanism = GSSAPI
2025-04-24 15:05:57.310 | 	sasl.oauthbearer.clock.skew.seconds = 30
2025-04-24 15:05:57.310 | 	sasl.oauthbearer.expected.audience = null
2025-04-24 15:05:57.310 | 	sasl.oauthbearer.expected.issuer = null
2025-04-24 15:05:57.310 | 	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
2025-04-24 15:05:57.310 | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
2025-04-24 15:05:57.310 | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
2025-04-24 15:05:57.310 | 	sasl.oauthbearer.jwks.endpoint.url = null
2025-04-24 15:05:57.310 | 	sasl.oauthbearer.scope.claim.name = scope
2025-04-24 15:05:57.310 | 	sasl.oauthbearer.sub.claim.name = sub
2025-04-24 15:05:57.310 | 	sasl.oauthbearer.token.endpoint.url = null
2025-04-24 15:05:57.310 | 	security.protocol = PLAINTEXT
2025-04-24 15:05:57.310 | 	security.providers = null
2025-04-24 15:05:57.310 | 	send.buffer.bytes = 131072
2025-04-24 15:05:57.310 | 	socket.connection.setup.timeout.max.ms = 30000
2025-04-24 15:05:57.310 | 	socket.connection.setup.timeout.ms = 10000
2025-04-24 15:05:57.310 | 	ssl.cipher.suites = null
2025-04-24 15:05:57.310 | 	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
2025-04-24 15:05:57.310 | 	ssl.endpoint.identification.algorithm = https
2025-04-24 15:05:57.310 | 	ssl.engine.factory.class = null
2025-04-24 15:05:57.310 | 	ssl.key.password = null
2025-04-24 15:05:57.310 | 	ssl.keymanager.algorithm = SunX509
2025-04-24 15:05:57.310 | 	ssl.keystore.certificate.chain = null
2025-04-24 15:05:57.310 | 	ssl.keystore.key = null
2025-04-24 15:05:57.310 | 	ssl.keystore.location = null
2025-04-24 15:05:57.310 | 	ssl.keystore.password = null
2025-04-24 15:05:57.310 | 	ssl.keystore.type = JKS
2025-04-24 15:05:57.310 | 	ssl.protocol = TLSv1.3
2025-04-24 15:05:57.310 | 	ssl.provider = null
2025-04-24 15:05:57.310 | 	ssl.secure.random.implementation = null
2025-04-24 15:05:57.310 | 	ssl.trustmanager.algorithm = PKIX
2025-04-24 15:05:57.310 | 	ssl.truststore.certificates = null
2025-04-24 15:05:57.310 | 	ssl.truststore.location = null
2025-04-24 15:05:57.310 | 	ssl.truststore.password = null
2025-04-24 15:05:57.310 | 	ssl.truststore.type = JKS
2025-04-24 15:05:57.310 | 
2025-04-24 15:05:57.310 | 25/04/24 06:05:57 INFO AdminClientConfig: AdminClientConfig values: 
2025-04-24 15:05:57.310 | 	auto.include.jmx.reporter = true
2025-04-24 15:05:57.310 | 	bootstrap.servers = [kafka:9092]
2025-04-24 15:05:57.310 | 	client.dns.lookup = use_all_dns_ips
2025-04-24 15:05:57.310 | 	client.id = 
2025-04-24 15:05:57.310 | 	connections.max.idle.ms = 300000
2025-04-24 15:05:57.310 | 	default.api.timeout.ms = 60000
2025-04-24 15:05:57.310 | 	metadata.max.age.ms = 300000
2025-04-24 15:05:57.310 | 	metric.reporters = []
2025-04-24 15:05:57.310 | 	metrics.num.samples = 2
2025-04-24 15:05:57.310 | 	metrics.recording.level = INFO
2025-04-24 15:05:57.310 | 	metrics.sample.window.ms = 30000
2025-04-24 15:05:57.310 | 	receive.buffer.bytes = 65536
2025-04-24 15:05:57.310 | 	reconnect.backoff.max.ms = 1000
2025-04-24 15:05:57.310 | 	reconnect.backoff.ms = 50
2025-04-24 15:05:57.310 | 	request.timeout.ms = 30000
2025-04-24 15:05:57.310 | 	retries = 2147483647
2025-04-24 15:05:57.310 | 	retry.backoff.ms = 100
2025-04-24 15:05:57.310 | 	sasl.client.callback.handler.class = null
2025-04-24 15:05:57.310 | 	sasl.jaas.config = null
2025-04-24 15:05:57.310 | 	sasl.kerberos.kinit.cmd = /usr/bin/kinit
2025-04-24 15:05:57.310 | 	sasl.kerberos.min.time.before.relogin = 60000
2025-04-24 15:05:57.310 | 	sasl.kerberos.service.name = null
2025-04-24 15:05:57.310 | 	sasl.kerberos.ticket.renew.jitter = 0.05
2025-04-24 15:05:57.310 | 	sasl.kerberos.ticket.renew.window.factor = 0.8
2025-04-24 15:05:57.310 | 	sasl.login.callback.handler.class = null
2025-04-24 15:05:57.310 | 	sasl.login.class = null
2025-04-24 15:05:57.310 | 	sasl.login.connect.timeout.ms = null
2025-04-24 15:05:57.310 | 	sasl.login.read.timeout.ms = null
2025-04-24 15:05:57.310 | 	sasl.login.refresh.buffer.seconds = 300
2025-04-24 15:05:57.310 | 	sasl.login.refresh.min.period.seconds = 60
2025-04-24 15:05:57.310 | 	sasl.login.refresh.window.factor = 0.8
2025-04-24 15:05:57.310 | 	sasl.login.refresh.window.jitter = 0.05
2025-04-24 15:05:57.310 | 	sasl.login.retry.backoff.max.ms = 10000
2025-04-24 15:05:57.310 | 	sasl.login.retry.backoff.ms = 100
2025-04-24 15:05:57.310 | 	sasl.mechanism = GSSAPI
2025-04-24 15:05:57.310 | 	sasl.oauthbearer.clock.skew.seconds = 30
2025-04-24 15:05:57.310 | 	sasl.oauthbearer.expected.audience = null
2025-04-24 15:05:57.310 | 	sasl.oauthbearer.expected.issuer = null
2025-04-24 15:05:57.310 | 	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
2025-04-24 15:05:57.310 | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
2025-04-24 15:05:57.310 | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
2025-04-24 15:05:57.310 | 	sasl.oauthbearer.jwks.endpoint.url = null
2025-04-24 15:05:57.310 | 	sasl.oauthbearer.scope.claim.name = scope
2025-04-24 15:05:57.310 | 	sasl.oauthbearer.sub.claim.name = sub
2025-04-24 15:05:57.310 | 	sasl.oauthbearer.token.endpoint.url = null
2025-04-24 15:05:57.310 | 	security.protocol = PLAINTEXT
2025-04-24 15:05:57.310 | 	security.providers = null
2025-04-24 15:05:57.310 | 	send.buffer.bytes = 131072
2025-04-24 15:05:57.310 | 	socket.connection.setup.timeout.max.ms = 30000
2025-04-24 15:05:57.310 | 	socket.connection.setup.timeout.ms = 10000
2025-04-24 15:05:57.310 | 	ssl.cipher.suites = null
2025-04-24 15:05:57.310 | 	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
2025-04-24 15:05:57.310 | 	ssl.endpoint.identification.algorithm = https
2025-04-24 15:05:57.310 | 	ssl.engine.factory.class = null
2025-04-24 15:05:57.310 | 	ssl.key.password = null
2025-04-24 15:05:57.310 | 	ssl.keymanager.algorithm = SunX509
2025-04-24 15:05:57.310 | 	ssl.keystore.certificate.chain = null
2025-04-24 15:05:57.310 | 	ssl.keystore.key = null
2025-04-24 15:05:57.310 | 	ssl.keystore.location = null
2025-04-24 15:05:57.310 | 	ssl.keystore.password = null
2025-04-24 15:05:57.310 | 	ssl.keystore.type = JKS
2025-04-24 15:05:57.310 | 	ssl.protocol = TLSv1.3
2025-04-24 15:05:57.310 | 	ssl.provider = null
2025-04-24 15:05:57.310 | 	ssl.secure.random.implementation = null
2025-04-24 15:05:57.310 | 	ssl.trustmanager.algorithm = PKIX
2025-04-24 15:05:57.310 | 	ssl.truststore.certificates = null
2025-04-24 15:05:57.310 | 	ssl.truststore.location = null
2025-04-24 15:05:57.310 | 	ssl.truststore.password = null
2025-04-24 15:05:57.310 | 	ssl.truststore.type = JKS
2025-04-24 15:05:57.310 | 
2025-04-24 15:05:57.310 | 25/04/24 06:05:57 INFO AdminClientConfig: AdminClientConfig values: 
2025-04-24 15:05:57.310 | 	auto.include.jmx.reporter = true
2025-04-24 15:05:57.310 | 	bootstrap.servers = [kafka:9092]
2025-04-24 15:05:57.310 | 	client.dns.lookup = use_all_dns_ips
2025-04-24 15:05:57.310 | 	client.id = 
2025-04-24 15:05:57.310 | 	connections.max.idle.ms = 300000
2025-04-24 15:05:57.310 | 	default.api.timeout.ms = 60000
2025-04-24 15:05:57.310 | 	metadata.max.age.ms = 300000
2025-04-24 15:05:57.310 | 	metric.reporters = []
2025-04-24 15:05:57.310 | 	metrics.num.samples = 2
2025-04-24 15:05:57.310 | 	metrics.recording.level = INFO
2025-04-24 15:05:57.310 | 	metrics.sample.window.ms = 30000
2025-04-24 15:05:57.310 | 	receive.buffer.bytes = 65536
2025-04-24 15:05:57.310 | 	reconnect.backoff.max.ms = 1000
2025-04-24 15:05:57.310 | 	reconnect.backoff.ms = 50
2025-04-24 15:05:57.310 | 	request.timeout.ms = 30000
2025-04-24 15:05:57.310 | 	retries = 2147483647
2025-04-24 15:05:57.310 | 	retry.backoff.ms = 100
2025-04-24 15:05:57.310 | 	sasl.client.callback.handler.class = null
2025-04-24 15:05:57.310 | 	sasl.jaas.config = null
2025-04-24 15:05:57.310 | 	sasl.kerberos.kinit.cmd = /usr/bin/kinit
2025-04-24 15:05:57.310 | 	sasl.kerberos.min.time.before.relogin = 60000
2025-04-24 15:05:57.310 | 	sasl.kerberos.service.name = null
2025-04-24 15:05:57.310 | 	sasl.kerberos.ticket.renew.jitter = 0.05
2025-04-24 15:05:57.310 | 	sasl.kerberos.ticket.renew.window.factor = 0.8
2025-04-24 15:05:57.310 | 	sasl.login.callback.handler.class = null
2025-04-24 15:05:57.310 | 	sasl.login.class = null
2025-04-24 15:05:57.310 | 	sasl.login.connect.timeout.ms = null
2025-04-24 15:05:57.310 | 	sasl.login.read.timeout.ms = null
2025-04-24 15:05:57.310 | 	sasl.login.refresh.buffer.seconds = 300
2025-04-24 15:05:57.310 | 	sasl.login.refresh.min.period.seconds = 60
2025-04-24 15:05:57.310 | 	sasl.login.refresh.window.factor = 0.8
2025-04-24 15:05:57.310 | 	sasl.login.refresh.window.jitter = 0.05
2025-04-24 15:05:57.310 | 	sasl.login.retry.backoff.max.ms = 10000
2025-04-24 15:05:57.310 | 	sasl.login.retry.backoff.ms = 100
2025-04-24 15:05:57.310 | 	sasl.mechanism = GSSAPI
2025-04-24 15:05:57.310 | 	sasl.oauthbearer.clock.skew.seconds = 30
2025-04-24 15:05:57.310 | 	sasl.oauthbearer.expected.audience = null
2025-04-24 15:05:57.310 | 	sasl.oauthbearer.expected.issuer = null
2025-04-24 15:05:57.310 | 	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
2025-04-24 15:05:57.310 | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
2025-04-24 15:05:57.310 | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
2025-04-24 15:05:57.310 | 	sasl.oauthbearer.jwks.endpoint.url = null
2025-04-24 15:05:57.310 | 	sasl.oauthbearer.scope.claim.name = scope
2025-04-24 15:05:57.310 | 	sasl.oauthbearer.sub.claim.name = sub
2025-04-24 15:05:57.310 | 	sasl.oauthbearer.token.endpoint.url = null
2025-04-24 15:05:57.310 | 	security.protocol = PLAINTEXT
2025-04-24 15:05:57.310 | 	security.providers = null
2025-04-24 15:05:57.310 | 	send.buffer.bytes = 131072
2025-04-24 15:05:57.310 | 	socket.connection.setup.timeout.max.ms = 30000
2025-04-24 15:05:57.310 | 	socket.connection.setup.timeout.ms = 10000
2025-04-24 15:05:57.310 | 	ssl.cipher.suites = null
2025-04-24 15:05:57.310 | 	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
2025-04-24 15:05:57.310 | 	ssl.endpoint.identification.algorithm = https
2025-04-24 15:05:57.310 | 	ssl.engine.factory.class = null
2025-04-24 15:05:57.310 | 	ssl.key.password = null
2025-04-24 15:05:57.310 | 	ssl.keymanager.algorithm = SunX509
2025-04-24 15:05:57.310 | 	ssl.keystore.certificate.chain = null
2025-04-24 15:05:57.310 | 	ssl.keystore.key = null
2025-04-24 15:05:57.310 | 	ssl.keystore.location = null
2025-04-24 15:05:57.310 | 	ssl.keystore.password = null
2025-04-24 15:05:57.310 | 	ssl.keystore.type = JKS
2025-04-24 15:05:57.310 | 	ssl.protocol = TLSv1.3
2025-04-24 15:05:57.310 | 	ssl.provider = null
2025-04-24 15:05:57.310 | 	ssl.secure.random.implementation = null
2025-04-24 15:05:57.310 | 	ssl.trustmanager.algorithm = PKIX
2025-04-24 15:05:57.310 | 	ssl.truststore.certificates = null
2025-04-24 15:05:57.310 | 	ssl.truststore.location = null
2025-04-24 15:05:57.310 | 	ssl.truststore.password = null
2025-04-24 15:05:57.310 | 	ssl.truststore.type = JKS
2025-04-24 15:05:57.310 | 
2025-04-24 15:05:57.310 | 25/04/24 06:05:57 INFO AdminClientConfig: AdminClientConfig values: 
2025-04-24 15:05:57.310 | 	auto.include.jmx.reporter = true
2025-04-24 15:05:57.310 | 	bootstrap.servers = [kafka:9092]
2025-04-24 15:05:57.310 | 	client.dns.lookup = use_all_dns_ips
2025-04-24 15:05:57.311 | 	client.id = 
2025-04-24 15:05:57.311 | 	connections.max.idle.ms = 300000
2025-04-24 15:05:57.311 | 	default.api.timeout.ms = 60000
2025-04-24 15:05:57.311 | 	metadata.max.age.ms = 300000
2025-04-24 15:05:57.311 | 	metric.reporters = []
2025-04-24 15:05:57.311 | 	metrics.num.samples = 2
2025-04-24 15:05:57.311 | 	metrics.recording.level = INFO
2025-04-24 15:05:57.311 | 	metrics.sample.window.ms = 30000
2025-04-24 15:05:57.311 | 	receive.buffer.bytes = 65536
2025-04-24 15:05:57.311 | 	reconnect.backoff.max.ms = 1000
2025-04-24 15:05:57.311 | 	reconnect.backoff.ms = 50
2025-04-24 15:05:57.311 | 	request.timeout.ms = 30000
2025-04-24 15:05:57.311 | 	retries = 2147483647
2025-04-24 15:05:57.311 | 	retry.backoff.ms = 100
2025-04-24 15:05:57.311 | 	sasl.client.callback.handler.class = null
2025-04-24 15:05:57.311 | 	sasl.jaas.config = null
2025-04-24 15:05:57.311 | 	sasl.kerberos.kinit.cmd = /usr/bin/kinit
2025-04-24 15:05:57.311 | 	sasl.kerberos.min.time.before.relogin = 60000
2025-04-24 15:05:57.311 | 	sasl.kerberos.service.name = null
2025-04-24 15:05:57.311 | 	sasl.kerberos.ticket.renew.jitter = 0.05
2025-04-24 15:05:57.311 | 	sasl.kerberos.ticket.renew.window.factor = 0.8
2025-04-24 15:05:57.311 | 	sasl.login.callback.handler.class = null
2025-04-24 15:05:57.311 | 	sasl.login.class = null
2025-04-24 15:05:57.311 | 	sasl.login.connect.timeout.ms = null
2025-04-24 15:05:57.311 | 	sasl.login.read.timeout.ms = null
2025-04-24 15:05:57.311 | 	sasl.login.refresh.buffer.seconds = 300
2025-04-24 15:05:57.311 | 	sasl.login.refresh.min.period.seconds = 60
2025-04-24 15:05:57.311 | 	sasl.login.refresh.window.factor = 0.8
2025-04-24 15:05:57.311 | 	sasl.login.refresh.window.jitter = 0.05
2025-04-24 15:05:57.311 | 	sasl.login.retry.backoff.max.ms = 10000
2025-04-24 15:05:57.311 | 	sasl.login.retry.backoff.ms = 100
2025-04-24 15:05:57.311 | 	sasl.mechanism = GSSAPI
2025-04-24 15:05:57.311 | 	sasl.oauthbearer.clock.skew.seconds = 30
2025-04-24 15:05:57.311 | 	sasl.oauthbearer.expected.audience = null
2025-04-24 15:05:57.311 | 	sasl.oauthbearer.expected.issuer = null
2025-04-24 15:05:57.311 | 	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
2025-04-24 15:05:57.311 | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
2025-04-24 15:05:57.311 | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
2025-04-24 15:05:57.311 | 	sasl.oauthbearer.jwks.endpoint.url = null
2025-04-24 15:05:57.311 | 	sasl.oauthbearer.scope.claim.name = scope
2025-04-24 15:05:57.311 | 	sasl.oauthbearer.sub.claim.name = sub
2025-04-24 15:05:57.311 | 	sasl.oauthbearer.token.endpoint.url = null
2025-04-24 15:05:57.311 | 	security.protocol = PLAINTEXT
2025-04-24 15:05:57.311 | 	security.providers = null
2025-04-24 15:05:57.311 | 	send.buffer.bytes = 131072
2025-04-24 15:05:57.311 | 	socket.connection.setup.timeout.max.ms = 30000
2025-04-24 15:05:57.311 | 	socket.connection.setup.timeout.ms = 10000
2025-04-24 15:05:57.311 | 	ssl.cipher.suites = null
2025-04-24 15:05:57.311 | 	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
2025-04-24 15:05:57.311 | 	ssl.endpoint.identification.algorithm = https
2025-04-24 15:05:57.311 | 	ssl.engine.factory.class = null
2025-04-24 15:05:57.311 | 	ssl.key.password = null
2025-04-24 15:05:57.311 | 	ssl.keymanager.algorithm = SunX509
2025-04-24 15:05:57.311 | 	ssl.keystore.certificate.chain = null
2025-04-24 15:05:57.311 | 	ssl.keystore.key = null
2025-04-24 15:05:57.311 | 	ssl.keystore.location = null
2025-04-24 15:05:57.311 | 	ssl.keystore.password = null
2025-04-24 15:05:57.311 | 	ssl.keystore.type = JKS
2025-04-24 15:05:57.311 | 	ssl.protocol = TLSv1.3
2025-04-24 15:05:57.311 | 	ssl.provider = null
2025-04-24 15:05:57.311 | 	ssl.secure.random.implementation = null
2025-04-24 15:05:57.311 | 	ssl.trustmanager.algorithm = PKIX
2025-04-24 15:05:57.311 | 	ssl.truststore.certificates = null
2025-04-24 15:05:57.311 | 	ssl.truststore.location = null
2025-04-24 15:05:57.311 | 	ssl.truststore.password = null
2025-04-24 15:05:57.311 | 	ssl.truststore.type = JKS
2025-04-24 15:05:57.311 | 
2025-04-24 15:05:57.311 | 25/04/24 06:05:57 INFO OffsetSeqLog: BatchIds found from listing: 
2025-04-24 15:05:57.311 | 25/04/24 06:05:57 INFO MicroBatchExecution: Starting new streaming query.
2025-04-24 15:05:57.311 | 25/04/24 06:05:57 INFO MicroBatchExecution: Stream started from {}
2025-04-24 15:05:57.311 | 25/04/24 06:05:57 INFO AdminClientConfig: AdminClientConfig values: 
2025-04-24 15:05:57.311 | 	auto.include.jmx.reporter = true
2025-04-24 15:05:57.311 | 	bootstrap.servers = [kafka:9092]
2025-04-24 15:05:57.311 | 	client.dns.lookup = use_all_dns_ips
2025-04-24 15:05:57.311 | 	client.id = 
2025-04-24 15:05:57.311 | 	connections.max.idle.ms = 300000
2025-04-24 15:05:57.311 | 	default.api.timeout.ms = 60000
2025-04-24 15:05:57.311 | 	metadata.max.age.ms = 300000
2025-04-24 15:05:57.311 | 	metric.reporters = []
2025-04-24 15:05:57.311 | 	metrics.num.samples = 2
2025-04-24 15:05:57.311 | 	metrics.recording.level = INFO
2025-04-24 15:05:57.311 | 	metrics.sample.window.ms = 30000
2025-04-24 15:05:57.311 | 	receive.buffer.bytes = 65536
2025-04-24 15:05:57.311 | 	reconnect.backoff.max.ms = 1000
2025-04-24 15:05:57.311 | 	reconnect.backoff.ms = 50
2025-04-24 15:05:57.311 | 	request.timeout.ms = 30000
2025-04-24 15:05:57.311 | 	retries = 2147483647
2025-04-24 15:05:57.311 | 	retry.backoff.ms = 100
2025-04-24 15:05:57.311 | 	sasl.client.callback.handler.class = null
2025-04-24 15:05:57.311 | 	sasl.jaas.config = null
2025-04-24 15:05:57.311 | 	sasl.kerberos.kinit.cmd = /usr/bin/kinit
2025-04-24 15:05:57.311 | 	sasl.kerberos.min.time.before.relogin = 60000
2025-04-24 15:05:57.311 | 	sasl.kerberos.service.name = null
2025-04-24 15:05:57.311 | 	sasl.kerberos.ticket.renew.jitter = 0.05
2025-04-24 15:05:57.311 | 	sasl.kerberos.ticket.renew.window.factor = 0.8
2025-04-24 15:05:57.311 | 	sasl.login.callback.handler.class = null
2025-04-24 15:05:57.311 | 	sasl.login.class = null
2025-04-24 15:05:57.311 | 	sasl.login.connect.timeout.ms = null
2025-04-24 15:05:57.311 | 	sasl.login.read.timeout.ms = null
2025-04-24 15:05:57.311 | 	sasl.login.refresh.buffer.seconds = 300
2025-04-24 15:05:57.311 | 	sasl.login.refresh.min.period.seconds = 60
2025-04-24 15:05:57.311 | 	sasl.login.refresh.window.factor = 0.8
2025-04-24 15:05:57.311 | 	sasl.login.refresh.window.jitter = 0.05
2025-04-24 15:05:57.311 | 	sasl.login.retry.backoff.max.ms = 10000
2025-04-24 15:05:57.311 | 	sasl.login.retry.backoff.ms = 100
2025-04-24 15:05:57.311 | 	sasl.mechanism = GSSAPI
2025-04-24 15:05:57.311 | 	sasl.oauthbearer.clock.skew.seconds = 30
2025-04-24 15:05:57.311 | 	sasl.oauthbearer.expected.audience = null
2025-04-24 15:05:57.311 | 	sasl.oauthbearer.expected.issuer = null
2025-04-24 15:05:57.311 | 	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
2025-04-24 15:05:57.311 | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
2025-04-24 15:05:57.311 | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
2025-04-24 15:05:57.311 | 	sasl.oauthbearer.jwks.endpoint.url = null
2025-04-24 15:05:57.311 | 	sasl.oauthbearer.scope.claim.name = scope
2025-04-24 15:05:57.311 | 	sasl.oauthbearer.sub.claim.name = sub
2025-04-24 15:05:57.311 | 	sasl.oauthbearer.token.endpoint.url = null
2025-04-24 15:05:57.311 | 	security.protocol = PLAINTEXT
2025-04-24 15:05:57.311 | 	security.providers = null
2025-04-24 15:05:57.311 | 	send.buffer.bytes = 131072
2025-04-24 15:05:57.311 | 	socket.connection.setup.timeout.max.ms = 30000
2025-04-24 15:05:57.311 | 	socket.connection.setup.timeout.ms = 10000
2025-04-24 15:05:57.311 | 	ssl.cipher.suites = null
2025-04-24 15:05:57.311 | 	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
2025-04-24 15:05:57.311 | 	ssl.endpoint.identification.algorithm = https
2025-04-24 15:05:57.311 | 	ssl.engine.factory.class = null
2025-04-24 15:05:57.311 | 	ssl.key.password = null
2025-04-24 15:05:57.311 | 	ssl.keymanager.algorithm = SunX509
2025-04-24 15:05:57.311 | 	ssl.keystore.certificate.chain = null
2025-04-24 15:05:57.311 | 	ssl.keystore.key = null
2025-04-24 15:05:57.311 | 	ssl.keystore.location = null
2025-04-24 15:05:57.311 | 	ssl.keystore.password = null
2025-04-24 15:05:57.311 | 	ssl.keystore.type = JKS
2025-04-24 15:05:57.311 | 	ssl.protocol = TLSv1.3
2025-04-24 15:05:57.311 | 	ssl.provider = null
2025-04-24 15:05:57.311 | 	ssl.secure.random.implementation = null
2025-04-24 15:05:57.311 | 	ssl.trustmanager.algorithm = PKIX
2025-04-24 15:05:57.311 | 	ssl.truststore.certificates = null
2025-04-24 15:05:57.311 | 	ssl.truststore.location = null
2025-04-24 15:05:57.311 | 	ssl.truststore.password = null
2025-04-24 15:05:57.311 | 	ssl.truststore.type = JKS
2025-04-24 15:05:57.311 | 
2025-04-24 15:05:57.314 | 25/04/24 06:05:57 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-ec737e79-bc80-4f79-9a41-d619dca08576. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.
2025-04-24 15:05:57.314 | 25/04/24 06:05:57 INFO ResolveWriteToStream: Checkpoint root file:///tmp/temporary-ec737e79-bc80-4f79-9a41-d619dca08576 resolved to file:/tmp/temporary-ec737e79-bc80-4f79-9a41-d619dca08576.
2025-04-24 15:05:57.314 | 25/04/24 06:05:57 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.
2025-04-24 15:05:57.317 | 25/04/24 06:05:57 INFO AdminClientConfig: AdminClientConfig values: 
2025-04-24 15:05:57.317 | 	auto.include.jmx.reporter = true
2025-04-24 15:05:57.317 | 	bootstrap.servers = [kafka:9092]
2025-04-24 15:05:57.317 | 	client.dns.lookup = use_all_dns_ips
2025-04-24 15:05:57.317 | 	client.id = 
2025-04-24 15:05:57.317 | 	connections.max.idle.ms = 300000
2025-04-24 15:05:57.317 | 	default.api.timeout.ms = 60000
2025-04-24 15:05:57.317 | 	metadata.max.age.ms = 300000
2025-04-24 15:05:57.317 | 	metric.reporters = []
2025-04-24 15:05:57.317 | 	metrics.num.samples = 2
2025-04-24 15:05:57.317 | 	metrics.recording.level = INFO
2025-04-24 15:05:57.317 | 	metrics.sample.window.ms = 30000
2025-04-24 15:05:57.317 | 	receive.buffer.bytes = 65536
2025-04-24 15:05:57.317 | 	reconnect.backoff.max.ms = 1000
2025-04-24 15:05:57.317 | 	reconnect.backoff.ms = 50
2025-04-24 15:05:57.317 | 	request.timeout.ms = 30000
2025-04-24 15:05:57.317 | 	retries = 2147483647
2025-04-24 15:05:57.317 | 	retry.backoff.ms = 100
2025-04-24 15:05:57.317 | 	sasl.client.callback.handler.class = null
2025-04-24 15:05:57.317 | 	sasl.jaas.config = null
2025-04-24 15:05:57.317 | 	sasl.kerberos.kinit.cmd = /usr/bin/kinit
2025-04-24 15:05:57.317 | 	sasl.kerberos.min.time.before.relogin = 60000
2025-04-24 15:05:57.317 | 	sasl.kerberos.service.name = null
2025-04-24 15:05:57.317 | 	sasl.kerberos.ticket.renew.jitter = 0.05
2025-04-24 15:05:57.317 | 	sasl.kerberos.ticket.renew.window.factor = 0.8
2025-04-24 15:05:57.317 | 	sasl.login.callback.handler.class = null
2025-04-24 15:05:57.317 | 	sasl.login.class = null
2025-04-24 15:05:57.317 | 	sasl.login.connect.timeout.ms = null
2025-04-24 15:05:57.317 | 	sasl.login.read.timeout.ms = null
2025-04-24 15:05:57.317 | 	sasl.login.refresh.buffer.seconds = 300
2025-04-24 15:05:57.317 | 	sasl.login.refresh.min.period.seconds = 60
2025-04-24 15:05:57.317 | 	sasl.login.refresh.window.factor = 0.8
2025-04-24 15:05:57.317 | 	sasl.login.refresh.window.jitter = 0.05
2025-04-24 15:05:57.317 | 	sasl.login.retry.backoff.max.ms = 10000
2025-04-24 15:05:57.317 | 	sasl.login.retry.backoff.ms = 100
2025-04-24 15:05:57.317 | 	sasl.mechanism = GSSAPI
2025-04-24 15:05:57.317 | 	sasl.oauthbearer.clock.skew.seconds = 30
2025-04-24 15:05:57.317 | 	sasl.oauthbearer.expected.audience = null
2025-04-24 15:05:57.317 | 	sasl.oauthbearer.expected.issuer = null
2025-04-24 15:05:57.317 | 	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
2025-04-24 15:05:57.317 | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
2025-04-24 15:05:57.317 | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
2025-04-24 15:05:57.317 | 	sasl.oauthbearer.jwks.endpoint.url = null
2025-04-24 15:05:57.317 | 	sasl.oauthbearer.scope.claim.name = scope
2025-04-24 15:05:57.317 | 	sasl.oauthbearer.sub.claim.name = sub
2025-04-24 15:05:57.317 | 	sasl.oauthbearer.token.endpoint.url = null
2025-04-24 15:05:57.317 | 	security.protocol = PLAINTEXT
2025-04-24 15:05:57.317 | 	security.providers = null
2025-04-24 15:05:57.317 | 	send.buffer.bytes = 131072
2025-04-24 15:05:57.317 | 	socket.connection.setup.timeout.max.ms = 30000
2025-04-24 15:05:57.317 | 	socket.connection.setup.timeout.ms = 10000
2025-04-24 15:05:57.317 | 	ssl.cipher.suites = null
2025-04-24 15:05:57.317 | 	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
2025-04-24 15:05:57.317 | 	ssl.endpoint.identification.algorithm = https
2025-04-24 15:05:57.317 | 	ssl.engine.factory.class = null
2025-04-24 15:05:57.317 | 	ssl.key.password = null
2025-04-24 15:05:57.317 | 	ssl.keymanager.algorithm = SunX509
2025-04-24 15:05:57.317 | 	ssl.keystore.certificate.chain = null
2025-04-24 15:05:57.317 | 	ssl.keystore.key = null
2025-04-24 15:05:57.317 | 	ssl.keystore.location = null
2025-04-24 15:05:57.317 | 	ssl.keystore.password = null
2025-04-24 15:05:57.317 | 	ssl.keystore.type = JKS
2025-04-24 15:05:57.317 | 	ssl.protocol = TLSv1.3
2025-04-24 15:05:57.317 | 	ssl.provider = null
2025-04-24 15:05:57.317 | 	ssl.secure.random.implementation = null
2025-04-24 15:05:57.317 | 	ssl.trustmanager.algorithm = PKIX
2025-04-24 15:05:57.317 | 	ssl.truststore.certificates = null
2025-04-24 15:05:57.317 | 	ssl.truststore.location = null
2025-04-24 15:05:57.317 | 	ssl.truststore.password = null
2025-04-24 15:05:57.317 | 	ssl.truststore.type = JKS
2025-04-24 15:05:57.317 | 
2025-04-24 15:05:57.322 | 25/04/24 06:05:57 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-ec737e79-bc80-4f79-9a41-d619dca08576/metadata using temp file file:/tmp/temporary-ec737e79-bc80-4f79-9a41-d619dca08576/.metadata.c0fe7363-5b2b-4fac-b1b2-0da087fa3fef.tmp
2025-04-24 15:05:57.335 | 25/04/24 06:05:57 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-ec737e79-bc80-4f79-9a41-d619dca08576/.metadata.c0fe7363-5b2b-4fac-b1b2-0da087fa3fef.tmp to file:/tmp/temporary-ec737e79-bc80-4f79-9a41-d619dca08576/metadata
2025-04-24 15:05:57.342 | 25/04/24 06:05:57 INFO MicroBatchExecution: Starting [id = c98466be-c74c-4620-a512-78fb48627126, runId = b5e1c45e-6ade-4009-a466-7e367ad05729]. Use file:/tmp/temporary-ec737e79-bc80-4f79-9a41-d619dca08576 to store the query checkpoint.
2025-04-24 15:05:57.344 | 25/04/24 06:05:57 INFO MicroBatchExecution: Reading table [org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@4d193687] from DataSourceV2 named 'kafka' [org.apache.spark.sql.kafka010.KafkaSourceProvider@203330cb]
2025-04-24 15:05:57.350 | 25/04/24 06:05:57 INFO OffsetSeqLog: BatchIds found from listing: 
2025-04-24 15:05:57.350 | 25/04/24 06:05:57 INFO OffsetSeqLog: BatchIds found from listing: 
2025-04-24 15:05:57.350 | 25/04/24 06:05:57 INFO MicroBatchExecution: Starting new streaming query.
2025-04-24 15:05:57.350 | 25/04/24 06:05:57 INFO MicroBatchExecution: Stream started from {}
2025-04-24 15:05:57.352 | 25/04/24 06:05:57 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-579966c4-93f8-4877-88e0-c9b4e4d718fa. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.
2025-04-24 15:05:57.352 | 25/04/24 06:05:57 INFO ResolveWriteToStream: Checkpoint root file:///tmp/temporary-579966c4-93f8-4877-88e0-c9b4e4d718fa resolved to file:/tmp/temporary-579966c4-93f8-4877-88e0-c9b4e4d718fa.
2025-04-24 15:05:57.352 | 25/04/24 06:05:57 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.
2025-04-24 15:05:57.356 | 25/04/24 06:05:57 INFO AdminClientConfig: AdminClientConfig values: 
2025-04-24 15:05:57.356 | 	auto.include.jmx.reporter = true
2025-04-24 15:05:57.356 | 	bootstrap.servers = [kafka:9092]
2025-04-24 15:05:57.356 | 	client.dns.lookup = use_all_dns_ips
2025-04-24 15:05:57.356 | 	client.id = 
2025-04-24 15:05:57.356 | 	connections.max.idle.ms = 300000
2025-04-24 15:05:57.356 | 	default.api.timeout.ms = 60000
2025-04-24 15:05:57.356 | 	metadata.max.age.ms = 300000
2025-04-24 15:05:57.356 | 	metric.reporters = []
2025-04-24 15:05:57.356 | 	metrics.num.samples = 2
2025-04-24 15:05:57.356 | 	metrics.recording.level = INFO
2025-04-24 15:05:57.356 | 	metrics.sample.window.ms = 30000
2025-04-24 15:05:57.356 | 	receive.buffer.bytes = 65536
2025-04-24 15:05:57.356 | 	reconnect.backoff.max.ms = 1000
2025-04-24 15:05:57.356 | 	reconnect.backoff.ms = 50
2025-04-24 15:05:57.356 | 	request.timeout.ms = 30000
2025-04-24 15:05:57.356 | 	retries = 2147483647
2025-04-24 15:05:57.356 | 	retry.backoff.ms = 100
2025-04-24 15:05:57.356 | 	sasl.client.callback.handler.class = null
2025-04-24 15:05:57.356 | 	sasl.jaas.config = null
2025-04-24 15:05:57.356 | 	sasl.kerberos.kinit.cmd = /usr/bin/kinit
2025-04-24 15:05:57.356 | 	sasl.kerberos.min.time.before.relogin = 60000
2025-04-24 15:05:57.356 | 	sasl.kerberos.service.name = null
2025-04-24 15:05:57.356 | 	sasl.kerberos.ticket.renew.jitter = 0.05
2025-04-24 15:05:57.356 | 	sasl.kerberos.ticket.renew.window.factor = 0.8
2025-04-24 15:05:57.356 | 	sasl.login.callback.handler.class = null
2025-04-24 15:05:57.356 | 	sasl.login.class = null
2025-04-24 15:05:57.356 | 	sasl.login.connect.timeout.ms = null
2025-04-24 15:05:57.356 | 	sasl.login.read.timeout.ms = null
2025-04-24 15:05:57.356 | 	sasl.login.refresh.buffer.seconds = 300
2025-04-24 15:05:57.356 | 	sasl.login.refresh.min.period.seconds = 60
2025-04-24 15:05:57.356 | 	sasl.login.refresh.window.factor = 0.8
2025-04-24 15:05:57.356 | 	sasl.login.refresh.window.jitter = 0.05
2025-04-24 15:05:57.356 | 	sasl.login.retry.backoff.max.ms = 10000
2025-04-24 15:05:57.356 | 	sasl.login.retry.backoff.ms = 100
2025-04-24 15:05:57.357 | 	sasl.mechanism = GSSAPI
2025-04-24 15:05:57.357 | 	sasl.oauthbearer.clock.skew.seconds = 30
2025-04-24 15:05:57.357 | 	sasl.oauthbearer.expected.audience = null
2025-04-24 15:05:57.357 | 	sasl.oauthbearer.expected.issuer = null
2025-04-24 15:05:57.357 | 	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
2025-04-24 15:05:57.357 | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
2025-04-24 15:05:57.357 | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
2025-04-24 15:05:57.357 | 	sasl.oauthbearer.jwks.endpoint.url = null
2025-04-24 15:05:57.357 | 	sasl.oauthbearer.scope.claim.name = scope
2025-04-24 15:05:57.357 | 	sasl.oauthbearer.sub.claim.name = sub
2025-04-24 15:05:57.357 | 	sasl.oauthbearer.token.endpoint.url = null
2025-04-24 15:05:57.357 | 	security.protocol = PLAINTEXT
2025-04-24 15:05:57.357 | 	security.providers = null
2025-04-24 15:05:57.357 | 	send.buffer.bytes = 131072
2025-04-24 15:05:57.357 | 	socket.connection.setup.timeout.max.ms = 30000
2025-04-24 15:05:57.357 | 	socket.connection.setup.timeout.ms = 10000
2025-04-24 15:05:57.357 | 	ssl.cipher.suites = null
2025-04-24 15:05:57.357 | 	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
2025-04-24 15:05:57.357 | 	ssl.endpoint.identification.algorithm = https
2025-04-24 15:05:57.357 | 	ssl.engine.factory.class = null
2025-04-24 15:05:57.357 | 	ssl.key.password = null
2025-04-24 15:05:57.357 | 	ssl.keymanager.algorithm = SunX509
2025-04-24 15:05:57.357 | 	ssl.keystore.certificate.chain = null
2025-04-24 15:05:57.357 | 	ssl.keystore.key = null
2025-04-24 15:05:57.357 | 	ssl.keystore.location = null
2025-04-24 15:05:57.357 | 	ssl.keystore.password = null
2025-04-24 15:05:57.357 | 	ssl.keystore.type = JKS
2025-04-24 15:05:57.357 | 	ssl.protocol = TLSv1.3
2025-04-24 15:05:57.357 | 	ssl.provider = null
2025-04-24 15:05:57.357 | 	ssl.secure.random.implementation = null
2025-04-24 15:05:57.357 | 	ssl.trustmanager.algorithm = PKIX
2025-04-24 15:05:57.357 | 	ssl.truststore.certificates = null
2025-04-24 15:05:57.357 | 	ssl.truststore.location = null
2025-04-24 15:05:57.357 | 	ssl.truststore.password = null
2025-04-24 15:05:57.357 | 	ssl.truststore.type = JKS
2025-04-24 15:05:57.357 | 
2025-04-24 15:05:57.360 | 25/04/24 06:05:57 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-579966c4-93f8-4877-88e0-c9b4e4d718fa/metadata using temp file file:/tmp/temporary-579966c4-93f8-4877-88e0-c9b4e4d718fa/.metadata.80ac434c-20d7-49f3-9a9d-87acd0117804.tmp
2025-04-24 15:05:57.375 | 25/04/24 06:05:57 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-579966c4-93f8-4877-88e0-c9b4e4d718fa/.metadata.80ac434c-20d7-49f3-9a9d-87acd0117804.tmp to file:/tmp/temporary-579966c4-93f8-4877-88e0-c9b4e4d718fa/metadata
2025-04-24 15:05:57.385 | 25/04/24 06:05:57 INFO MicroBatchExecution: Starting [id = bf093a8a-eda4-4fb0-8a5e-c7f83d067314, runId = eb9ca31a-970c-4955-9f6f-e162c04f7b99]. Use file:/tmp/temporary-579966c4-93f8-4877-88e0-c9b4e4d718fa to store the query checkpoint.
2025-04-24 15:05:57.386 | 25/04/24 06:05:57 INFO MicroBatchExecution: Reading table [org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@4d193687] from DataSourceV2 named 'kafka' [org.apache.spark.sql.kafka010.KafkaSourceProvider@203330cb]
2025-04-24 15:05:57.387 | 25/04/24 06:05:57 INFO OffsetSeqLog: BatchIds found from listing: 
2025-04-24 15:05:57.388 | 25/04/24 06:05:57 INFO OffsetSeqLog: BatchIds found from listing: 
2025-04-24 15:05:57.388 | 25/04/24 06:05:57 INFO MicroBatchExecution: Starting new streaming query.
2025-04-24 15:05:57.388 | 25/04/24 06:05:57 INFO MicroBatchExecution: Stream started from {}
2025-04-24 15:05:57.394 | 25/04/24 06:05:57 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-98aba8b4-e403-4f10-96a8-9c75af6e71ba. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.
2025-04-24 15:05:57.394 | 25/04/24 06:05:57 INFO ResolveWriteToStream: Checkpoint root file:///tmp/temporary-98aba8b4-e403-4f10-96a8-9c75af6e71ba resolved to file:/tmp/temporary-98aba8b4-e403-4f10-96a8-9c75af6e71ba.
2025-04-24 15:05:57.394 | 25/04/24 06:05:57 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.
2025-04-24 15:05:57.396 | 25/04/24 06:05:57 WARN AdminClientConfig: These configurations '[key.deserializer, value.deserializer, enable.auto.commit, max.poll.records, auto.offset.reset]' were supplied but are not used yet.
2025-04-24 15:05:57.396 | 25/04/24 06:05:57 WARN AdminClientConfig: These configurations '[key.deserializer, value.deserializer, enable.auto.commit, max.poll.records, auto.offset.reset]' were supplied but are not used yet.
2025-04-24 15:05:57.398 | 25/04/24 06:05:57 WARN AdminClientConfig: These configurations '[key.deserializer, value.deserializer, enable.auto.commit, max.poll.records, auto.offset.reset]' were supplied but are not used yet.
2025-04-24 15:05:57.398 | 25/04/24 06:05:57 WARN AdminClientConfig: These configurations '[key.deserializer, value.deserializer, enable.auto.commit, max.poll.records, auto.offset.reset]' were supplied but are not used yet.
2025-04-24 15:05:57.398 | 25/04/24 06:05:57 WARN AdminClientConfig: These configurations '[key.deserializer, value.deserializer, enable.auto.commit, max.poll.records, auto.offset.reset]' were supplied but are not used yet.
2025-04-24 15:05:57.398 | 25/04/24 06:05:57 WARN AdminClientConfig: These configurations '[key.deserializer, value.deserializer, enable.auto.commit, max.poll.records, auto.offset.reset]' were supplied but are not used yet.
2025-04-24 15:05:57.398 | 25/04/24 06:05:57 WARN AdminClientConfig: These configurations '[key.deserializer, value.deserializer, enable.auto.commit, max.poll.records, auto.offset.reset]' were supplied but are not used yet.
2025-04-24 15:05:57.399 | 25/04/24 06:05:57 WARN AdminClientConfig: These configurations '[key.deserializer, value.deserializer, enable.auto.commit, max.poll.records, auto.offset.reset]' were supplied but are not used yet.
2025-04-24 15:05:57.399 | 25/04/24 06:05:57 INFO AppInfoParser: Kafka version: 3.4.1
2025-04-24 15:05:57.399 | 25/04/24 06:05:57 INFO AppInfoParser: Kafka commitId: 8a516edc2755df89
2025-04-24 15:05:57.399 | 25/04/24 06:05:57 INFO AppInfoParser: Kafka startTimeMs: 1745474757396
2025-04-24 15:05:57.400 | 25/04/24 06:05:57 WARN AdminClientConfig: These configurations '[key.deserializer, value.deserializer, enable.auto.commit, max.poll.records, auto.offset.reset]' were supplied but are not used yet.
2025-04-24 15:05:57.401 | 25/04/24 06:05:57 INFO AppInfoParser: Kafka version: 3.4.1
2025-04-24 15:05:57.401 | 25/04/24 06:05:57 INFO AppInfoParser: Kafka commitId: 8a516edc2755df89
2025-04-24 15:05:57.401 | 25/04/24 06:05:57 INFO AppInfoParser: Kafka startTimeMs: 1745474757399
2025-04-24 15:05:57.401 | 25/04/24 06:05:57 INFO AppInfoParser: Kafka version: 3.4.1
2025-04-24 15:05:57.401 | 25/04/24 06:05:57 INFO AppInfoParser: Kafka commitId: 8a516edc2755df89
2025-04-24 15:05:57.401 | 25/04/24 06:05:57 INFO AppInfoParser: Kafka startTimeMs: 1745474757397
2025-04-24 15:05:57.401 | 25/04/24 06:05:57 INFO AppInfoParser: Kafka version: 3.4.1
2025-04-24 15:05:57.401 | 25/04/24 06:05:57 INFO AppInfoParser: Kafka commitId: 8a516edc2755df89
2025-04-24 15:05:57.401 | 25/04/24 06:05:57 INFO AppInfoParser: Kafka startTimeMs: 1745474757396
2025-04-24 15:05:57.401 | 25/04/24 06:05:57 INFO AppInfoParser: Kafka version: 3.4.1
2025-04-24 15:05:57.402 | 25/04/24 06:05:57 INFO AppInfoParser: Kafka commitId: 8a516edc2755df89
2025-04-24 15:05:57.402 | 25/04/24 06:05:57 INFO AppInfoParser: Kafka startTimeMs: 1745474757398
2025-04-24 15:05:57.402 | 25/04/24 06:05:57 INFO AppInfoParser: Kafka version: 3.4.1
2025-04-24 15:05:57.402 | 25/04/24 06:05:57 INFO AppInfoParser: Kafka commitId: 8a516edc2755df89
2025-04-24 15:05:57.402 | 25/04/24 06:05:57 INFO AppInfoParser: Kafka startTimeMs: 1745474757398
2025-04-24 15:05:57.402 | 25/04/24 06:05:57 INFO AppInfoParser: Kafka version: 3.4.1
2025-04-24 15:05:57.402 | 25/04/24 06:05:57 INFO AppInfoParser: Kafka commitId: 8a516edc2755df89
2025-04-24 15:05:57.403 | 25/04/24 06:05:57 INFO AppInfoParser: Kafka startTimeMs: 1745474757398
2025-04-24 15:05:57.403 | 25/04/24 06:05:57 INFO AppInfoParser: Kafka version: 3.4.1
2025-04-24 15:05:57.403 | 25/04/24 06:05:57 INFO AppInfoParser: Kafka commitId: 8a516edc2755df89
2025-04-24 15:05:57.403 | 25/04/24 06:05:57 INFO AppInfoParser: Kafka startTimeMs: 1745474757398
2025-04-24 15:05:57.403 | 25/04/24 06:05:57 INFO AppInfoParser: Kafka version: 3.4.1
2025-04-24 15:05:57.403 | 25/04/24 06:05:57 INFO AppInfoParser: Kafka commitId: 8a516edc2755df89
2025-04-24 15:05:57.403 | 25/04/24 06:05:57 INFO AppInfoParser: Kafka startTimeMs: 1745474757397
2025-04-24 15:05:57.408 | 25/04/24 06:05:57 INFO AdminClientConfig: AdminClientConfig values: 
2025-04-24 15:05:57.408 | 	auto.include.jmx.reporter = true
2025-04-24 15:05:57.408 | 	bootstrap.servers = [kafka:9092]
2025-04-24 15:05:57.408 | 	client.dns.lookup = use_all_dns_ips
2025-04-24 15:05:57.408 | 	client.id = 
2025-04-24 15:05:57.408 | 	connections.max.idle.ms = 300000
2025-04-24 15:05:57.408 | 	default.api.timeout.ms = 60000
2025-04-24 15:05:57.408 | 	metadata.max.age.ms = 300000
2025-04-24 15:05:57.408 | 	metric.reporters = []
2025-04-24 15:05:57.408 | 	metrics.num.samples = 2
2025-04-24 15:05:57.408 | 	metrics.recording.level = INFO
2025-04-24 15:05:57.408 | 	metrics.sample.window.ms = 30000
2025-04-24 15:05:57.408 | 	receive.buffer.bytes = 65536
2025-04-24 15:05:57.408 | 	reconnect.backoff.max.ms = 1000
2025-04-24 15:05:57.408 | 	reconnect.backoff.ms = 50
2025-04-24 15:05:57.408 | 	request.timeout.ms = 30000
2025-04-24 15:05:57.408 | 	retries = 2147483647
2025-04-24 15:05:57.408 | 	retry.backoff.ms = 100
2025-04-24 15:05:57.408 | 	sasl.client.callback.handler.class = null
2025-04-24 15:05:57.408 | 	sasl.jaas.config = null
2025-04-24 15:05:57.408 | 	sasl.kerberos.kinit.cmd = /usr/bin/kinit
2025-04-24 15:05:57.408 | 	sasl.kerberos.min.time.before.relogin = 60000
2025-04-24 15:05:57.408 | 	sasl.kerberos.service.name = null
2025-04-24 15:05:57.408 | 	sasl.kerberos.ticket.renew.jitter = 0.05
2025-04-24 15:05:57.408 | 	sasl.kerberos.ticket.renew.window.factor = 0.8
2025-04-24 15:05:57.408 | 	sasl.login.callback.handler.class = null
2025-04-24 15:05:57.408 | 	sasl.login.class = null
2025-04-24 15:05:57.408 | 	sasl.login.connect.timeout.ms = null
2025-04-24 15:05:57.408 | 	sasl.login.read.timeout.ms = null
2025-04-24 15:05:57.408 | 	sasl.login.refresh.buffer.seconds = 300
2025-04-24 15:05:57.408 | 	sasl.login.refresh.min.period.seconds = 60
2025-04-24 15:05:57.408 | 	sasl.login.refresh.window.factor = 0.8
2025-04-24 15:05:57.408 | 	sasl.login.refresh.window.jitter = 0.05
2025-04-24 15:05:57.408 | 	sasl.login.retry.backoff.max.ms = 10000
2025-04-24 15:05:57.408 | 	sasl.login.retry.backoff.ms = 100
2025-04-24 15:05:57.408 | 	sasl.mechanism = GSSAPI
2025-04-24 15:05:57.409 | 	sasl.oauthbearer.clock.skew.seconds = 30
2025-04-24 15:05:57.409 | 	sasl.oauthbearer.expected.audience = null
2025-04-24 15:05:57.409 | 	sasl.oauthbearer.expected.issuer = null
2025-04-24 15:05:57.409 | 	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
2025-04-24 15:05:57.409 | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
2025-04-24 15:05:57.409 | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
2025-04-24 15:05:57.409 | 	sasl.oauthbearer.jwks.endpoint.url = null
2025-04-24 15:05:57.409 | 	sasl.oauthbearer.scope.claim.name = scope
2025-04-24 15:05:57.409 | 	sasl.oauthbearer.sub.claim.name = sub
2025-04-24 15:05:57.409 | 	sasl.oauthbearer.token.endpoint.url = null
2025-04-24 15:05:57.409 | 	security.protocol = PLAINTEXT
2025-04-24 15:05:57.409 | 	security.providers = null
2025-04-24 15:05:57.409 | 	send.buffer.bytes = 131072
2025-04-24 15:05:57.409 | 	socket.connection.setup.timeout.max.ms = 30000
2025-04-24 15:05:57.409 | 	socket.connection.setup.timeout.ms = 10000
2025-04-24 15:05:57.409 | 	ssl.cipher.suites = null
2025-04-24 15:05:57.409 | 	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
2025-04-24 15:05:57.409 | 	ssl.endpoint.identification.algorithm = https
2025-04-24 15:05:57.409 | 	ssl.engine.factory.class = null
2025-04-24 15:05:57.409 | 	ssl.key.password = null
2025-04-24 15:05:57.409 | 	ssl.keymanager.algorithm = SunX509
2025-04-24 15:05:57.409 | 	ssl.keystore.certificate.chain = null
2025-04-24 15:05:57.409 | 	ssl.keystore.key = null
2025-04-24 15:05:57.409 | 	ssl.keystore.location = null
2025-04-24 15:05:57.409 | 	ssl.keystore.password = null
2025-04-24 15:05:57.409 | 	ssl.keystore.type = JKS
2025-04-24 15:05:57.409 | 	ssl.protocol = TLSv1.3
2025-04-24 15:05:57.409 | 	ssl.provider = null
2025-04-24 15:05:57.409 | 	ssl.secure.random.implementation = null
2025-04-24 15:05:57.409 | 	ssl.trustmanager.algorithm = PKIX
2025-04-24 15:05:57.409 | 	ssl.truststore.certificates = null
2025-04-24 15:05:57.409 | 	ssl.truststore.location = null
2025-04-24 15:05:57.409 | 	ssl.truststore.password = null
2025-04-24 15:05:57.409 | 	ssl.truststore.type = JKS
2025-04-24 15:05:57.409 | 
2025-04-24 15:05:57.409 | 25/04/24 06:05:57 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-98aba8b4-e403-4f10-96a8-9c75af6e71ba/metadata using temp file file:/tmp/temporary-98aba8b4-e403-4f10-96a8-9c75af6e71ba/.metadata.e05f02bd-ce75-4dff-a9bd-569e2a7622ca.tmp
2025-04-24 15:05:57.413 | 25/04/24 06:05:57 WARN AdminClientConfig: These configurations '[key.deserializer, value.deserializer, enable.auto.commit, max.poll.records, auto.offset.reset]' were supplied but are not used yet.
2025-04-24 15:05:57.413 | 25/04/24 06:05:57 INFO AppInfoParser: Kafka version: 3.4.1
2025-04-24 15:05:57.413 | 25/04/24 06:05:57 INFO AppInfoParser: Kafka commitId: 8a516edc2755df89
2025-04-24 15:05:57.413 | 25/04/24 06:05:57 INFO AppInfoParser: Kafka startTimeMs: 1745474757413
2025-04-24 15:05:57.427 | 25/04/24 06:05:57 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-98aba8b4-e403-4f10-96a8-9c75af6e71ba/.metadata.e05f02bd-ce75-4dff-a9bd-569e2a7622ca.tmp to file:/tmp/temporary-98aba8b4-e403-4f10-96a8-9c75af6e71ba/metadata
2025-04-24 15:05:57.435 | 25/04/24 06:05:57 INFO MicroBatchExecution: Starting [id = 2529966a-2e54-49cf-ab64-7ed6cd055e9a, runId = d016eef7-9012-44d2-a4b7-3a2e83ac142f]. Use file:/tmp/temporary-98aba8b4-e403-4f10-96a8-9c75af6e71ba to store the query checkpoint.
2025-04-24 15:05:57.436 | 25/04/24 06:05:57 INFO MicroBatchExecution: Reading table [org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@4d193687] from DataSourceV2 named 'kafka' [org.apache.spark.sql.kafka010.KafkaSourceProvider@203330cb]
2025-04-24 15:05:57.437 | 25/04/24 06:05:57 INFO OffsetSeqLog: BatchIds found from listing: 
2025-04-24 15:05:57.438 | 25/04/24 06:05:57 INFO OffsetSeqLog: BatchIds found from listing: 
2025-04-24 15:05:57.438 | 25/04/24 06:05:57 INFO MicroBatchExecution: Starting new streaming query.
2025-04-24 15:05:57.439 | 25/04/24 06:05:57 INFO MicroBatchExecution: Stream started from {}
2025-04-24 15:05:57.441 | 25/04/24 06:05:57 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-116fac01-2a6c-40a3-b256-01c68ac34ec9. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.
2025-04-24 15:05:57.442 | 25/04/24 06:05:57 INFO ResolveWriteToStream: Checkpoint root file:///tmp/temporary-116fac01-2a6c-40a3-b256-01c68ac34ec9 resolved to file:/tmp/temporary-116fac01-2a6c-40a3-b256-01c68ac34ec9.
2025-04-24 15:05:57.443 | 25/04/24 06:05:57 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.
2025-04-24 15:05:57.445 | 25/04/24 06:05:57 INFO AdminClientConfig: AdminClientConfig values: 
2025-04-24 15:05:57.445 | 	auto.include.jmx.reporter = true
2025-04-24 15:05:57.445 | 	bootstrap.servers = [kafka:9092]
2025-04-24 15:05:57.445 | 	client.dns.lookup = use_all_dns_ips
2025-04-24 15:05:57.445 | 	client.id = 
2025-04-24 15:05:57.445 | 	connections.max.idle.ms = 300000
2025-04-24 15:05:57.445 | 	default.api.timeout.ms = 60000
2025-04-24 15:05:57.445 | 	metadata.max.age.ms = 300000
2025-04-24 15:05:57.445 | 	metric.reporters = []
2025-04-24 15:05:57.445 | 	metrics.num.samples = 2
2025-04-24 15:05:57.445 | 	metrics.recording.level = INFO
2025-04-24 15:05:57.445 | 	metrics.sample.window.ms = 30000
2025-04-24 15:05:57.445 | 	receive.buffer.bytes = 65536
2025-04-24 15:05:57.445 | 	reconnect.backoff.max.ms = 1000
2025-04-24 15:05:57.445 | 	reconnect.backoff.ms = 50
2025-04-24 15:05:57.445 | 	request.timeout.ms = 30000
2025-04-24 15:05:57.445 | 	retries = 2147483647
2025-04-24 15:05:57.445 | 	retry.backoff.ms = 100
2025-04-24 15:05:57.445 | 	sasl.client.callback.handler.class = null
2025-04-24 15:05:57.445 | 	sasl.jaas.config = null
2025-04-24 15:05:57.445 | 	sasl.kerberos.kinit.cmd = /usr/bin/kinit
2025-04-24 15:05:57.445 | 	sasl.kerberos.min.time.before.relogin = 60000
2025-04-24 15:05:57.445 | 	sasl.kerberos.service.name = null
2025-04-24 15:05:57.445 | 	sasl.kerberos.ticket.renew.jitter = 0.05
2025-04-24 15:05:57.445 | 	sasl.kerberos.ticket.renew.window.factor = 0.8
2025-04-24 15:05:57.445 | 	sasl.login.callback.handler.class = null
2025-04-24 15:05:57.445 | 	sasl.login.class = null
2025-04-24 15:05:57.445 | 	sasl.login.connect.timeout.ms = null
2025-04-24 15:05:57.445 | 	sasl.login.read.timeout.ms = null
2025-04-24 15:05:57.445 | 	sasl.login.refresh.buffer.seconds = 300
2025-04-24 15:05:57.445 | 	sasl.login.refresh.min.period.seconds = 60
2025-04-24 15:05:57.445 | 	sasl.login.refresh.window.factor = 0.8
2025-04-24 15:05:57.445 | 	sasl.login.refresh.window.jitter = 0.05
2025-04-24 15:05:57.445 | 	sasl.login.retry.backoff.max.ms = 10000
2025-04-24 15:05:57.445 | 	sasl.login.retry.backoff.ms = 100
2025-04-24 15:05:57.445 | 	sasl.mechanism = GSSAPI
2025-04-24 15:05:57.445 | 	sasl.oauthbearer.clock.skew.seconds = 30
2025-04-24 15:05:57.445 | 	sasl.oauthbearer.expected.audience = null
2025-04-24 15:05:57.445 | 	sasl.oauthbearer.expected.issuer = null
2025-04-24 15:05:57.445 | 	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
2025-04-24 15:05:57.445 | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
2025-04-24 15:05:57.445 | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
2025-04-24 15:05:57.445 | 	sasl.oauthbearer.jwks.endpoint.url = null
2025-04-24 15:05:57.445 | 	sasl.oauthbearer.scope.claim.name = scope
2025-04-24 15:05:57.445 | 	sasl.oauthbearer.sub.claim.name = sub
2025-04-24 15:05:57.445 | 	sasl.oauthbearer.token.endpoint.url = null
2025-04-24 15:05:57.445 | 	security.protocol = PLAINTEXT
2025-04-24 15:05:57.445 | 	security.providers = null
2025-04-24 15:05:57.445 | 	send.buffer.bytes = 131072
2025-04-24 15:05:57.445 | 	socket.connection.setup.timeout.max.ms = 30000
2025-04-24 15:05:57.445 | 	socket.connection.setup.timeout.ms = 10000
2025-04-24 15:05:57.445 | 	ssl.cipher.suites = null
2025-04-24 15:05:57.445 | 	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
2025-04-24 15:05:57.445 | 	ssl.endpoint.identification.algorithm = https
2025-04-24 15:05:57.445 | 	ssl.engine.factory.class = null
2025-04-24 15:05:57.445 | 	ssl.key.password = null
2025-04-24 15:05:57.445 | 	ssl.keymanager.algorithm = SunX509
2025-04-24 15:05:57.445 | 	ssl.keystore.certificate.chain = null
2025-04-24 15:05:57.445 | 	ssl.keystore.key = null
2025-04-24 15:05:57.445 | 	ssl.keystore.location = null
2025-04-24 15:05:57.445 | 	ssl.keystore.password = null
2025-04-24 15:05:57.445 | 	ssl.keystore.type = JKS
2025-04-24 15:05:57.445 | 	ssl.protocol = TLSv1.3
2025-04-24 15:05:57.445 | 	ssl.provider = null
2025-04-24 15:05:57.445 | 	ssl.secure.random.implementation = null
2025-04-24 15:05:57.445 | 	ssl.trustmanager.algorithm = PKIX
2025-04-24 15:05:57.445 | 	ssl.truststore.certificates = null
2025-04-24 15:05:57.445 | 	ssl.truststore.location = null
2025-04-24 15:05:57.445 | 	ssl.truststore.password = null
2025-04-24 15:05:57.445 | 	ssl.truststore.type = JKS
2025-04-24 15:05:57.445 | 
2025-04-24 15:05:57.447 | 25/04/24 06:05:57 WARN AdminClientConfig: These configurations '[key.deserializer, value.deserializer, enable.auto.commit, max.poll.records, auto.offset.reset]' were supplied but are not used yet.
2025-04-24 15:05:57.447 | 25/04/24 06:05:57 INFO AppInfoParser: Kafka version: 3.4.1
2025-04-24 15:05:57.447 | 25/04/24 06:05:57 INFO AppInfoParser: Kafka commitId: 8a516edc2755df89
2025-04-24 15:05:57.447 | 25/04/24 06:05:57 INFO AppInfoParser: Kafka startTimeMs: 1745474757447
2025-04-24 15:05:57.452 | 25/04/24 06:05:57 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-116fac01-2a6c-40a3-b256-01c68ac34ec9/metadata using temp file file:/tmp/temporary-116fac01-2a6c-40a3-b256-01c68ac34ec9/.metadata.a75f79d3-765f-42ce-b2b2-6a5314a928bc.tmp
2025-04-24 15:05:57.466 | 25/04/24 06:05:57 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-116fac01-2a6c-40a3-b256-01c68ac34ec9/.metadata.a75f79d3-765f-42ce-b2b2-6a5314a928bc.tmp to file:/tmp/temporary-116fac01-2a6c-40a3-b256-01c68ac34ec9/metadata
2025-04-24 15:05:57.473 | 25/04/24 06:05:57 INFO MicroBatchExecution: Starting [id = ed63a3b3-8cee-43b3-aa93-58cdd8184c27, runId = 41d2a38e-8435-45c5-ad6e-8ad5b53bde17]. Use file:/tmp/temporary-116fac01-2a6c-40a3-b256-01c68ac34ec9 to store the query checkpoint.
2025-04-24 15:05:57.474 | 25/04/24 06:05:57 INFO MicroBatchExecution: Reading table [org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@4d193687] from DataSourceV2 named 'kafka' [org.apache.spark.sql.kafka010.KafkaSourceProvider@203330cb]
2025-04-24 15:05:57.474 | [✅ DEBUG] dim_time 스키마
2025-04-24 15:05:57.476 | root
2025-04-24 15:05:57.476 |  |-- timestamp: timestamp (nullable = true)
2025-04-24 15:05:57.476 |  |-- user_id: string (nullable = true)
2025-04-24 15:05:57.476 |  |-- amount: integer (nullable = true)
2025-04-24 15:05:57.476 |  |-- level: string (nullable = true)
2025-04-24 15:05:57.476 |  |-- year: integer (nullable = true)
2025-04-24 15:05:57.476 |  |-- month: integer (nullable = true)
2025-04-24 15:05:57.476 |  |-- day: integer (nullable = true)
2025-04-24 15:05:57.476 |  |-- hour: integer (nullable = true)
2025-04-24 15:05:57.476 |  |-- minute: integer (nullable = true)
2025-04-24 15:05:57.476 | 
2025-04-24 15:05:57.478 | 25/04/24 06:05:57 INFO OffsetSeqLog: BatchIds found from listing: 
2025-04-24 15:05:57.478 | 25/04/24 06:05:57 INFO OffsetSeqLog: BatchIds found from listing: 
2025-04-24 15:05:57.478 | 25/04/24 06:05:57 INFO MicroBatchExecution: Starting new streaming query.
2025-04-24 15:05:57.478 | 25/04/24 06:05:57 INFO MicroBatchExecution: Stream started from {}
2025-04-24 15:05:57.485 | 25/04/24 06:05:57 INFO AdminClientConfig: AdminClientConfig values: 
2025-04-24 15:05:57.485 | 	auto.include.jmx.reporter = true
2025-04-24 15:05:57.485 | 	bootstrap.servers = [kafka:9092]
2025-04-24 15:05:57.485 | 	client.dns.lookup = use_all_dns_ips
2025-04-24 15:05:57.485 | 	client.id = 
2025-04-24 15:05:57.485 | 	connections.max.idle.ms = 300000
2025-04-24 15:05:57.485 | 	default.api.timeout.ms = 60000
2025-04-24 15:05:57.485 | 	metadata.max.age.ms = 300000
2025-04-24 15:05:57.485 | 	metric.reporters = []
2025-04-24 15:05:57.485 | 	metrics.num.samples = 2
2025-04-24 15:05:57.485 | 	metrics.recording.level = INFO
2025-04-24 15:05:57.485 | 	metrics.sample.window.ms = 30000
2025-04-24 15:05:57.485 | 	receive.buffer.bytes = 65536
2025-04-24 15:05:57.485 | 	reconnect.backoff.max.ms = 1000
2025-04-24 15:05:57.485 | 	reconnect.backoff.ms = 50
2025-04-24 15:05:57.485 | 	request.timeout.ms = 30000
2025-04-24 15:05:57.485 | 	retries = 2147483647
2025-04-24 15:05:57.485 | 	retry.backoff.ms = 100
2025-04-24 15:05:57.485 | 	sasl.client.callback.handler.class = null
2025-04-24 15:05:57.485 | 	sasl.jaas.config = null
2025-04-24 15:05:57.485 | 	sasl.kerberos.kinit.cmd = /usr/bin/kinit
2025-04-24 15:05:57.485 | 	sasl.kerberos.min.time.before.relogin = 60000
2025-04-24 15:05:57.485 | 	sasl.kerberos.service.name = null
2025-04-24 15:05:57.485 | 	sasl.kerberos.ticket.renew.jitter = 0.05
2025-04-24 15:05:57.485 | 	sasl.kerberos.ticket.renew.window.factor = 0.8
2025-04-24 15:05:57.485 | 	sasl.login.callback.handler.class = null
2025-04-24 15:05:57.485 | 	sasl.login.class = null
2025-04-24 15:05:57.485 | 	sasl.login.connect.timeout.ms = null
2025-04-24 15:05:57.485 | 	sasl.login.read.timeout.ms = null
2025-04-24 15:05:57.485 | 	sasl.login.refresh.buffer.seconds = 300
2025-04-24 15:05:57.485 | 	sasl.login.refresh.min.period.seconds = 60
2025-04-24 15:05:57.485 | 	sasl.login.refresh.window.factor = 0.8
2025-04-24 15:05:57.485 | 	sasl.login.refresh.window.jitter = 0.05
2025-04-24 15:05:57.485 | 	sasl.login.retry.backoff.max.ms = 10000
2025-04-24 15:05:57.485 | 	sasl.login.retry.backoff.ms = 100
2025-04-24 15:05:57.485 | 	sasl.mechanism = GSSAPI
2025-04-24 15:05:57.485 | 	sasl.oauthbearer.clock.skew.seconds = 30
2025-04-24 15:05:57.485 | 	sasl.oauthbearer.expected.audience = null
2025-04-24 15:05:57.485 | 	sasl.oauthbearer.expected.issuer = null
2025-04-24 15:05:57.485 | 	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
2025-04-24 15:05:57.485 | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
2025-04-24 15:05:57.485 | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
2025-04-24 15:05:57.485 | 	sasl.oauthbearer.jwks.endpoint.url = null
2025-04-24 15:05:57.485 | 	sasl.oauthbearer.scope.claim.name = scope
2025-04-24 15:05:57.485 | 	sasl.oauthbearer.sub.claim.name = sub
2025-04-24 15:05:57.485 | 	sasl.oauthbearer.token.endpoint.url = null
2025-04-24 15:05:57.485 | 	security.protocol = PLAINTEXT
2025-04-24 15:05:57.485 | 	security.providers = null
2025-04-24 15:05:57.485 | 	send.buffer.bytes = 131072
2025-04-24 15:05:57.485 | 	socket.connection.setup.timeout.max.ms = 30000
2025-04-24 15:05:57.485 | 	socket.connection.setup.timeout.ms = 10000
2025-04-24 15:05:57.485 | 	ssl.cipher.suites = null
2025-04-24 15:05:57.485 | 	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
2025-04-24 15:05:57.485 | 	ssl.endpoint.identification.algorithm = https
2025-04-24 15:05:57.485 | 	ssl.engine.factory.class = null
2025-04-24 15:05:57.485 | 	ssl.key.password = null
2025-04-24 15:05:57.485 | 	ssl.keymanager.algorithm = SunX509
2025-04-24 15:05:57.485 | 	ssl.keystore.certificate.chain = null
2025-04-24 15:05:57.485 | 	ssl.keystore.key = null
2025-04-24 15:05:57.485 | 	ssl.keystore.location = null
2025-04-24 15:05:57.485 | 	ssl.keystore.password = null
2025-04-24 15:05:57.485 | 	ssl.keystore.type = JKS
2025-04-24 15:05:57.485 | 	ssl.protocol = TLSv1.3
2025-04-24 15:05:57.485 | 	ssl.provider = null
2025-04-24 15:05:57.485 | 	ssl.secure.random.implementation = null
2025-04-24 15:05:57.485 | 	ssl.trustmanager.algorithm = PKIX
2025-04-24 15:05:57.485 | 	ssl.truststore.certificates = null
2025-04-24 15:05:57.485 | 	ssl.truststore.location = null
2025-04-24 15:05:57.485 | 	ssl.truststore.password = null
2025-04-24 15:05:57.485 | 	ssl.truststore.type = JKS
2025-04-24 15:05:57.485 | 
2025-04-24 15:05:57.486 | 25/04/24 06:05:57 WARN AdminClientConfig: These configurations '[key.deserializer, value.deserializer, enable.auto.commit, max.poll.records, auto.offset.reset]' were supplied but are not used yet.
2025-04-24 15:05:57.487 | 25/04/24 06:05:57 INFO AppInfoParser: Kafka version: 3.4.1
2025-04-24 15:05:57.487 | 25/04/24 06:05:57 INFO AppInfoParser: Kafka commitId: 8a516edc2755df89
2025-04-24 15:05:57.487 | 25/04/24 06:05:57 INFO AppInfoParser: Kafka startTimeMs: 1745474757486
2025-04-24 15:05:57.655 | 25/04/24 06:05:57 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-6b4dd807-0b15-4df4-b97a-87fbdf0b3438/sources/0/0 using temp file file:/tmp/temporary-6b4dd807-0b15-4df4-b97a-87fbdf0b3438/sources/0/.0.bf8c617b-c994-45fc-ae64-dfd215beb7b8.tmp
2025-04-24 15:05:57.655 | 25/04/24 06:05:57 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-ef85e29e-f0bd-4d21-8c8c-96b8720dae2d/sources/0/0 using temp file file:/tmp/temporary-ef85e29e-f0bd-4d21-8c8c-96b8720dae2d/sources/0/.0.b93c586c-8c00-4976-8434-efd249d63835.tmp
2025-04-24 15:05:57.655 | 25/04/24 06:05:57 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-c78571f4-8fa3-463d-a680-ad63781afb6f/sources/0/0 using temp file file:/tmp/temporary-c78571f4-8fa3-463d-a680-ad63781afb6f/sources/0/.0.fc899bbc-defd-43c3-afc5-497eeb553e62.tmp
2025-04-24 15:05:57.655 | 25/04/24 06:05:57 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-579966c4-93f8-4877-88e0-c9b4e4d718fa/sources/0/0 using temp file file:/tmp/temporary-579966c4-93f8-4877-88e0-c9b4e4d718fa/sources/0/.0.e6b94430-3ae1-40e4-9b5f-e26bec7d5e19.tmp
2025-04-24 15:05:57.655 | 25/04/24 06:05:57 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-18018b60-cd91-4654-b55d-89552988ee35/sources/0/0 using temp file file:/tmp/temporary-18018b60-cd91-4654-b55d-89552988ee35/sources/0/.0.beb02f30-8333-4597-add2-c85a8d3629cd.tmp
2025-04-24 15:05:57.655 | 25/04/24 06:05:57 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-116fac01-2a6c-40a3-b256-01c68ac34ec9/sources/0/0 using temp file file:/tmp/temporary-116fac01-2a6c-40a3-b256-01c68ac34ec9/sources/0/.0.93439314-ed05-4172-9428-0664c29875e3.tmp
2025-04-24 15:05:57.672 | 25/04/24 06:05:57 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-c78571f4-8fa3-463d-a680-ad63781afb6f/sources/0/.0.fc899bbc-defd-43c3-afc5-497eeb553e62.tmp to file:/tmp/temporary-c78571f4-8fa3-463d-a680-ad63781afb6f/sources/0/0
2025-04-24 15:05:57.673 | 25/04/24 06:05:57 INFO KafkaMicroBatchStream: Initial offsets: {"notify_topic":{"0":53380},"inventory_topic":{"0":53122},"auth_topic":{"0":52087},"order_topic":{"0":52545},"etc_topic":{"0":53565},"payment_topic":{"0":53015}}
2025-04-24 15:05:57.673 | 25/04/24 06:05:57 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-6b4dd807-0b15-4df4-b97a-87fbdf0b3438/sources/0/.0.bf8c617b-c994-45fc-ae64-dfd215beb7b8.tmp to file:/tmp/temporary-6b4dd807-0b15-4df4-b97a-87fbdf0b3438/sources/0/0
2025-04-24 15:05:57.673 | 25/04/24 06:05:57 INFO KafkaMicroBatchStream: Initial offsets: {"notify_topic":{"0":53380},"inventory_topic":{"0":53122},"auth_topic":{"0":52087},"order_topic":{"0":52545},"etc_topic":{"0":53565},"payment_topic":{"0":53015}}
2025-04-24 15:05:57.673 | 25/04/24 06:05:57 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-ef85e29e-f0bd-4d21-8c8c-96b8720dae2d/sources/0/.0.b93c586c-8c00-4976-8434-efd249d63835.tmp to file:/tmp/temporary-ef85e29e-f0bd-4d21-8c8c-96b8720dae2d/sources/0/0
2025-04-24 15:05:57.673 | 25/04/24 06:05:57 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-579966c4-93f8-4877-88e0-c9b4e4d718fa/sources/0/.0.e6b94430-3ae1-40e4-9b5f-e26bec7d5e19.tmp to file:/tmp/temporary-579966c4-93f8-4877-88e0-c9b4e4d718fa/sources/0/0
2025-04-24 15:05:57.673 | 25/04/24 06:05:57 INFO KafkaMicroBatchStream: Initial offsets: {"notify_topic":{"0":53380},"inventory_topic":{"0":53122},"auth_topic":{"0":52087},"order_topic":{"0":52545},"etc_topic":{"0":53565},"payment_topic":{"0":53015}}
2025-04-24 15:05:57.673 | 25/04/24 06:05:57 INFO KafkaMicroBatchStream: Initial offsets: {"notify_topic":{"0":53380},"inventory_topic":{"0":53122},"auth_topic":{"0":52087},"order_topic":{"0":52545},"etc_topic":{"0":53565},"payment_topic":{"0":53015}}
2025-04-24 15:05:57.674 | 25/04/24 06:05:57 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-116fac01-2a6c-40a3-b256-01c68ac34ec9/sources/0/.0.93439314-ed05-4172-9428-0664c29875e3.tmp to file:/tmp/temporary-116fac01-2a6c-40a3-b256-01c68ac34ec9/sources/0/0
2025-04-24 15:05:57.674 | 25/04/24 06:05:57 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-18018b60-cd91-4654-b55d-89552988ee35/sources/0/.0.beb02f30-8333-4597-add2-c85a8d3629cd.tmp to file:/tmp/temporary-18018b60-cd91-4654-b55d-89552988ee35/sources/0/0
2025-04-24 15:05:57.674 | 25/04/24 06:05:57 INFO KafkaMicroBatchStream: Initial offsets: {"notify_topic":{"0":53380},"inventory_topic":{"0":53122},"auth_topic":{"0":52087},"order_topic":{"0":52545},"etc_topic":{"0":53565},"payment_topic":{"0":53015}}
2025-04-24 15:05:57.674 | 25/04/24 06:05:57 INFO KafkaMicroBatchStream: Initial offsets: {"notify_topic":{"0":53380},"inventory_topic":{"0":53122},"auth_topic":{"0":52087},"order_topic":{"0":52545},"etc_topic":{"0":53565},"payment_topic":{"0":53015}}
2025-04-24 15:05:57.689 | 25/04/24 06:05:57 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-116fac01-2a6c-40a3-b256-01c68ac34ec9/offsets/0 using temp file file:/tmp/temporary-116fac01-2a6c-40a3-b256-01c68ac34ec9/offsets/.0.d3e51cac-0f1b-4137-9e1a-29c5e8ecbf89.tmp
2025-04-24 15:05:57.691 | 25/04/24 06:05:57 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-579966c4-93f8-4877-88e0-c9b4e4d718fa/offsets/0 using temp file file:/tmp/temporary-579966c4-93f8-4877-88e0-c9b4e4d718fa/offsets/.0.247df612-cc72-4a0c-8bc9-1922409a5662.tmp
2025-04-24 15:05:57.692 | 25/04/24 06:05:57 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-c78571f4-8fa3-463d-a680-ad63781afb6f/offsets/0 using temp file file:/tmp/temporary-c78571f4-8fa3-463d-a680-ad63781afb6f/offsets/.0.494c0b53-8068-4e2c-ab8a-8cfd89c8e455.tmp
2025-04-24 15:05:57.694 | 25/04/24 06:05:57 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-6b4dd807-0b15-4df4-b97a-87fbdf0b3438/offsets/0 using temp file file:/tmp/temporary-6b4dd807-0b15-4df4-b97a-87fbdf0b3438/offsets/.0.353ed571-f7e4-4605-8aec-beb833eb64a7.tmp
2025-04-24 15:05:57.696 | 25/04/24 06:05:57 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-ef85e29e-f0bd-4d21-8c8c-96b8720dae2d/offsets/0 using temp file file:/tmp/temporary-ef85e29e-f0bd-4d21-8c8c-96b8720dae2d/offsets/.0.0b630e7f-2d29-4b27-8aca-346b30de4949.tmp
2025-04-24 15:05:57.696 | 25/04/24 06:05:57 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-18018b60-cd91-4654-b55d-89552988ee35/offsets/0 using temp file file:/tmp/temporary-18018b60-cd91-4654-b55d-89552988ee35/offsets/.0.b9b6f064-6e99-4e98-bbe8-afcce0a1f0ca.tmp
2025-04-24 15:05:57.716 | 25/04/24 06:05:57 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-ef85e29e-f0bd-4d21-8c8c-96b8720dae2d/offsets/.0.0b630e7f-2d29-4b27-8aca-346b30de4949.tmp to file:/tmp/temporary-ef85e29e-f0bd-4d21-8c8c-96b8720dae2d/offsets/0
2025-04-24 15:05:57.716 | 25/04/24 06:05:57 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-c78571f4-8fa3-463d-a680-ad63781afb6f/offsets/.0.494c0b53-8068-4e2c-ab8a-8cfd89c8e455.tmp to file:/tmp/temporary-c78571f4-8fa3-463d-a680-ad63781afb6f/offsets/0
2025-04-24 15:05:57.716 | 25/04/24 06:05:57 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-6b4dd807-0b15-4df4-b97a-87fbdf0b3438/offsets/.0.353ed571-f7e4-4605-8aec-beb833eb64a7.tmp to file:/tmp/temporary-6b4dd807-0b15-4df4-b97a-87fbdf0b3438/offsets/0
2025-04-24 15:05:57.716 | 25/04/24 06:05:57 INFO MicroBatchExecution: Committed offsets for batch 0. Metadata OffsetSeqMetadata(0,1745474757688,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.optimizer.pruneFiltersCanPruneStreamingSubplan -> false, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
2025-04-24 15:05:57.716 | 25/04/24 06:05:57 INFO MicroBatchExecution: Committed offsets for batch 0. Metadata OffsetSeqMetadata(0,1745474757689,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.optimizer.pruneFiltersCanPruneStreamingSubplan -> false, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
2025-04-24 15:05:57.716 | 25/04/24 06:05:57 INFO MicroBatchExecution: Committed offsets for batch 0. Metadata OffsetSeqMetadata(0,1745474757687,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.optimizer.pruneFiltersCanPruneStreamingSubplan -> false, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
2025-04-24 15:05:57.716 | 25/04/24 06:05:57 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-18018b60-cd91-4654-b55d-89552988ee35/offsets/.0.b9b6f064-6e99-4e98-bbe8-afcce0a1f0ca.tmp to file:/tmp/temporary-18018b60-cd91-4654-b55d-89552988ee35/offsets/0
2025-04-24 15:05:57.716 | 25/04/24 06:05:57 INFO MicroBatchExecution: Committed offsets for batch 0. Metadata OffsetSeqMetadata(0,1745474757689,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.optimizer.pruneFiltersCanPruneStreamingSubplan -> false, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
2025-04-24 15:05:57.717 | 25/04/24 06:05:57 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-579966c4-93f8-4877-88e0-c9b4e4d718fa/offsets/.0.247df612-cc72-4a0c-8bc9-1922409a5662.tmp to file:/tmp/temporary-579966c4-93f8-4877-88e0-c9b4e4d718fa/offsets/0
2025-04-24 15:05:57.717 | 25/04/24 06:05:57 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-116fac01-2a6c-40a3-b256-01c68ac34ec9/offsets/.0.d3e51cac-0f1b-4137-9e1a-29c5e8ecbf89.tmp to file:/tmp/temporary-116fac01-2a6c-40a3-b256-01c68ac34ec9/offsets/0
2025-04-24 15:05:57.717 | 25/04/24 06:05:57 INFO MicroBatchExecution: Committed offsets for batch 0. Metadata OffsetSeqMetadata(0,1745474757687,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.optimizer.pruneFiltersCanPruneStreamingSubplan -> false, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
2025-04-24 15:05:57.717 | 25/04/24 06:05:57 INFO MicroBatchExecution: Committed offsets for batch 0. Metadata OffsetSeqMetadata(0,1745474757683,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.optimizer.pruneFiltersCanPruneStreamingSubplan -> false, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
2025-04-24 15:05:57.735 | 25/04/24 06:05:57 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-2cacd18c-322e-4cfa-8e0d-c1008c859651/sources/0/0 using temp file file:/tmp/temporary-2cacd18c-322e-4cfa-8e0d-c1008c859651/sources/0/.0.078eb155-a908-446c-b090-09cf8c08cf9e.tmp
2025-04-24 15:05:57.735 | 25/04/24 06:05:57 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-98aba8b4-e403-4f10-96a8-9c75af6e71ba/sources/0/0 using temp file file:/tmp/temporary-98aba8b4-e403-4f10-96a8-9c75af6e71ba/sources/0/.0.f9fcd845-ed13-45e6-abff-5b5de08a8389.tmp
2025-04-24 15:05:57.737 | 25/04/24 06:05:57 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-33d80dac-a68a-4d9e-9955-2288fc90154c/sources/0/0 using temp file file:/tmp/temporary-33d80dac-a68a-4d9e-9955-2288fc90154c/sources/0/.0.1ae765f2-6c09-459e-b90e-43577f2010ac.tmp
2025-04-24 15:05:57.744 | 25/04/24 06:05:57 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-2ce6649d-5b58-4ef3-98a8-fb21f300afef/sources/0/0 using temp file file:/tmp/temporary-2ce6649d-5b58-4ef3-98a8-fb21f300afef/sources/0/.0.20a63a54-a781-4434-8ee1-3a494d8d75b0.tmp
2025-04-24 15:05:57.746 | 25/04/24 06:05:57 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-ec737e79-bc80-4f79-9a41-d619dca08576/sources/0/0 using temp file file:/tmp/temporary-ec737e79-bc80-4f79-9a41-d619dca08576/sources/0/.0.eb9f98c5-5a33-4aaf-9888-c0d1f6a44eb1.tmp
2025-04-24 15:05:57.750 | 25/04/24 06:05:57 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-3c1df5f2-e1da-4beb-8c35-3451ca7efd9d/sources/0/0 using temp file file:/tmp/temporary-3c1df5f2-e1da-4beb-8c35-3451ca7efd9d/sources/0/.0.9ef7c384-90ab-4752-bafb-2b7f04884efa.tmp
2025-04-24 15:05:57.764 | 25/04/24 06:05:57 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-2cacd18c-322e-4cfa-8e0d-c1008c859651/sources/0/.0.078eb155-a908-446c-b090-09cf8c08cf9e.tmp to file:/tmp/temporary-2cacd18c-322e-4cfa-8e0d-c1008c859651/sources/0/0
2025-04-24 15:05:57.764 | 25/04/24 06:05:57 INFO KafkaMicroBatchStream: Initial offsets: {"notify_topic":{"0":53380},"inventory_topic":{"0":53122},"auth_topic":{"0":52087},"order_topic":{"0":52545},"etc_topic":{"0":53565},"payment_topic":{"0":53015}}
2025-04-24 15:05:57.773 | 25/04/24 06:05:57 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-ec737e79-bc80-4f79-9a41-d619dca08576/sources/0/.0.eb9f98c5-5a33-4aaf-9888-c0d1f6a44eb1.tmp to file:/tmp/temporary-ec737e79-bc80-4f79-9a41-d619dca08576/sources/0/0
2025-04-24 15:05:57.773 | 25/04/24 06:05:57 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-98aba8b4-e403-4f10-96a8-9c75af6e71ba/sources/0/.0.f9fcd845-ed13-45e6-abff-5b5de08a8389.tmp to file:/tmp/temporary-98aba8b4-e403-4f10-96a8-9c75af6e71ba/sources/0/0
2025-04-24 15:05:57.773 | 25/04/24 06:05:57 INFO KafkaMicroBatchStream: Initial offsets: {"notify_topic":{"0":53380},"inventory_topic":{"0":53122},"auth_topic":{"0":52087},"order_topic":{"0":52545},"etc_topic":{"0":53565},"payment_topic":{"0":53015}}
2025-04-24 15:05:57.773 | 25/04/24 06:05:57 INFO KafkaMicroBatchStream: Initial offsets: {"notify_topic":{"0":53380},"inventory_topic":{"0":53122},"auth_topic":{"0":52087},"order_topic":{"0":52545},"etc_topic":{"0":53565},"payment_topic":{"0":53015}}
2025-04-24 15:05:57.774 | 25/04/24 06:05:57 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-2ce6649d-5b58-4ef3-98a8-fb21f300afef/sources/0/.0.20a63a54-a781-4434-8ee1-3a494d8d75b0.tmp to file:/tmp/temporary-2ce6649d-5b58-4ef3-98a8-fb21f300afef/sources/0/0
2025-04-24 15:05:57.774 | 25/04/24 06:05:57 INFO KafkaMicroBatchStream: Initial offsets: {"notify_topic":{"0":53380},"inventory_topic":{"0":53122},"auth_topic":{"0":52087},"order_topic":{"0":52545},"etc_topic":{"0":53565},"payment_topic":{"0":53015}}
2025-04-24 15:05:57.778 | 25/04/24 06:05:57 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-33d80dac-a68a-4d9e-9955-2288fc90154c/sources/0/.0.1ae765f2-6c09-459e-b90e-43577f2010ac.tmp to file:/tmp/temporary-33d80dac-a68a-4d9e-9955-2288fc90154c/sources/0/0
2025-04-24 15:05:57.778 | 25/04/24 06:05:57 INFO KafkaMicroBatchStream: Initial offsets: {"notify_topic":{"0":53380},"inventory_topic":{"0":53122},"auth_topic":{"0":52087},"order_topic":{"0":52545},"etc_topic":{"0":53565},"payment_topic":{"0":53015}}
2025-04-24 15:05:57.780 | 25/04/24 06:05:57 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-3c1df5f2-e1da-4beb-8c35-3451ca7efd9d/sources/0/.0.9ef7c384-90ab-4752-bafb-2b7f04884efa.tmp to file:/tmp/temporary-3c1df5f2-e1da-4beb-8c35-3451ca7efd9d/sources/0/0
2025-04-24 15:05:57.780 | 25/04/24 06:05:57 INFO KafkaMicroBatchStream: Initial offsets: {"notify_topic":{"0":53380},"inventory_topic":{"0":53122},"auth_topic":{"0":52087},"order_topic":{"0":52545},"etc_topic":{"0":53565},"payment_topic":{"0":53015}}
2025-04-24 15:05:57.784 | 25/04/24 06:05:57 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-2cacd18c-322e-4cfa-8e0d-c1008c859651/offsets/0 using temp file file:/tmp/temporary-2cacd18c-322e-4cfa-8e0d-c1008c859651/offsets/.0.7a185ab2-f0ff-4285-bae7-511b0c0602e8.tmp
2025-04-24 15:05:57.785 | 25/04/24 06:05:57 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-2ce6649d-5b58-4ef3-98a8-fb21f300afef/offsets/0 using temp file file:/tmp/temporary-2ce6649d-5b58-4ef3-98a8-fb21f300afef/offsets/.0.6381b97e-4d7f-4c8b-b65a-bb1e7baf2b3b.tmp
2025-04-24 15:05:57.787 | 25/04/24 06:05:57 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-ec737e79-bc80-4f79-9a41-d619dca08576/offsets/0 using temp file file:/tmp/temporary-ec737e79-bc80-4f79-9a41-d619dca08576/offsets/.0.0b581aff-7857-49af-a698-9a126dceda11.tmp
2025-04-24 15:05:57.788 | 25/04/24 06:05:57 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-98aba8b4-e403-4f10-96a8-9c75af6e71ba/offsets/0 using temp file file:/tmp/temporary-98aba8b4-e403-4f10-96a8-9c75af6e71ba/offsets/.0.0860cbab-b713-46b2-969b-f5a0a50af5ba.tmp
2025-04-24 15:05:57.806 | 25/04/24 06:05:57 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-33d80dac-a68a-4d9e-9955-2288fc90154c/offsets/0 using temp file file:/tmp/temporary-33d80dac-a68a-4d9e-9955-2288fc90154c/offsets/.0.740bb97b-3079-42c1-8505-61ea77b85dc6.tmp
2025-04-24 15:05:57.808 | 25/04/24 06:05:57 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-3c1df5f2-e1da-4beb-8c35-3451ca7efd9d/offsets/0 using temp file file:/tmp/temporary-3c1df5f2-e1da-4beb-8c35-3451ca7efd9d/offsets/.0.ff0b3c1a-6f63-4765-a44d-4565af621e77.tmp
2025-04-24 15:05:57.820 | 25/04/24 06:05:57 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-2ce6649d-5b58-4ef3-98a8-fb21f300afef/offsets/.0.6381b97e-4d7f-4c8b-b65a-bb1e7baf2b3b.tmp to file:/tmp/temporary-2ce6649d-5b58-4ef3-98a8-fb21f300afef/offsets/0
2025-04-24 15:05:57.820 | 25/04/24 06:05:57 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-ec737e79-bc80-4f79-9a41-d619dca08576/offsets/.0.0b581aff-7857-49af-a698-9a126dceda11.tmp to file:/tmp/temporary-ec737e79-bc80-4f79-9a41-d619dca08576/offsets/0
2025-04-24 15:05:57.820 | 25/04/24 06:05:57 INFO MicroBatchExecution: Committed offsets for batch 0. Metadata OffsetSeqMetadata(0,1745474757781,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.optimizer.pruneFiltersCanPruneStreamingSubplan -> false, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
2025-04-24 15:05:57.820 | 25/04/24 06:05:57 INFO MicroBatchExecution: Committed offsets for batch 0. Metadata OffsetSeqMetadata(0,1745474757780,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.optimizer.pruneFiltersCanPruneStreamingSubplan -> false, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
2025-04-24 15:05:57.822 | 25/04/24 06:05:57 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-98aba8b4-e403-4f10-96a8-9c75af6e71ba/offsets/.0.0860cbab-b713-46b2-969b-f5a0a50af5ba.tmp to file:/tmp/temporary-98aba8b4-e403-4f10-96a8-9c75af6e71ba/offsets/0
2025-04-24 15:05:57.823 | 25/04/24 06:05:57 INFO MicroBatchExecution: Committed offsets for batch 0. Metadata OffsetSeqMetadata(0,1745474757780,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.optimizer.pruneFiltersCanPruneStreamingSubplan -> false, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
2025-04-24 15:05:57.829 | 25/04/24 06:05:57 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-33d80dac-a68a-4d9e-9955-2288fc90154c/offsets/.0.740bb97b-3079-42c1-8505-61ea77b85dc6.tmp to file:/tmp/temporary-33d80dac-a68a-4d9e-9955-2288fc90154c/offsets/0
2025-04-24 15:05:57.829 | 25/04/24 06:05:57 INFO MicroBatchExecution: Committed offsets for batch 0. Metadata OffsetSeqMetadata(0,1745474757790,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.optimizer.pruneFiltersCanPruneStreamingSubplan -> false, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
2025-04-24 15:05:57.829 | 25/04/24 06:05:57 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-2cacd18c-322e-4cfa-8e0d-c1008c859651/offsets/.0.7a185ab2-f0ff-4285-bae7-511b0c0602e8.tmp to file:/tmp/temporary-2cacd18c-322e-4cfa-8e0d-c1008c859651/offsets/0
2025-04-24 15:05:57.829 | 25/04/24 06:05:57 INFO MicroBatchExecution: Committed offsets for batch 0. Metadata OffsetSeqMetadata(0,1745474757776,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.optimizer.pruneFiltersCanPruneStreamingSubplan -> false, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
2025-04-24 15:05:57.838 | 25/04/24 06:05:57 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-3c1df5f2-e1da-4beb-8c35-3451ca7efd9d/offsets/.0.ff0b3c1a-6f63-4765-a44d-4565af621e77.tmp to file:/tmp/temporary-3c1df5f2-e1da-4beb-8c35-3451ca7efd9d/offsets/0
2025-04-24 15:05:57.838 | 25/04/24 06:05:57 INFO MicroBatchExecution: Committed offsets for batch 0. Metadata OffsetSeqMetadata(0,1745474757793,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.optimizer.pruneFiltersCanPruneStreamingSubplan -> false, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
2025-04-24 15:05:57.863 | 25/04/24 06:05:57 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
2025-04-24 15:05:57.863 | 25/04/24 06:05:57 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
2025-04-24 15:05:57.863 | 25/04/24 06:05:57 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
2025-04-24 15:05:57.868 | 25/04/24 06:05:57 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
2025-04-24 15:05:57.869 | 25/04/24 06:05:57 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
2025-04-24 15:05:57.869 | 25/04/24 06:05:57 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
2025-04-24 15:05:57.955 | 25/04/24 06:05:57 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
2025-04-24 15:05:57.955 | 25/04/24 06:05:57 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
2025-04-24 15:05:57.955 | 25/04/24 06:05:57 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
2025-04-24 15:05:57.955 | 25/04/24 06:05:57 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
2025-04-24 15:05:57.955 | 25/04/24 06:05:57 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
2025-04-24 15:05:57.959 | 25/04/24 06:05:57 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
2025-04-24 15:05:58.003 | 25/04/24 06:05:58 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
2025-04-24 15:05:58.005 | 25/04/24 06:05:58 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
2025-04-24 15:05:58.007 | 25/04/24 06:05:58 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
2025-04-24 15:05:58.009 | 25/04/24 06:05:58 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
2025-04-24 15:05:58.009 | 25/04/24 06:05:58 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
2025-04-24 15:05:58.009 | 25/04/24 06:05:58 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
2025-04-24 15:05:58.010 | 25/04/24 06:05:58 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
2025-04-24 15:05:58.010 | 25/04/24 06:05:58 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
2025-04-24 15:05:58.011 | 25/04/24 06:05:58 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
2025-04-24 15:05:58.012 | 25/04/24 06:05:58 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
2025-04-24 15:05:58.021 | 25/04/24 06:05:58 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
2025-04-24 15:05:58.021 | 25/04/24 06:05:58 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
2025-04-24 15:05:58.125 | 25/04/24 06:05:58 INFO CodeGenerator: Code generated in 215.705996 ms
2025-04-24 15:05:58.131 | 25/04/24 06:05:58 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
2025-04-24 15:05:58.131 | 25/04/24 06:05:58 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
2025-04-24 15:05:58.132 | 25/04/24 06:05:58 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
2025-04-24 15:05:58.132 | 25/04/24 06:05:58 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
2025-04-24 15:05:58.132 | 25/04/24 06:05:58 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
2025-04-24 15:05:58.133 | 25/04/24 06:05:58 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
2025-04-24 15:05:58.134 | 25/04/24 06:05:58 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
2025-04-24 15:05:58.134 | 25/04/24 06:05:58 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
2025-04-24 15:05:58.134 | 25/04/24 06:05:58 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
2025-04-24 15:05:58.134 | 25/04/24 06:05:58 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
2025-04-24 15:05:58.135 | 25/04/24 06:05:58 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
2025-04-24 15:05:58.136 | 25/04/24 06:05:58 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
2025-04-24 15:05:58.136 | 25/04/24 06:05:58 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
2025-04-24 15:05:58.138 | 25/04/24 06:05:58 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
2025-04-24 15:05:58.138 | 25/04/24 06:05:58 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
2025-04-24 15:05:58.138 | 25/04/24 06:05:58 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
2025-04-24 15:05:58.139 | 25/04/24 06:05:58 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
2025-04-24 15:05:58.139 | 25/04/24 06:05:58 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
2025-04-24 15:05:58.140 | 25/04/24 06:05:58 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
2025-04-24 15:05:58.142 | 25/04/24 06:05:58 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
2025-04-24 15:05:58.142 | 25/04/24 06:05:58 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
2025-04-24 15:05:58.143 | 25/04/24 06:05:58 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
2025-04-24 15:05:58.143 | 25/04/24 06:05:58 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
2025-04-24 15:05:58.145 | 25/04/24 06:05:58 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
2025-04-24 15:05:58.145 | 25/04/24 06:05:58 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
2025-04-24 15:05:58.146 | 25/04/24 06:05:58 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
2025-04-24 15:05:58.147 | 25/04/24 06:05:58 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
2025-04-24 15:05:58.147 | 25/04/24 06:05:58 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
2025-04-24 15:05:58.179 | 25/04/24 06:05:58 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
2025-04-24 15:05:58.181 | 25/04/24 06:05:58 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
2025-04-24 15:05:58.181 | 25/04/24 06:05:58 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
2025-04-24 15:05:58.184 | 25/04/24 06:05:58 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
2025-04-24 15:05:58.196 | 25/04/24 06:05:58 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
2025-04-24 15:05:58.198 | 25/04/24 06:05:58 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
2025-04-24 15:05:58.200 | 25/04/24 06:05:58 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
2025-04-24 15:05:58.202 | 25/04/24 06:05:58 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
2025-04-24 15:05:58.204 | 25/04/24 06:05:58 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
2025-04-24 15:05:58.205 | 25/04/24 06:05:58 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
2025-04-24 15:05:58.208 | 25/04/24 06:05:58 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
2025-04-24 15:05:58.211 | 25/04/24 06:05:58 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
2025-04-24 15:05:58.215 | 25/04/24 06:05:58 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
2025-04-24 15:05:58.215 | 25/04/24 06:05:58 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
2025-04-24 15:05:58.216 | 25/04/24 06:05:58 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
2025-04-24 15:05:58.218 | 25/04/24 06:05:58 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
2025-04-24 15:05:58.219 | 25/04/24 06:05:58 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
2025-04-24 15:05:58.223 | 25/04/24 06:05:58 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
2025-04-24 15:05:58.224 | 25/04/24 06:05:58 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
2025-04-24 15:05:58.224 | 25/04/24 06:05:58 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
2025-04-24 15:05:58.224 | 25/04/24 06:05:58 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
2025-04-24 15:05:58.224 | 25/04/24 06:05:58 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
2025-04-24 15:05:58.225 | 25/04/24 06:05:58 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
2025-04-24 15:05:58.226 | 25/04/24 06:05:58 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
2025-04-24 15:05:58.226 | 25/04/24 06:05:58 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
2025-04-24 15:05:58.227 | 25/04/24 06:05:58 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
2025-04-24 15:05:58.230 | 25/04/24 06:05:58 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
2025-04-24 15:05:58.230 | 25/04/24 06:05:58 INFO CodeGenerator: Code generated in 108.693109 ms
2025-04-24 15:05:58.232 | 25/04/24 06:05:58 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
2025-04-24 15:05:58.250 | 25/04/24 06:05:58 INFO CodeGenerator: Code generated in 127.937401 ms
2025-04-24 15:05:58.252 | 25/04/24 06:05:58 INFO CodeGenerator: Code generated in 47.999568 ms
2025-04-24 15:05:58.254 | Traceback (most recent call last):
2025-04-24 15:05:58.254 |   File "/app/app.py", line 76, in <module>
2025-04-24 15:05:58.254 |     dim_time.show(5, truncate=False)
2025-04-24 15:05:58.254 |   File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/sql/dataframe.py", line 947, in show
2025-04-24 15:05:58.254 |   File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/sql/dataframe.py", line 978, in _show_string
2025-04-24 15:05:58.254 |   File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py", line 1322, in __call__
2025-04-24 15:05:58.254 |   File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/errors/exceptions/captured.py", line 185, in deco
2025-04-24 15:05:58.270 | 25/04/24 06:05:58 INFO CodeGenerator: Code generated in 148.041905 ms
2025-04-24 15:05:58.272 | pyspark.errors.exceptions.captured.AnalysisException: Queries with streaming sources must be executed with writeStream.start();
2025-04-24 15:05:58.273 | kafka
2025-04-24 15:05:58.273 | 25/04/24 06:05:58 INFO CodeGenerator: Code generated in 27.568105 ms
2025-04-24 15:05:58.292 | 25/04/24 06:05:58 INFO CodeGenerator: Code generated in 72.899055 ms
2025-04-24 15:05:58.294 | 25/04/24 06:05:58 INFO CodeGenerator: Code generated in 47.475705 ms
2025-04-24 15:05:58.316 | 25/04/24 06:05:58 INFO SparkContext: Invoking stop() from shutdown hook
2025-04-24 15:05:58.316 | 25/04/24 06:05:58 INFO SparkContext: SparkContext is stopping with exitCode 0.
2025-04-24 15:05:58.321 | 25/04/24 06:05:58 INFO CodeGenerator: Code generated in 86.916253 ms
2025-04-24 15:05:58.332 | 25/04/24 06:05:58 INFO CodeGenerator: Code generated in 24.548686 ms
2025-04-24 15:05:58.348 | 25/04/24 06:05:58 ERROR MicroBatchExecution: Query [id = f3e9493a-b283-448c-8c2e-f81a3cccf91b, runId = 7027e2f9-367f-47c4-b287-66d27ec2882b] terminated with error
2025-04-24 15:05:58.348 | py4j.Py4JException: Cannot obtain a new communication channel
2025-04-24 15:05:58.348 | 	at py4j.CallbackClient.sendCommand(CallbackClient.java:380)
2025-04-24 15:05:58.348 | 	at py4j.CallbackClient.sendCommand(CallbackClient.java:356)
2025-04-24 15:05:58.348 | 	at py4j.reflection.PythonProxyHandler.invoke(PythonProxyHandler.java:106)
2025-04-24 15:05:58.348 | 	at jdk.proxy3/jdk.proxy3.$Proxy30.call(Unknown Source)
2025-04-24 15:05:58.348 | 	at org.apache.spark.sql.execution.streaming.sources.PythonForeachBatchHelper$.$anonfun$callForeachBatch$1(ForeachBatchSink.scala:53)
2025-04-24 15:05:58.348 | 	at org.apache.spark.sql.execution.streaming.sources.PythonForeachBatchHelper$.$anonfun$callForeachBatch$1$adapted(ForeachBatchSink.scala:53)
2025-04-24 15:05:58.348 | 	at org.apache.spark.sql.execution.streaming.sources.ForeachBatchSink.addBatch(ForeachBatchSink.scala:34)
2025-04-24 15:05:58.348 | 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$17(MicroBatchExecution.scala:732)
2025-04-24 15:05:58.348 | 	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)
2025-04-24 15:05:58.348 | 	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
2025-04-24 15:05:58.348 | 	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
2025-04-24 15:05:58.348 | 	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
2025-04-24 15:05:58.348 | 	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
2025-04-24 15:05:58.348 | 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$16(MicroBatchExecution.scala:729)
2025-04-24 15:05:58.348 | 	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)
2025-04-24 15:05:58.348 | 	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)
2025-04-24 15:05:58.348 | 	at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)
2025-04-24 15:05:58.348 | 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.runBatch(MicroBatchExecution.scala:729)
2025-04-24 15:05:58.348 | 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:286)
2025-04-24 15:05:58.348 | 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
2025-04-24 15:05:58.348 | 	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)
2025-04-24 15:05:58.348 | 	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)
2025-04-24 15:05:58.348 | 	at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)
2025-04-24 15:05:58.348 | 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)
2025-04-24 15:05:58.348 | 	at org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)
2025-04-24 15:05:58.348 | 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)
2025-04-24 15:05:58.348 | 	at org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)
2025-04-24 15:05:58.348 | 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
2025-04-24 15:05:58.348 | 	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
2025-04-24 15:05:58.348 | 	at org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)
2025-04-24 15:05:58.348 | 	at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)
2025-04-24 15:05:58.348 | 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
2025-04-24 15:05:58.348 | 	at org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)
2025-04-24 15:05:58.348 | 	at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)
2025-04-24 15:05:58.348 | 25/04/24 06:05:58 ERROR MicroBatchExecution: Query [id = aff125db-ad22-45d3-8d65-cae02357b9c3, runId = 46804a04-de66-4b1e-b791-2fd2cee06947] terminated with error
2025-04-24 15:05:58.348 | py4j.Py4JException: Cannot obtain a new communication channel
2025-04-24 15:05:58.348 | 	at py4j.CallbackClient.sendCommand(CallbackClient.java:380)
2025-04-24 15:05:58.348 | 	at py4j.CallbackClient.sendCommand(CallbackClient.java:356)
2025-04-24 15:05:58.348 | 	at py4j.reflection.PythonProxyHandler.invoke(PythonProxyHandler.java:106)
2025-04-24 15:05:58.348 | 	at jdk.proxy3/jdk.proxy3.$Proxy30.call(Unknown Source)
2025-04-24 15:05:58.348 | 	at org.apache.spark.sql.execution.streaming.sources.PythonForeachBatchHelper$.$anonfun$callForeachBatch$1(ForeachBatchSink.scala:53)
2025-04-24 15:05:58.348 | 	at org.apache.spark.sql.execution.streaming.sources.PythonForeachBatchHelper$.$anonfun$callForeachBatch$1$adapted(ForeachBatchSink.scala:53)
2025-04-24 15:05:58.348 | 	at org.apache.spark.sql.execution.streaming.sources.ForeachBatchSink.addBatch(ForeachBatchSink.scala:34)
2025-04-24 15:05:58.348 | 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$17(MicroBatchExecution.scala:732)
2025-04-24 15:05:58.348 | 	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)
2025-04-24 15:05:58.348 | 	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
2025-04-24 15:05:58.348 | 	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
2025-04-24 15:05:58.348 | 	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
2025-04-24 15:05:58.348 | 	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
2025-04-24 15:05:58.348 | 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$16(MicroBatchExecution.scala:729)
2025-04-24 15:05:58.348 | 	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)
2025-04-24 15:05:58.348 | 	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)
2025-04-24 15:05:58.348 | 	at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)
2025-04-24 15:05:58.348 | 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.runBatch(MicroBatchExecution.scala:729)
2025-04-24 15:05:58.348 | 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:286)
2025-04-24 15:05:58.348 | 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
2025-04-24 15:05:58.348 | 	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)
2025-04-24 15:05:58.348 | 	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)
2025-04-24 15:05:58.348 | 	at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)
2025-04-24 15:05:58.348 | 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)
2025-04-24 15:05:58.348 | 	at org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)
2025-04-24 15:05:58.348 | 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)
2025-04-24 15:05:58.348 | 	at org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)
2025-04-24 15:05:58.348 | 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
2025-04-24 15:05:58.348 | 	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
2025-04-24 15:05:58.348 | 	at org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)
2025-04-24 15:05:58.348 | 	at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)
2025-04-24 15:05:58.348 | 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
2025-04-24 15:05:58.348 | 	at org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)
2025-04-24 15:05:58.348 | 	at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)
2025-04-24 15:05:58.349 | 25/04/24 06:05:58 ERROR MicroBatchExecution: Query [id = dd378de2-a37c-4818-995d-91e5aec4c185, runId = 57157b6a-9d62-409d-a827-3ef872726f83] terminated with error
2025-04-24 15:05:58.349 | py4j.Py4JException: Cannot obtain a new communication channel
2025-04-24 15:05:58.349 | 	at py4j.CallbackClient.sendCommand(CallbackClient.java:380)
2025-04-24 15:05:58.349 | 	at py4j.CallbackClient.sendCommand(CallbackClient.java:356)
2025-04-24 15:05:58.349 | 	at py4j.reflection.PythonProxyHandler.invoke(PythonProxyHandler.java:106)
2025-04-24 15:05:58.349 | 	at jdk.proxy3/jdk.proxy3.$Proxy30.call(Unknown Source)
2025-04-24 15:05:58.349 | 	at org.apache.spark.sql.execution.streaming.sources.PythonForeachBatchHelper$.$anonfun$callForeachBatch$1(ForeachBatchSink.scala:53)
2025-04-24 15:05:58.349 | 	at org.apache.spark.sql.execution.streaming.sources.PythonForeachBatchHelper$.$anonfun$callForeachBatch$1$adapted(ForeachBatchSink.scala:53)
2025-04-24 15:05:58.349 | 	at org.apache.spark.sql.execution.streaming.sources.ForeachBatchSink.addBatch(ForeachBatchSink.scala:34)
2025-04-24 15:05:58.349 | 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$17(MicroBatchExecution.scala:732)
2025-04-24 15:05:58.349 | 	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)
2025-04-24 15:05:58.349 | 	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
2025-04-24 15:05:58.349 | 	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
2025-04-24 15:05:58.349 | 	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
2025-04-24 15:05:58.349 | 	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
2025-04-24 15:05:58.349 | 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$16(MicroBatchExecution.scala:729)
2025-04-24 15:05:58.349 | 	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)
2025-04-24 15:05:58.349 | 	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)
2025-04-24 15:05:58.349 | 	at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)
2025-04-24 15:05:58.349 | 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.runBatch(MicroBatchExecution.scala:729)
2025-04-24 15:05:58.349 | 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:286)
2025-04-24 15:05:58.349 | 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
2025-04-24 15:05:58.349 | 	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)
2025-04-24 15:05:58.349 | 	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)
2025-04-24 15:05:58.349 | 	at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)
2025-04-24 15:05:58.349 | 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)
2025-04-24 15:05:58.349 | 	at org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)
2025-04-24 15:05:58.349 | 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)
2025-04-24 15:05:58.349 | 	at org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)
2025-04-24 15:05:58.349 | 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
2025-04-24 15:05:58.349 | 	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
2025-04-24 15:05:58.349 | 	at org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)
2025-04-24 15:05:58.349 | 	at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)
2025-04-24 15:05:58.349 | 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
2025-04-24 15:05:58.349 | 	at org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)
2025-04-24 15:05:58.349 | 	at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)
2025-04-24 15:05:58.351 | 25/04/24 06:05:58 INFO AppInfoParser: App info kafka.admin.client for adminclient-8 unregistered
2025-04-24 15:05:58.351 | 25/04/24 06:05:58 ERROR MicroBatchExecution: Query [id = 00653682-fa1a-49ed-863e-fa929f908a51, runId = efd8353c-1172-4f58-8d98-3b8ff2db3c7d] terminated with error
2025-04-24 15:05:58.351 | py4j.Py4JException: Cannot obtain a new communication channel
2025-04-24 15:05:58.351 | 	at py4j.CallbackClient.sendCommand(CallbackClient.java:380)
2025-04-24 15:05:58.351 | 	at py4j.CallbackClient.sendCommand(CallbackClient.java:356)
2025-04-24 15:05:58.351 | 	at py4j.reflection.PythonProxyHandler.invoke(PythonProxyHandler.java:106)
2025-04-24 15:05:58.351 | 	at jdk.proxy3/jdk.proxy3.$Proxy30.call(Unknown Source)
2025-04-24 15:05:58.351 | 	at org.apache.spark.sql.execution.streaming.sources.PythonForeachBatchHelper$.$anonfun$callForeachBatch$1(ForeachBatchSink.scala:53)
2025-04-24 15:05:58.351 | 	at org.apache.spark.sql.execution.streaming.sources.PythonForeachBatchHelper$.$anonfun$callForeachBatch$1$adapted(ForeachBatchSink.scala:53)
2025-04-24 15:05:58.351 | 	at org.apache.spark.sql.execution.streaming.sources.ForeachBatchSink.addBatch(ForeachBatchSink.scala:34)
2025-04-24 15:05:58.351 | 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$17(MicroBatchExecution.scala:732)
2025-04-24 15:05:58.351 | 	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)
2025-04-24 15:05:58.351 | 	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
2025-04-24 15:05:58.351 | 	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
2025-04-24 15:05:58.351 | 	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
2025-04-24 15:05:58.351 | 	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
2025-04-24 15:05:58.351 | 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$16(MicroBatchExecution.scala:729)
2025-04-24 15:05:58.351 | 	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)
2025-04-24 15:05:58.351 | 	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)
2025-04-24 15:05:58.351 | 	at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)
2025-04-24 15:05:58.351 | 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.runBatch(MicroBatchExecution.scala:729)
2025-04-24 15:05:58.351 | 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:286)
2025-04-24 15:05:58.351 | 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
2025-04-24 15:05:58.351 | 	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)
2025-04-24 15:05:58.351 | 	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)
2025-04-24 15:05:58.351 | 	at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)
2025-04-24 15:05:58.351 | 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)
2025-04-24 15:05:58.351 | 	at org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)
2025-04-24 15:05:58.351 | 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)
2025-04-24 15:05:58.351 | 	at org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)
2025-04-24 15:05:58.351 | 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
2025-04-24 15:05:58.351 | 	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
2025-04-24 15:05:58.351 | 	at org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)
2025-04-24 15:05:58.351 | 	at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)
2025-04-24 15:05:58.351 | 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
2025-04-24 15:05:58.351 | 	at org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)
2025-04-24 15:05:58.351 | 	at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)
2025-04-24 15:05:58.351 | 25/04/24 06:05:58 ERROR MicroBatchExecution: Query [id = 6f1a8887-49c1-41df-b38c-99a40b9eb6fa, runId = b22d2d34-fa2f-4234-94db-afe323ff5e0d] terminated with error
2025-04-24 15:05:58.352 | py4j.Py4JException: Cannot obtain a new communication channel
2025-04-24 15:05:58.352 | 	at py4j.CallbackClient.sendCommand(CallbackClient.java:380)
2025-04-24 15:05:58.352 | 	at py4j.CallbackClient.sendCommand(CallbackClient.java:356)
2025-04-24 15:05:58.352 | 	at py4j.reflection.PythonProxyHandler.invoke(PythonProxyHandler.java:106)
2025-04-24 15:05:58.352 | 	at jdk.proxy3/jdk.proxy3.$Proxy30.call(Unknown Source)
2025-04-24 15:05:58.352 | 	at org.apache.spark.sql.execution.streaming.sources.PythonForeachBatchHelper$.$anonfun$callForeachBatch$1(ForeachBatchSink.scala:53)
2025-04-24 15:05:58.352 | 	at org.apache.spark.sql.execution.streaming.sources.PythonForeachBatchHelper$.$anonfun$callForeachBatch$1$adapted(ForeachBatchSink.scala:53)
2025-04-24 15:05:58.352 | 	at org.apache.spark.sql.execution.streaming.sources.ForeachBatchSink.addBatch(ForeachBatchSink.scala:34)
2025-04-24 15:05:58.352 | 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$17(MicroBatchExecution.scala:732)
2025-04-24 15:05:58.352 | 	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)
2025-04-24 15:05:58.352 | 	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
2025-04-24 15:05:58.352 | 	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
2025-04-24 15:05:58.352 | 	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
2025-04-24 15:05:58.352 | 	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
2025-04-24 15:05:58.352 | 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$16(MicroBatchExecution.scala:729)
2025-04-24 15:05:58.352 | 	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)
2025-04-24 15:05:58.352 | 	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)
2025-04-24 15:05:58.352 | 	at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)
2025-04-24 15:05:58.352 | 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.runBatch(MicroBatchExecution.scala:729)
2025-04-24 15:05:58.352 | 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:286)
2025-04-24 15:05:58.352 | 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
2025-04-24 15:05:58.352 | 	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)
2025-04-24 15:05:58.352 | 	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)
2025-04-24 15:05:58.352 | 	at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)
2025-04-24 15:05:58.352 | 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)
2025-04-24 15:05:58.352 | 	at org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)
2025-04-24 15:05:58.352 | 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)
2025-04-24 15:05:58.352 | 	at org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)
2025-04-24 15:05:58.352 | 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
2025-04-24 15:05:58.352 | 	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
2025-04-24 15:05:58.352 | 	at org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)
2025-04-24 15:05:58.352 | 	at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)
2025-04-24 15:05:58.352 | 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
2025-04-24 15:05:58.352 | 	at org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)
2025-04-24 15:05:58.352 | 	at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)
2025-04-24 15:05:58.352 | 25/04/24 06:05:58 INFO AppInfoParser: App info kafka.admin.client for adminclient-5 unregistered
2025-04-24 15:05:58.353 | 25/04/24 06:05:58 INFO AppInfoParser: App info kafka.admin.client for adminclient-6 unregistered
2025-04-24 15:05:58.353 | 25/04/24 06:05:58 INFO Metrics: Metrics scheduler closed
2025-04-24 15:05:58.353 | 25/04/24 06:05:58 INFO Metrics: Closing reporter org.apache.kafka.common.metrics.JmxReporter
2025-04-24 15:05:58.353 | 25/04/24 06:05:58 INFO Metrics: Metrics reporters closed
2025-04-24 15:05:58.353 | 25/04/24 06:05:58 INFO AppInfoParser: App info kafka.admin.client for adminclient-4 unregistered
2025-04-24 15:05:58.354 | 25/04/24 06:05:58 INFO SparkUI: Stopped Spark web UI at http://2904d747298a:4040
2025-04-24 15:05:58.356 | 25/04/24 06:05:58 INFO AppInfoParser: App info kafka.admin.client for adminclient-1 unregistered
2025-04-24 15:05:58.357 | 25/04/24 06:05:58 ERROR MicroBatchExecution: Query [id = b981a279-063c-4d07-953e-5000a8a9c86c, runId = 4afe7425-0e6c-4ad7-9410-03a4942cca19] terminated with error
2025-04-24 15:05:58.357 | py4j.Py4JException: Cannot obtain a new communication channel
2025-04-24 15:05:58.357 | 	at py4j.CallbackClient.sendCommand(CallbackClient.java:380)
2025-04-24 15:05:58.357 | 	at py4j.CallbackClient.sendCommand(CallbackClient.java:356)
2025-04-24 15:05:58.357 | 	at py4j.reflection.PythonProxyHandler.invoke(PythonProxyHandler.java:106)
2025-04-24 15:05:58.357 | 	at jdk.proxy3/jdk.proxy3.$Proxy30.call(Unknown Source)
2025-04-24 15:05:58.357 | 	at org.apache.spark.sql.execution.streaming.sources.PythonForeachBatchHelper$.$anonfun$callForeachBatch$1(ForeachBatchSink.scala:53)
2025-04-24 15:05:58.357 | 	at org.apache.spark.sql.execution.streaming.sources.PythonForeachBatchHelper$.$anonfun$callForeachBatch$1$adapted(ForeachBatchSink.scala:53)
2025-04-24 15:05:58.357 | 	at org.apache.spark.sql.execution.streaming.sources.ForeachBatchSink.addBatch(ForeachBatchSink.scala:34)
2025-04-24 15:05:58.357 | 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$17(MicroBatchExecution.scala:732)
2025-04-24 15:05:58.357 | 	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)
2025-04-24 15:05:58.357 | 	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
2025-04-24 15:05:58.357 | 	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
2025-04-24 15:05:58.357 | 	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
2025-04-24 15:05:58.357 | 	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
2025-04-24 15:05:58.357 | 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$16(MicroBatchExecution.scala:729)
2025-04-24 15:05:58.357 | 	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)
2025-04-24 15:05:58.357 | 	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)
2025-04-24 15:05:58.357 | 	at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)
2025-04-24 15:05:58.357 | 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.runBatch(MicroBatchExecution.scala:729)
2025-04-24 15:05:58.357 | 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:286)
2025-04-24 15:05:58.357 | 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
2025-04-24 15:05:58.357 | 	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)
2025-04-24 15:05:58.357 | 	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)
2025-04-24 15:05:58.357 | 	at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)
2025-04-24 15:05:58.357 | 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)
2025-04-24 15:05:58.357 | 	at org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)
2025-04-24 15:05:58.357 | 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)
2025-04-24 15:05:58.357 | 	at org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)
2025-04-24 15:05:58.357 | 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
2025-04-24 15:05:58.357 | 	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
2025-04-24 15:05:58.357 | 	at org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)
2025-04-24 15:05:58.357 | 	at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)
2025-04-24 15:05:58.357 | 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
2025-04-24 15:05:58.357 | 	at org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)
2025-04-24 15:05:58.357 | 	at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)
2025-04-24 15:05:58.357 | 25/04/24 06:05:58 INFO MicroBatchExecution: Async log purge executor pool for query [id = 00653682-fa1a-49ed-863e-fa929f908a51, runId = efd8353c-1172-4f58-8d98-3b8ff2db3c7d] has been shutdown
2025-04-24 15:05:58.358 | 25/04/24 06:05:58 INFO Metrics: Metrics scheduler closed
2025-04-24 15:05:58.358 | 25/04/24 06:05:58 INFO Metrics: Closing reporter org.apache.kafka.common.metrics.JmxReporter
2025-04-24 15:05:58.358 | 25/04/24 06:05:58 INFO Metrics: Metrics reporters closed
2025-04-24 15:05:58.358 | 25/04/24 06:05:58 INFO Metrics: Metrics scheduler closed
2025-04-24 15:05:58.358 | 25/04/24 06:05:58 INFO Metrics: Closing reporter org.apache.kafka.common.metrics.JmxReporter
2025-04-24 15:05:58.358 | 25/04/24 06:05:58 INFO Metrics: Metrics reporters closed
2025-04-24 15:05:58.358 | 25/04/24 06:05:58 INFO MicroBatchExecution: Async log purge executor pool for query [id = dd378de2-a37c-4818-995d-91e5aec4c185, runId = 57157b6a-9d62-409d-a827-3ef872726f83] has been shutdown
2025-04-24 15:05:58.360 | 25/04/24 06:05:58 INFO Metrics: Metrics scheduler closed
2025-04-24 15:05:58.360 | 25/04/24 06:05:58 INFO Metrics: Closing reporter org.apache.kafka.common.metrics.JmxReporter
2025-04-24 15:05:58.361 | 25/04/24 06:05:58 INFO Metrics: Metrics scheduler closed
2025-04-24 15:05:58.361 | 25/04/24 06:05:58 INFO Metrics: Closing reporter org.apache.kafka.common.metrics.JmxReporter
2025-04-24 15:05:58.363 | 25/04/24 06:05:58 INFO Metrics: Metrics reporters closed
2025-04-24 15:05:58.363 | 25/04/24 06:05:58 INFO Metrics: Metrics reporters closed
2025-04-24 15:05:58.363 | 25/04/24 06:05:58 INFO MicroBatchExecution: Async log purge executor pool for query [id = aff125db-ad22-45d3-8d65-cae02357b9c3, runId = 46804a04-de66-4b1e-b791-2fd2cee06947] has been shutdown
2025-04-24 15:05:58.363 | 25/04/24 06:05:58 INFO MicroBatchExecution: Async log purge executor pool for query [id = f3e9493a-b283-448c-8c2e-f81a3cccf91b, runId = 7027e2f9-367f-47c4-b287-66d27ec2882b] has been shutdown
2025-04-24 15:05:58.363 | 25/04/24 06:05:58 INFO StandaloneSchedulerBackend: Shutting down all executors
2025-04-24 15:05:58.363 | 25/04/24 06:05:58 INFO StandaloneSchedulerBackend$StandaloneDriverEndpoint: Asking each executor to shut down
2025-04-24 15:05:58.363 | 25/04/24 06:05:58 INFO AppInfoParser: App info kafka.admin.client for adminclient-3 unregistered
2025-04-24 15:05:58.364 | 25/04/24 06:05:58 INFO Metrics: Metrics scheduler closed
2025-04-24 15:05:58.364 | 25/04/24 06:05:58 INFO Metrics: Closing reporter org.apache.kafka.common.metrics.JmxReporter
2025-04-24 15:05:58.364 | 25/04/24 06:05:58 INFO Metrics: Metrics reporters closed
2025-04-24 15:05:58.364 | 25/04/24 06:05:58 INFO MicroBatchExecution: Async log purge executor pool for query [id = b981a279-063c-4d07-953e-5000a8a9c86c, runId = 4afe7425-0e6c-4ad7-9410-03a4942cca19] has been shutdown
2025-04-24 15:05:58.365 | 25/04/24 06:05:58 INFO MicroBatchExecution: Async log purge executor pool for query [id = 6f1a8887-49c1-41df-b38c-99a40b9eb6fa, runId = b22d2d34-fa2f-4234-94db-afe323ff5e0d] has been shutdown
2025-04-24 15:05:58.367 | 25/04/24 06:05:58 INFO CodeGenerator: Code generated in 58.03808 ms
2025-04-24 15:05:58.377 | 25/04/24 06:05:58 INFO CodeGenerator: Code generated in 30.612085 ms
2025-04-24 15:05:58.390 | 25/04/24 06:05:58 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
2025-04-24 15:05:58.390 | 25/04/24 06:05:58 ERROR MicroBatchExecution: Query [id = c98466be-c74c-4620-a512-78fb48627126, runId = b5e1c45e-6ade-4009-a466-7e367ad05729] terminated with error
2025-04-24 15:05:58.390 | java.lang.IllegalStateException: LiveListenerBus is stopped.
2025-04-24 15:05:58.390 | 	at org.apache.spark.scheduler.LiveListenerBus.addToQueue(LiveListenerBus.scala:92)
2025-04-24 15:05:58.390 | 	at org.apache.spark.sql.execution.streaming.StreamingQueryListenerBus.$anonfun$new$1(StreamingQueryListenerBus.scala:48)
2025-04-24 15:05:58.390 | 	at org.apache.spark.sql.execution.streaming.StreamingQueryListenerBus.$anonfun$new$1$adapted(StreamingQueryListenerBus.scala:48)
2025-04-24 15:05:58.390 | 	at scala.Option.foreach(Option.scala:407)
2025-04-24 15:05:58.390 | 	at org.apache.spark.sql.execution.streaming.StreamingQueryListenerBus.<init>(StreamingQueryListenerBus.scala:48)
2025-04-24 15:05:58.390 | 	at org.apache.spark.sql.streaming.StreamingQueryManager.<init>(StreamingQueryManager.scala:54)
2025-04-24 15:05:58.390 | 	at org.apache.spark.sql.internal.BaseSessionStateBuilder.streamingQueryManager(BaseSessionStateBuilder.scala:340)
2025-04-24 15:05:58.390 | 	at org.apache.spark.sql.internal.BaseSessionStateBuilder.$anonfun$build$4(BaseSessionStateBuilder.scala:378)
2025-04-24 15:05:58.390 | 	at org.apache.spark.sql.internal.SessionState.streamingQueryManager$lzycompute(SessionState.scala:100)
2025-04-24 15:05:58.390 | 	at org.apache.spark.sql.internal.SessionState.streamingQueryManager(SessionState.scala:100)
2025-04-24 15:05:58.390 | 	at org.apache.spark.sql.SparkSession.streams(SparkSession.scala:238)
2025-04-24 15:05:58.390 | 	at org.apache.spark.sql.execution.streaming.BaseStreamingDeduplicateExec.doExecute(statefulOperators.scala:916)
2025-04-24 15:05:58.390 | 	at org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:195)
2025-04-24 15:05:58.390 | 	at org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:246)
2025-04-24 15:05:58.390 | 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
2025-04-24 15:05:58.390 | 	at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:243)
2025-04-24 15:05:58.390 | 	at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:191)
2025-04-24 15:05:58.390 | 	at org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:207)
2025-04-24 15:05:58.390 | 	at org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:206)
2025-04-24 15:05:58.390 | 	at org.apache.spark.sql.execution.streaming.sources.ForeachBatchSink.addBatch(ForeachBatchSink.scala:30)
2025-04-24 15:05:58.390 | 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$17(MicroBatchExecution.scala:732)
2025-04-24 15:05:58.390 | 	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)
2025-04-24 15:05:58.390 | 	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
2025-04-24 15:05:58.390 | 	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
2025-04-24 15:05:58.390 | 	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
2025-04-24 15:05:58.390 | 	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
2025-04-24 15:05:58.390 | 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$16(MicroBatchExecution.scala:729)
2025-04-24 15:05:58.390 | 	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)
2025-04-24 15:05:58.390 | 	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)
2025-04-24 15:05:58.390 | 	at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)
2025-04-24 15:05:58.390 | 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.runBatch(MicroBatchExecution.scala:729)
2025-04-24 15:05:58.390 | 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:286)
2025-04-24 15:05:58.390 | 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
2025-04-24 15:05:58.390 | 	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)
2025-04-24 15:05:58.390 | 	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)
2025-04-24 15:05:58.390 | 	at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)
2025-04-24 15:05:58.390 | 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)
2025-04-24 15:05:58.390 | 	at org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)
2025-04-24 15:05:58.390 | 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)
2025-04-24 15:05:58.390 | 	at org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)
2025-04-24 15:05:58.390 | 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
2025-04-24 15:05:58.390 | 	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
2025-04-24 15:05:58.390 | 	at org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)
2025-04-24 15:05:58.390 | 	at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)
2025-04-24 15:05:58.390 | 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
2025-04-24 15:05:58.390 | 	at org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)
2025-04-24 15:05:58.390 | 	at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)
2025-04-24 15:05:58.391 | 25/04/24 06:05:58 INFO AppInfoParser: App info kafka.admin.client for adminclient-9 unregistered
2025-04-24 15:05:58.393 | 25/04/24 06:05:58 INFO Metrics: Metrics scheduler closed
2025-04-24 15:05:58.393 | 25/04/24 06:05:58 INFO Metrics: Closing reporter org.apache.kafka.common.metrics.JmxReporter
2025-04-24 15:05:58.393 | 25/04/24 06:05:58 INFO Metrics: Metrics reporters closed
2025-04-24 15:05:58.395 | 25/04/24 06:05:58 INFO MicroBatchExecution: Async log purge executor pool for query [id = c98466be-c74c-4620-a512-78fb48627126, runId = b5e1c45e-6ade-4009-a466-7e367ad05729] has been shutdown
2025-04-24 15:05:58.403 | 25/04/24 06:05:58 INFO CodeGenerator: Code generated in 84.500448 ms
2025-04-24 15:05:58.404 | 25/04/24 06:05:58 INFO CodeGenerator: Code generated in 13.823903 ms
2025-04-24 15:05:58.421 | 25/04/24 06:05:58 ERROR MicroBatchExecution: Query [id = 25d3d7ed-535e-4772-9fc0-63351b9b5ef9, runId = f71c82ed-061e-41c6-9580-6396f33635a9] terminated with error
2025-04-24 15:05:58.421 | java.lang.IllegalStateException: LiveListenerBus is stopped.
2025-04-24 15:05:58.421 | 	at org.apache.spark.scheduler.LiveListenerBus.addToQueue(LiveListenerBus.scala:92)
2025-04-24 15:05:58.421 | 	at org.apache.spark.sql.execution.streaming.StreamingQueryListenerBus.$anonfun$new$1(StreamingQueryListenerBus.scala:48)
2025-04-24 15:05:58.421 | 	at org.apache.spark.sql.execution.streaming.StreamingQueryListenerBus.$anonfun$new$1$adapted(StreamingQueryListenerBus.scala:48)
2025-04-24 15:05:58.421 | 	at scala.Option.foreach(Option.scala:407)
2025-04-24 15:05:58.421 | 	at org.apache.spark.sql.execution.streaming.StreamingQueryListenerBus.<init>(StreamingQueryListenerBus.scala:48)
2025-04-24 15:05:58.421 | 	at org.apache.spark.sql.streaming.StreamingQueryManager.<init>(StreamingQueryManager.scala:54)
2025-04-24 15:05:58.421 | 	at org.apache.spark.sql.internal.BaseSessionStateBuilder.streamingQueryManager(BaseSessionStateBuilder.scala:340)
2025-04-24 15:05:58.421 | 	at org.apache.spark.sql.internal.BaseSessionStateBuilder.$anonfun$build$4(BaseSessionStateBuilder.scala:378)
2025-04-24 15:05:58.421 | 	at org.apache.spark.sql.internal.SessionState.streamingQueryManager$lzycompute(SessionState.scala:100)
2025-04-24 15:05:58.421 | 	at org.apache.spark.sql.internal.SessionState.streamingQueryManager(SessionState.scala:100)
2025-04-24 15:05:58.421 | 	at org.apache.spark.sql.SparkSession.streams(SparkSession.scala:238)
2025-04-24 15:05:58.421 | 	at org.apache.spark.sql.execution.streaming.BaseStreamingDeduplicateExec.doExecute(statefulOperators.scala:916)
2025-04-24 15:05:58.421 | 	at org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:195)
2025-04-24 15:05:58.421 | 	at org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:246)
2025-04-24 15:05:58.421 | 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
2025-04-24 15:05:58.421 | 	at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:243)
2025-04-24 15:05:58.421 | 	at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:191)
2025-04-24 15:05:58.421 | 	at org.apache.spark.sql.execution.InputAdapter.inputRDD(WholeStageCodegenExec.scala:527)
2025-04-24 15:05:58.421 | 	at org.apache.spark.sql.execution.InputRDDCodegen.inputRDDs(WholeStageCodegenExec.scala:455)
2025-04-24 15:05:58.421 | 	at org.apache.spark.sql.execution.InputRDDCodegen.inputRDDs$(WholeStageCodegenExec.scala:454)
2025-04-24 15:05:58.421 | 	at org.apache.spark.sql.execution.InputAdapter.inputRDDs(WholeStageCodegenExec.scala:498)
2025-04-24 15:05:58.421 | 	at org.apache.spark.sql.execution.ProjectExec.inputRDDs(basicPhysicalOperators.scala:51)
2025-04-24 15:05:58.421 | 	at org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:751)
2025-04-24 15:05:58.421 | 	at org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:195)
2025-04-24 15:05:58.421 | 	at org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:246)
2025-04-24 15:05:58.421 | 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
2025-04-24 15:05:58.421 | 	at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:243)
2025-04-24 15:05:58.421 | 	at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:191)
2025-04-24 15:05:58.421 | 	at org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:207)
2025-04-24 15:05:58.421 | 	at org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:206)
2025-04-24 15:05:58.421 | 	at org.apache.spark.sql.execution.streaming.sources.ForeachBatchSink.addBatch(ForeachBatchSink.scala:30)
2025-04-24 15:05:58.421 | 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$17(MicroBatchExecution.scala:732)
2025-04-24 15:05:58.421 | 	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)
2025-04-24 15:05:58.421 | 	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
2025-04-24 15:05:58.421 | 	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
2025-04-24 15:05:58.421 | 	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
2025-04-24 15:05:58.421 | 	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
2025-04-24 15:05:58.421 | 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$16(MicroBatchExecution.scala:729)
2025-04-24 15:05:58.421 | 	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)
2025-04-24 15:05:58.421 | 	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)
2025-04-24 15:05:58.421 | 	at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)
2025-04-24 15:05:58.421 | 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.runBatch(MicroBatchExecution.scala:729)
2025-04-24 15:05:58.421 | 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:286)
2025-04-24 15:05:58.421 | 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
2025-04-24 15:05:58.421 | 	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)
2025-04-24 15:05:58.421 | 	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)
2025-04-24 15:05:58.421 | 	at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)
2025-04-24 15:05:58.421 | 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)
2025-04-24 15:05:58.421 | 	at org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)
2025-04-24 15:05:58.421 | 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)
2025-04-24 15:05:58.421 | 	at org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)
2025-04-24 15:05:58.421 | 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
2025-04-24 15:05:58.421 | 	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
2025-04-24 15:05:58.421 | 	at org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)
2025-04-24 15:05:58.421 | 	at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)
2025-04-24 15:05:58.421 | 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
2025-04-24 15:05:58.421 | 	at org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)
2025-04-24 15:05:58.421 | 	at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)
2025-04-24 15:05:58.421 | 25/04/24 06:05:58 INFO AppInfoParser: App info kafka.admin.client for adminclient-2 unregistered
2025-04-24 15:05:58.422 | 25/04/24 06:05:58 INFO MemoryStore: MemoryStore cleared
2025-04-24 15:05:58.422 | 25/04/24 06:05:58 INFO Metrics: Metrics scheduler closed
2025-04-24 15:05:58.422 | 25/04/24 06:05:58 INFO Metrics: Closing reporter org.apache.kafka.common.metrics.JmxReporter
2025-04-24 15:05:58.422 | 25/04/24 06:05:58 INFO BlockManager: BlockManager stopped
2025-04-24 15:05:58.422 | 25/04/24 06:05:58 INFO Metrics: Metrics reporters closed
2025-04-24 15:05:58.422 | 25/04/24 06:05:58 INFO CodeGenerator: Code generated in 38.904373 ms
2025-04-24 15:05:58.422 | 25/04/24 06:05:58 INFO MicroBatchExecution: Async log purge executor pool for query [id = 25d3d7ed-535e-4772-9fc0-63351b9b5ef9, runId = f71c82ed-061e-41c6-9580-6396f33635a9] has been shutdown
2025-04-24 15:05:58.452 | 25/04/24 06:05:58 INFO CodeGenerator: Code generated in 29.882671 ms
2025-04-24 15:05:58.453 | 25/04/24 06:05:58 INFO BlockManagerMaster: BlockManagerMaster stopped
2025-04-24 15:05:58.460 | 25/04/24 06:05:58 INFO CodeGenerator: Code generated in 24.142556 ms
2025-04-24 15:05:58.474 | 25/04/24 06:05:58 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
2025-04-24 15:05:58.479 | 25/04/24 06:05:58 ERROR MicroBatchExecution: Query [id = 2e8bba16-c696-4b17-8e4b-d77e80407107, runId = 56258db2-de72-41de-91ae-411133733965] terminated with error
2025-04-24 15:05:58.479 | java.lang.IllegalStateException: RpcEnv has been stopped
2025-04-24 15:05:58.479 | 	at org.apache.spark.rpc.netty.Dispatcher.registerRpcEndpoint(Dispatcher.scala:60)
2025-04-24 15:05:58.479 | 	at org.apache.spark.rpc.netty.NettyRpcEnv.setupEndpoint(NettyRpcEnv.scala:136)
2025-04-24 15:05:58.479 | 	at org.apache.spark.sql.execution.streaming.state.StateStoreCoordinatorRef$.forDriver(StateStoreCoordinator.scala:72)
2025-04-24 15:05:58.479 | 	at org.apache.spark.sql.streaming.StreamingQueryManager.<init>(StreamingQueryManager.scala:52)
2025-04-24 15:05:58.479 | 	at org.apache.spark.sql.internal.BaseSessionStateBuilder.streamingQueryManager(BaseSessionStateBuilder.scala:340)
2025-04-24 15:05:58.479 | 	at org.apache.spark.sql.internal.BaseSessionStateBuilder.$anonfun$build$4(BaseSessionStateBuilder.scala:378)
2025-04-24 15:05:58.479 | 	at org.apache.spark.sql.internal.SessionState.streamingQueryManager$lzycompute(SessionState.scala:100)
2025-04-24 15:05:58.479 | 	at org.apache.spark.sql.internal.SessionState.streamingQueryManager(SessionState.scala:100)
2025-04-24 15:05:58.479 | 	at org.apache.spark.sql.SparkSession.streams(SparkSession.scala:238)
2025-04-24 15:05:58.479 | 	at org.apache.spark.sql.execution.streaming.BaseStreamingDeduplicateExec.doExecute(statefulOperators.scala:916)
2025-04-24 15:05:58.479 | 	at org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:195)
2025-04-24 15:05:58.479 | 	at org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:246)
2025-04-24 15:05:58.479 | 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
2025-04-24 15:05:58.479 | 	at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:243)
2025-04-24 15:05:58.479 | 	at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:191)
2025-04-24 15:05:58.479 | 	at org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:207)
2025-04-24 15:05:58.479 | 	at org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:206)
2025-04-24 15:05:58.479 | 	at org.apache.spark.sql.execution.streaming.sources.ForeachBatchSink.addBatch(ForeachBatchSink.scala:30)
2025-04-24 15:05:58.479 | 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$17(MicroBatchExecution.scala:732)
2025-04-24 15:05:58.479 | 	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)
2025-04-24 15:05:58.479 | 	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
2025-04-24 15:05:58.479 | 	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
2025-04-24 15:05:58.479 | 	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
2025-04-24 15:05:58.479 | 	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
2025-04-24 15:05:58.479 | 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$16(MicroBatchExecution.scala:729)
2025-04-24 15:05:58.479 | 	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)
2025-04-24 15:05:58.479 | 	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)
2025-04-24 15:05:58.479 | 	at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)
2025-04-24 15:05:58.479 | 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.runBatch(MicroBatchExecution.scala:729)
2025-04-24 15:05:58.479 | 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:286)
2025-04-24 15:05:58.479 | 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
2025-04-24 15:05:58.479 | 	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)
2025-04-24 15:05:58.479 | 	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)
2025-04-24 15:05:58.479 | 	at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)
2025-04-24 15:05:58.479 | 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)
2025-04-24 15:05:58.479 | 	at org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)
2025-04-24 15:05:58.479 | 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)
2025-04-24 15:05:58.479 | 	at org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)
2025-04-24 15:05:58.479 | 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
2025-04-24 15:05:58.479 | 	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
2025-04-24 15:05:58.479 | 	at org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)
2025-04-24 15:05:58.479 | 	at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)
2025-04-24 15:05:58.479 | 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
2025-04-24 15:05:58.479 | 	at org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)
2025-04-24 15:05:58.479 | 	at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)
2025-04-24 15:05:58.479 | 25/04/24 06:05:58 INFO AppInfoParser: App info kafka.admin.client for adminclient-7 unregistered
2025-04-24 15:05:58.480 | 25/04/24 06:05:58 INFO Metrics: Metrics scheduler closed
2025-04-24 15:05:58.480 | 25/04/24 06:05:58 INFO Metrics: Closing reporter org.apache.kafka.common.metrics.JmxReporter
2025-04-24 15:05:58.480 | 25/04/24 06:05:58 INFO Metrics: Metrics reporters closed
2025-04-24 15:05:58.482 | 25/04/24 06:05:58 INFO MicroBatchExecution: Async log purge executor pool for query [id = 2e8bba16-c696-4b17-8e4b-d77e80407107, runId = 56258db2-de72-41de-91ae-411133733965] has been shutdown
2025-04-24 15:05:58.496 | 25/04/24 06:05:58 WARN NettyRpcEnv: Ignored failure: java.util.concurrent.RejectedExecutionException: Task java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask@680a39ad[Not completed, task = java.util.concurrent.Executors$RunnableAdapter@6dc5be05[Wrapped task = org.apache.spark.rpc.netty.NettyRpcEnv$$anon$1@41a1f0ba]] rejected from java.util.concurrent.ScheduledThreadPoolExecutor@4c763b1e[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 0]
2025-04-24 15:05:58.497 | Exception in thread "stream execution thread for [id = 2e8bba16-c696-4b17-8e4b-d77e80407107, runId = 56258db2-de72-41de-91ae-411133733965]" org.apache.spark.SparkException: Exception thrown in awaitResult: 
2025-04-24 15:05:58.497 | 	at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)
2025-04-24 15:05:58.497 | 	at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)
2025-04-24 15:05:58.497 | 	at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
2025-04-24 15:05:58.497 | 	at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)
2025-04-24 15:05:58.497 | 	at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)
2025-04-24 15:05:58.497 | 	at org.apache.spark.sql.execution.streaming.state.StateStoreCoordinatorRef.deactivateInstances(StateStoreCoordinator.scala:119)
2025-04-24 15:05:58.497 | 	at org.apache.spark.sql.streaming.StreamingQueryManager.notifyQueryTermination(StreamingQueryManager.scala:426)
2025-04-24 15:05:58.497 | 	at org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$3(StreamExecution.scala:360)
2025-04-24 15:05:58.497 | 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
2025-04-24 15:05:58.497 | 	at org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)
2025-04-24 15:05:58.497 | 	at org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:340)
2025-04-24 15:05:58.497 | 	at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)
2025-04-24 15:05:58.497 | 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
2025-04-24 15:05:58.497 | 	at org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)
2025-04-24 15:05:58.497 | 	at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)
2025-04-24 15:05:58.497 | Caused by: org.apache.spark.rpc.RpcEnvStoppedException: RpcEnv already stopped.
2025-04-24 15:05:58.497 | 	at org.apache.spark.rpc.netty.Dispatcher.postMessage(Dispatcher.scala:176)
2025-04-24 15:05:58.497 | 	at org.apache.spark.rpc.netty.Dispatcher.postLocalMessage(Dispatcher.scala:144)
2025-04-24 15:05:58.497 | 	at org.apache.spark.rpc.netty.NettyRpcEnv.askAbortable(NettyRpcEnv.scala:242)
2025-04-24 15:05:58.497 | 	at org.apache.spark.rpc.netty.NettyRpcEndpointRef.askAbortable(NettyRpcEnv.scala:554)
2025-04-24 15:05:58.497 | 	at org.apache.spark.rpc.netty.NettyRpcEndpointRef.ask(NettyRpcEnv.scala:558)
2025-04-24 15:05:58.497 | 	at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:100)
2025-04-24 15:05:58.497 | 	... 11 more
2025-04-24 15:05:58.499 | 25/04/24 06:05:58 ERROR MicroBatchExecution: Query [id = ed63a3b3-8cee-43b3-aa93-58cdd8184c27, runId = 41d2a38e-8435-45c5-ad6e-8ad5b53bde17] terminated with error
2025-04-24 15:05:58.499 | java.lang.IllegalStateException: LiveListenerBus is stopped.
2025-04-24 15:05:58.499 | 	at org.apache.spark.scheduler.LiveListenerBus.addToQueue(LiveListenerBus.scala:92)
2025-04-24 15:05:58.499 | 	at org.apache.spark.sql.execution.streaming.StreamingQueryListenerBus.$anonfun$new$1(StreamingQueryListenerBus.scala:48)
2025-04-24 15:05:58.499 | 	at org.apache.spark.sql.execution.streaming.StreamingQueryListenerBus.$anonfun$new$1$adapted(StreamingQueryListenerBus.scala:48)
2025-04-24 15:05:58.499 | 	at scala.Option.foreach(Option.scala:407)
2025-04-24 15:05:58.499 | 	at org.apache.spark.sql.execution.streaming.StreamingQueryListenerBus.<init>(StreamingQueryListenerBus.scala:48)
2025-04-24 15:05:58.499 | 	at org.apache.spark.sql.streaming.StreamingQueryManager.<init>(StreamingQueryManager.scala:54)
2025-04-24 15:05:58.499 | 	at org.apache.spark.sql.internal.BaseSessionStateBuilder.streamingQueryManager(BaseSessionStateBuilder.scala:340)
2025-04-24 15:05:58.499 | 	at org.apache.spark.sql.internal.BaseSessionStateBuilder.$anonfun$build$4(BaseSessionStateBuilder.scala:378)
2025-04-24 15:05:58.499 | 	at org.apache.spark.sql.internal.SessionState.streamingQueryManager$lzycompute(SessionState.scala:100)
2025-04-24 15:05:58.499 | 	at org.apache.spark.sql.internal.SessionState.streamingQueryManager(SessionState.scala:100)
2025-04-24 15:05:58.499 | 	at org.apache.spark.sql.SparkSession.streams(SparkSession.scala:238)
2025-04-24 15:05:58.499 | 	at org.apache.spark.sql.execution.streaming.BaseStreamingDeduplicateExec.doExecute(statefulOperators.scala:916)
2025-04-24 15:05:58.499 | 	at org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:195)
2025-04-24 15:05:58.499 | 	at org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:246)
2025-04-24 15:05:58.499 | 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
2025-04-24 15:05:58.499 | 	at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:243)
2025-04-24 15:05:58.499 | 	at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:191)
2025-04-24 15:05:58.499 | 	at org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:207)
2025-04-24 15:05:58.499 | 	at org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:206)
2025-04-24 15:05:58.499 | 	at org.apache.spark.sql.execution.streaming.sources.ForeachBatchSink.addBatch(ForeachBatchSink.scala:30)
2025-04-24 15:05:58.499 | 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$17(MicroBatchExecution.scala:732)
2025-04-24 15:05:58.499 | 	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)
2025-04-24 15:05:58.499 | 	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
2025-04-24 15:05:58.499 | 	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
2025-04-24 15:05:58.499 | 	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
2025-04-24 15:05:58.499 | 	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
2025-04-24 15:05:58.499 | 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$16(MicroBatchExecution.scala:729)
2025-04-24 15:05:58.499 | 	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)
2025-04-24 15:05:58.499 | 	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)
2025-04-24 15:05:58.499 | 	at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)
2025-04-24 15:05:58.499 | 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.runBatch(MicroBatchExecution.scala:729)
2025-04-24 15:05:58.499 | 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:286)
2025-04-24 15:05:58.499 | 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
2025-04-24 15:05:58.499 | 	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)
2025-04-24 15:05:58.499 | 	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)
2025-04-24 15:05:58.499 | 	at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)
2025-04-24 15:05:58.499 | 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)
2025-04-24 15:05:58.499 | 	at org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)
2025-04-24 15:05:58.499 | 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)
2025-04-24 15:05:58.499 | 	at org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)
2025-04-24 15:05:58.499 | 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
2025-04-24 15:05:58.499 | 	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
2025-04-24 15:05:58.499 | 	at org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)
2025-04-24 15:05:58.499 | 	at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)
2025-04-24 15:05:58.499 | 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
2025-04-24 15:05:58.499 | 	at org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)
2025-04-24 15:05:58.499 | 	at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)
2025-04-24 15:05:58.499 | 25/04/24 06:05:58 INFO AppInfoParser: App info kafka.admin.client for adminclient-12 unregistered
2025-04-24 15:05:58.500 | 25/04/24 06:05:58 INFO Metrics: Metrics scheduler closed
2025-04-24 15:05:58.500 | 25/04/24 06:05:58 INFO Metrics: Closing reporter org.apache.kafka.common.metrics.JmxReporter
2025-04-24 15:05:58.501 | 25/04/24 06:05:58 INFO Metrics: Metrics reporters closed
2025-04-24 15:05:58.504 | 25/04/24 06:05:58 INFO MicroBatchExecution: Async log purge executor pool for query [id = ed63a3b3-8cee-43b3-aa93-58cdd8184c27, runId = 41d2a38e-8435-45c5-ad6e-8ad5b53bde17] has been shutdown
2025-04-24 15:05:58.505 | 25/04/24 06:05:58 WARN NettyRpcEnv: Ignored failure: java.util.concurrent.RejectedExecutionException: Task java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask@466639af[Not completed, task = java.util.concurrent.Executors$RunnableAdapter@67fc941[Wrapped task = org.apache.spark.rpc.netty.NettyRpcEnv$$anon$1@6a3fedef]] rejected from java.util.concurrent.ScheduledThreadPoolExecutor@4c763b1e[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 0]
2025-04-24 15:05:58.505 | Exception in thread "stream execution thread for [id = ed63a3b3-8cee-43b3-aa93-58cdd8184c27, runId = 41d2a38e-8435-45c5-ad6e-8ad5b53bde17]" org.apache.spark.SparkException: Exception thrown in awaitResult: 
2025-04-24 15:05:58.505 | 	at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)
2025-04-24 15:05:58.505 | 	at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)
2025-04-24 15:05:58.505 | 	at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
2025-04-24 15:05:58.505 | 	at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)
2025-04-24 15:05:58.505 | 	at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)
2025-04-24 15:05:58.505 | 	at org.apache.spark.sql.execution.streaming.state.StateStoreCoordinatorRef.deactivateInstances(StateStoreCoordinator.scala:119)
2025-04-24 15:05:58.505 | 	at org.apache.spark.sql.streaming.StreamingQueryManager.notifyQueryTermination(StreamingQueryManager.scala:426)
2025-04-24 15:05:58.505 | 	at org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$3(StreamExecution.scala:360)
2025-04-24 15:05:58.505 | 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
2025-04-24 15:05:58.505 | 	at org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)
2025-04-24 15:05:58.505 | 	at org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:340)
2025-04-24 15:05:58.505 | 	at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)
2025-04-24 15:05:58.505 | 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
2025-04-24 15:05:58.505 | 	at org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)
2025-04-24 15:05:58.505 | 	at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)
2025-04-24 15:05:58.505 | Caused by: org.apache.spark.rpc.RpcEnvStoppedException: RpcEnv already stopped.
2025-04-24 15:05:58.505 | 	at org.apache.spark.rpc.netty.Dispatcher.postMessage(Dispatcher.scala:176)
2025-04-24 15:05:58.505 | 	at org.apache.spark.rpc.netty.Dispatcher.postLocalMessage(Dispatcher.scala:144)
2025-04-24 15:05:58.505 | 	at org.apache.spark.rpc.netty.NettyRpcEnv.askAbortable(NettyRpcEnv.scala:242)
2025-04-24 15:05:58.505 | 	at org.apache.spark.rpc.netty.NettyRpcEndpointRef.askAbortable(NettyRpcEnv.scala:554)
2025-04-24 15:05:58.505 | 	at org.apache.spark.rpc.netty.NettyRpcEndpointRef.ask(NettyRpcEnv.scala:558)
2025-04-24 15:05:58.505 | 	at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:100)
2025-04-24 15:05:58.505 | 	... 11 more
2025-04-24 15:05:58.518 | 25/04/24 06:05:58 ERROR MicroBatchExecution: Query [id = 2529966a-2e54-49cf-ab64-7ed6cd055e9a, runId = d016eef7-9012-44d2-a4b7-3a2e83ac142f] terminated with error
2025-04-24 15:05:58.518 | java.lang.IllegalStateException: Cannot call methods on a stopped SparkContext.
2025-04-24 15:05:58.518 | This stopped SparkContext was created at:
2025-04-24 15:05:58.518 | 
2025-04-24 15:05:58.518 | org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)
2025-04-24 15:05:58.518 | java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
2025-04-24 15:05:58.518 | java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(Unknown Source)
2025-04-24 15:05:58.518 | java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(Unknown Source)
2025-04-24 15:05:58.518 | java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Unknown Source)
2025-04-24 15:05:58.518 | java.base/java.lang.reflect.Constructor.newInstance(Unknown Source)
2025-04-24 15:05:58.518 | py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)
2025-04-24 15:05:58.518 | py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
2025-04-24 15:05:58.518 | py4j.Gateway.invoke(Gateway.java:238)
2025-04-24 15:05:58.518 | py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)
2025-04-24 15:05:58.518 | py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)
2025-04-24 15:05:58.518 | py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
2025-04-24 15:05:58.518 | py4j.ClientServerConnection.run(ClientServerConnection.java:106)
2025-04-24 15:05:58.518 | java.base/java.lang.Thread.run(Unknown Source)
2025-04-24 15:05:58.518 | 
2025-04-24 15:05:58.518 | The currently active SparkContext was created at:
2025-04-24 15:05:58.518 | 
2025-04-24 15:05:58.518 | org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)
2025-04-24 15:05:58.518 | java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
2025-04-24 15:05:58.518 | java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(Unknown Source)
2025-04-24 15:05:58.518 | java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(Unknown Source)
2025-04-24 15:05:58.518 | java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Unknown Source)
2025-04-24 15:05:58.518 | java.base/java.lang.reflect.Constructor.newInstance(Unknown Source)
2025-04-24 15:05:58.518 | py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)
2025-04-24 15:05:58.518 | py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
2025-04-24 15:05:58.518 | py4j.Gateway.invoke(Gateway.java:238)
2025-04-24 15:05:58.518 | py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)
2025-04-24 15:05:58.518 | py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)
2025-04-24 15:05:58.518 | py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
2025-04-24 15:05:58.518 | py4j.ClientServerConnection.run(ClientServerConnection.java:106)
2025-04-24 15:05:58.518 | java.base/java.lang.Thread.run(Unknown Source)
2025-04-24 15:05:58.518 |          
2025-04-24 15:05:58.518 | 	at org.apache.spark.SparkContext.assertNotStopped(SparkContext.scala:122)
2025-04-24 15:05:58.518 | 	at org.apache.spark.SparkContext.broadcastInternal(SparkContext.scala:1654)
2025-04-24 15:05:58.518 | 	at org.apache.spark.SparkContext.broadcast(SparkContext.scala:1639)
2025-04-24 15:05:58.518 | 	at org.apache.spark.sql.execution.streaming.state.BaseStateStoreRDD.<init>(StateStoreRDD.scala:42)
2025-04-24 15:05:58.518 | 	at org.apache.spark.sql.execution.streaming.state.StateStoreRDD.<init>(StateStoreRDD.scala:115)
2025-04-24 15:05:58.518 | 	at org.apache.spark.sql.execution.streaming.state.package$StateStoreOps.mapPartitionsWithStateStore(package.scala:71)
2025-04-24 15:05:58.518 | 	at org.apache.spark.sql.execution.streaming.BaseStreamingDeduplicateExec.doExecute(statefulOperators.scala:917)
2025-04-24 15:05:58.518 | 	at org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:195)
2025-04-24 15:05:58.518 | 	at org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:246)
2025-04-24 15:05:58.518 | 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
2025-04-24 15:05:58.518 | 	at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:243)
2025-04-24 15:05:58.518 | 	at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:191)
2025-04-24 15:05:58.518 | 	at org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:207)
2025-04-24 15:05:58.518 | 	at org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:206)
2025-04-24 15:05:58.518 | 	at org.apache.spark.sql.execution.streaming.sources.ForeachBatchSink.addBatch(ForeachBatchSink.scala:30)
2025-04-24 15:05:58.518 | 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$17(MicroBatchExecution.scala:732)
2025-04-24 15:05:58.518 | 	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)
2025-04-24 15:05:58.518 | 	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
2025-04-24 15:05:58.518 | 	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
2025-04-24 15:05:58.518 | 	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
2025-04-24 15:05:58.518 | 	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
2025-04-24 15:05:58.518 | 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$16(MicroBatchExecution.scala:729)
2025-04-24 15:05:58.518 | 	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)
2025-04-24 15:05:58.518 | 	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)
2025-04-24 15:05:58.518 | 	at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)
2025-04-24 15:05:58.518 | 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.runBatch(MicroBatchExecution.scala:729)
2025-04-24 15:05:58.518 | 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:286)
2025-04-24 15:05:58.518 | 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
2025-04-24 15:05:58.518 | 	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)
2025-04-24 15:05:58.518 | 	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)
2025-04-24 15:05:58.518 | 	at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)
2025-04-24 15:05:58.518 | 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)
2025-04-24 15:05:58.518 | 	at org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)
2025-04-24 15:05:58.518 | 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)
2025-04-24 15:05:58.518 | 	at org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)
2025-04-24 15:05:58.518 | 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
2025-04-24 15:05:58.518 | 	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
2025-04-24 15:05:58.518 | 	at org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)
2025-04-24 15:05:58.518 | 	at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)
2025-04-24 15:05:58.518 | 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
2025-04-24 15:05:58.518 | 	at org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)
2025-04-24 15:05:58.518 | 	at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)
2025-04-24 15:05:58.519 | 25/04/24 06:05:58 INFO AppInfoParser: App info kafka.admin.client for adminclient-11 unregistered
2025-04-24 15:05:58.522 | 25/04/24 06:05:58 INFO Metrics: Metrics scheduler closed
2025-04-24 15:05:58.522 | 25/04/24 06:05:58 INFO Metrics: Closing reporter org.apache.kafka.common.metrics.JmxReporter
2025-04-24 15:05:58.522 | 25/04/24 06:05:58 INFO Metrics: Metrics reporters closed
2025-04-24 15:05:58.522 | 25/04/24 06:05:58 ERROR MicroBatchExecution: Query [id = bf093a8a-eda4-4fb0-8a5e-c7f83d067314, runId = eb9ca31a-970c-4955-9f6f-e162c04f7b99] terminated with error
2025-04-24 15:05:58.522 | java.lang.IllegalStateException: Cannot call methods on a stopped SparkContext.
2025-04-24 15:05:58.522 | This stopped SparkContext was created at:
2025-04-24 15:05:58.522 | 
2025-04-24 15:05:58.522 | org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)
2025-04-24 15:05:58.522 | java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
2025-04-24 15:05:58.522 | java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(Unknown Source)
2025-04-24 15:05:58.522 | java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(Unknown Source)
2025-04-24 15:05:58.522 | java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Unknown Source)
2025-04-24 15:05:58.522 | java.base/java.lang.reflect.Constructor.newInstance(Unknown Source)
2025-04-24 15:05:58.522 | py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)
2025-04-24 15:05:58.522 | py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
2025-04-24 15:05:58.522 | py4j.Gateway.invoke(Gateway.java:238)
2025-04-24 15:05:58.522 | py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)
2025-04-24 15:05:58.522 | py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)
2025-04-24 15:05:58.522 | py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
2025-04-24 15:05:58.522 | py4j.ClientServerConnection.run(ClientServerConnection.java:106)
2025-04-24 15:05:58.522 | java.base/java.lang.Thread.run(Unknown Source)
2025-04-24 15:05:58.522 | 
2025-04-24 15:05:58.522 | The currently active SparkContext was created at:
2025-04-24 15:05:58.522 | 
2025-04-24 15:05:58.522 | org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)
2025-04-24 15:05:58.522 | java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
2025-04-24 15:05:58.522 | java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(Unknown Source)
2025-04-24 15:05:58.522 | java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(Unknown Source)
2025-04-24 15:05:58.522 | java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Unknown Source)
2025-04-24 15:05:58.522 | java.base/java.lang.reflect.Constructor.newInstance(Unknown Source)
2025-04-24 15:05:58.522 | py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)
2025-04-24 15:05:58.522 | py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
2025-04-24 15:05:58.522 | py4j.Gateway.invoke(Gateway.java:238)
2025-04-24 15:05:58.522 | py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)
2025-04-24 15:05:58.522 | py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)
2025-04-24 15:05:58.522 | py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
2025-04-24 15:05:58.522 | py4j.ClientServerConnection.run(ClientServerConnection.java:106)
2025-04-24 15:05:58.522 | java.base/java.lang.Thread.run(Unknown Source)
2025-04-24 15:05:58.522 |          
2025-04-24 15:05:58.522 | 	at org.apache.spark.SparkContext.assertNotStopped(SparkContext.scala:122)
2025-04-24 15:05:58.522 | 	at org.apache.spark.SparkContext.broadcastInternal(SparkContext.scala:1654)
2025-04-24 15:05:58.522 | 	at org.apache.spark.SparkContext.broadcast(SparkContext.scala:1639)
2025-04-24 15:05:58.522 | 	at org.apache.spark.sql.execution.streaming.state.BaseStateStoreRDD.<init>(StateStoreRDD.scala:42)
2025-04-24 15:05:58.522 | 	at org.apache.spark.sql.execution.streaming.state.StateStoreRDD.<init>(StateStoreRDD.scala:115)
2025-04-24 15:05:58.522 | 	at org.apache.spark.sql.execution.streaming.state.package$StateStoreOps.mapPartitionsWithStateStore(package.scala:71)
2025-04-24 15:05:58.522 | 	at org.apache.spark.sql.execution.streaming.BaseStreamingDeduplicateExec.doExecute(statefulOperators.scala:917)
2025-04-24 15:05:58.522 | 	at org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:195)
2025-04-24 15:05:58.522 | 	at org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:246)
2025-04-24 15:05:58.522 | 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
2025-04-24 15:05:58.522 | 	at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:243)
2025-04-24 15:05:58.522 | 	at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:191)
2025-04-24 15:05:58.522 | 	at org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:207)
2025-04-24 15:05:58.522 | 	at org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:206)
2025-04-24 15:05:58.522 | 	at org.apache.spark.sql.execution.streaming.sources.ForeachBatchSink.addBatch(ForeachBatchSink.scala:30)
2025-04-24 15:05:58.522 | 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$17(MicroBatchExecution.scala:732)
2025-04-24 15:05:58.522 | 	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)
2025-04-24 15:05:58.522 | 	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
2025-04-24 15:05:58.522 | 	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
2025-04-24 15:05:58.522 | 	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
2025-04-24 15:05:58.522 | 	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
2025-04-24 15:05:58.522 | 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$16(MicroBatchExecution.scala:729)
2025-04-24 15:05:58.522 | 	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)
2025-04-24 15:05:58.522 | 	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)
2025-04-24 15:05:58.522 | 	at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)
2025-04-24 15:05:58.522 | 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.runBatch(MicroBatchExecution.scala:729)
2025-04-24 15:05:58.522 | 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:286)
2025-04-24 15:05:58.522 | 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
2025-04-24 15:05:58.522 | 	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)
2025-04-24 15:05:58.522 | 	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)
2025-04-24 15:05:58.522 | 	at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)
2025-04-24 15:05:58.522 | 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)
2025-04-24 15:05:58.522 | 	at org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)
2025-04-24 15:05:58.522 | 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)
2025-04-24 15:05:58.522 | 	at org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)
2025-04-24 15:05:58.522 | 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
2025-04-24 15:05:58.522 | 	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
2025-04-24 15:05:58.522 | 	at org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)
2025-04-24 15:05:58.522 | 	at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)
2025-04-24 15:05:58.522 | 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
2025-04-24 15:05:58.522 | 	at org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)
2025-04-24 15:05:58.522 | 	at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)
2025-04-24 15:05:58.523 | 25/04/24 06:05:58 INFO MicroBatchExecution: Async log purge executor pool for query [id = 2529966a-2e54-49cf-ab64-7ed6cd055e9a, runId = d016eef7-9012-44d2-a4b7-3a2e83ac142f] has been shutdown
2025-04-24 15:05:58.523 | 25/04/24 06:05:58 INFO AppInfoParser: App info kafka.admin.client for adminclient-10 unregistered
2025-04-24 15:05:58.523 | 25/04/24 06:05:58 WARN NettyRpcEnv: Ignored failure: java.util.concurrent.RejectedExecutionException: Task java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask@432ab73e[Not completed, task = java.util.concurrent.Executors$RunnableAdapter@5a402248[Wrapped task = org.apache.spark.rpc.netty.NettyRpcEnv$$anon$1@1373143f]] rejected from java.util.concurrent.ScheduledThreadPoolExecutor@4c763b1e[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 0]
2025-04-24 15:05:58.523 | Exception in thread "stream execution thread for [id = 2529966a-2e54-49cf-ab64-7ed6cd055e9a, runId = d016eef7-9012-44d2-a4b7-3a2e83ac142f]" org.apache.spark.SparkException: Exception thrown in awaitResult: 
2025-04-24 15:05:58.523 | 	at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)
2025-04-24 15:05:58.523 | 	at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)
2025-04-24 15:05:58.523 | 	at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
2025-04-24 15:05:58.523 | 	at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)
2025-04-24 15:05:58.523 | 	at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)
2025-04-24 15:05:58.523 | 	at org.apache.spark.sql.execution.streaming.state.StateStoreCoordinatorRef.deactivateInstances(StateStoreCoordinator.scala:119)
2025-04-24 15:05:58.523 | 	at org.apache.spark.sql.streaming.StreamingQueryManager.notifyQueryTermination(StreamingQueryManager.scala:426)
2025-04-24 15:05:58.524 | 	at org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$3(StreamExecution.scala:360)
2025-04-24 15:05:58.524 | 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
2025-04-24 15:05:58.524 | 	at org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)
2025-04-24 15:05:58.524 | 	at org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:340)
2025-04-24 15:05:58.524 | 	at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)
2025-04-24 15:05:58.524 | 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
2025-04-24 15:05:58.524 | 	at org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)
2025-04-24 15:05:58.524 | 	at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)
2025-04-24 15:05:58.524 | Caused by: org.apache.spark.rpc.RpcEnvStoppedException: RpcEnv already stopped.
2025-04-24 15:05:58.524 | 	at org.apache.spark.rpc.netty.Dispatcher.postMessage(Dispatcher.scala:176)
2025-04-24 15:05:58.524 | 	at org.apache.spark.rpc.netty.Dispatcher.postLocalMessage(Dispatcher.scala:144)
2025-04-24 15:05:58.524 | 	at org.apache.spark.rpc.netty.NettyRpcEnv.askAbortable(NettyRpcEnv.scala:242)
2025-04-24 15:05:58.524 | 	at org.apache.spark.rpc.netty.NettyRpcEndpointRef.askAbortable(NettyRpcEnv.scala:554)
2025-04-24 15:05:58.524 | 	at org.apache.spark.rpc.netty.NettyRpcEndpointRef.ask(NettyRpcEnv.scala:558)
2025-04-24 15:05:58.524 | 	at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:100)
2025-04-24 15:05:58.524 | 	... 11 more
2025-04-24 15:05:58.525 | 25/04/24 06:05:58 INFO SparkContext: Successfully stopped SparkContext
2025-04-24 15:05:58.525 | 25/04/24 06:05:58 INFO ShutdownHookManager: Shutdown hook called
2025-04-24 15:05:58.525 | 25/04/24 06:05:58 INFO Metrics: Metrics scheduler closed
2025-04-24 15:05:58.525 | 25/04/24 06:05:58 INFO Metrics: Closing reporter org.apache.kafka.common.metrics.JmxReporter
2025-04-24 15:05:58.525 | 25/04/24 06:05:58 INFO Metrics: Metrics reporters closed
2025-04-24 15:05:58.525 | 25/04/24 06:05:58 INFO ShutdownHookManager: Deleting directory /tmp/temporary-2ce6649d-5b58-4ef3-98a8-fb21f300afef
2025-04-24 15:05:58.525 | 25/04/24 06:05:58 INFO MicroBatchExecution: Async log purge executor pool for query [id = bf093a8a-eda4-4fb0-8a5e-c7f83d067314, runId = eb9ca31a-970c-4955-9f6f-e162c04f7b99] has been shutdown
2025-04-24 15:05:58.526 | 25/04/24 06:05:58 WARN NettyRpcEnv: Ignored failure: java.util.concurrent.RejectedExecutionException: Task java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask@23930103[Not completed, task = java.util.concurrent.Executors$RunnableAdapter@7ef286df[Wrapped task = org.apache.spark.rpc.netty.NettyRpcEnv$$anon$1@bfe68e9]] rejected from java.util.concurrent.ScheduledThreadPoolExecutor@4c763b1e[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 0]
2025-04-24 15:05:58.526 | Exception in thread "stream execution thread for [id = bf093a8a-eda4-4fb0-8a5e-c7f83d067314, runId = eb9ca31a-970c-4955-9f6f-e162c04f7b99]" org.apache.spark.SparkException: Exception thrown in awaitResult: 
2025-04-24 15:05:58.526 | 	at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)
2025-04-24 15:05:58.526 | 	at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)
2025-04-24 15:05:58.526 | 	at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
2025-04-24 15:05:58.526 | 	at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)
2025-04-24 15:05:58.526 | 	at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)
2025-04-24 15:05:58.526 | 	at org.apache.spark.sql.execution.streaming.state.StateStoreCoordinatorRef.deactivateInstances(StateStoreCoordinator.scala:119)
2025-04-24 15:05:58.526 | 	at org.apache.spark.sql.streaming.StreamingQueryManager.notifyQueryTermination(StreamingQueryManager.scala:426)
2025-04-24 15:05:58.526 | 	at org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$3(StreamExecution.scala:360)
2025-04-24 15:05:58.526 | 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
2025-04-24 15:05:58.526 | 	at org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)
2025-04-24 15:05:58.526 | 	at org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:340)
2025-04-24 15:05:58.526 | 	at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)
2025-04-24 15:05:58.526 | 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
2025-04-24 15:05:58.526 | 	at org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)
2025-04-24 15:05:58.526 | 	at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)
2025-04-24 15:05:58.526 | Caused by: org.apache.spark.rpc.RpcEnvStoppedException: RpcEnv already stopped.
2025-04-24 15:05:58.526 | 	at org.apache.spark.rpc.netty.Dispatcher.postMessage(Dispatcher.scala:176)
2025-04-24 15:05:58.526 | 	at org.apache.spark.rpc.netty.Dispatcher.postLocalMessage(Dispatcher.scala:144)
2025-04-24 15:05:58.526 | 	at org.apache.spark.rpc.netty.NettyRpcEnv.askAbortable(NettyRpcEnv.scala:242)
2025-04-24 15:05:58.526 | 	at org.apache.spark.rpc.netty.NettyRpcEndpointRef.askAbortable(NettyRpcEnv.scala:554)
2025-04-24 15:05:58.526 | 	at org.apache.spark.rpc.netty.NettyRpcEndpointRef.ask(NettyRpcEnv.scala:558)
2025-04-24 15:05:58.526 | 	at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:100)
2025-04-24 15:05:58.526 | 	... 11 more
2025-04-24 15:05:58.528 | 25/04/24 06:05:58 INFO ShutdownHookManager: Deleting directory /tmp/temporary-18018b60-cd91-4654-b55d-89552988ee35
2025-04-24 15:05:58.530 | 25/04/24 06:05:58 INFO ShutdownHookManager: Deleting directory /tmp/spark-6d03b30f-0726-4720-ba3e-bc44a6c0a10e/pyspark-b102f0e0-14b1-4c38-b529-0337719594eb
2025-04-24 15:05:58.532 | 25/04/24 06:05:58 INFO ShutdownHookManager: Deleting directory /tmp/spark-84610308-f2ed-4e9c-98ed-5bf0e26f3726
2025-04-24 15:05:58.534 | 25/04/24 06:05:58 INFO ShutdownHookManager: Deleting directory /tmp/temporary-3c1df5f2-e1da-4beb-8c35-3451ca7efd9d
2025-04-24 15:05:58.536 | 25/04/24 06:05:58 INFO ShutdownHookManager: Deleting directory /tmp/temporary-2cacd18c-322e-4cfa-8e0d-c1008c859651
2025-04-24 15:05:58.539 | 25/04/24 06:05:58 INFO ShutdownHookManager: Deleting directory /tmp/spark-6d03b30f-0726-4720-ba3e-bc44a6c0a10e
2025-04-24 15:05:58.542 | 25/04/24 06:05:58 INFO ShutdownHookManager: Deleting directory /tmp/temporary-33d80dac-a68a-4d9e-9955-2288fc90154c
2025-04-24 15:05:58.544 | 25/04/24 06:05:58 INFO ShutdownHookManager: Deleting directory /tmp/temporary-6b4dd807-0b15-4df4-b97a-87fbdf0b3438
2025-04-24 15:05:58.546 | 25/04/24 06:05:58 INFO ShutdownHookManager: Deleting directory /tmp/temporary-116fac01-2a6c-40a3-b256-01c68ac34ec9
2025-04-24 15:05:58.549 | 25/04/24 06:05:58 INFO ShutdownHookManager: Deleting directory /tmp/temporary-579966c4-93f8-4877-88e0-c9b4e4d718fa
2025-04-24 15:05:58.552 | 25/04/24 06:05:58 INFO ShutdownHookManager: Deleting directory /tmp/temporary-98aba8b4-e403-4f10-96a8-9c75af6e71ba
2025-04-24 15:05:58.554 | 25/04/24 06:05:58 INFO ShutdownHookManager: Deleting directory /tmp/temporary-ec737e79-bc80-4f79-9a41-d619dca08576
2025-04-24 15:05:58.556 | 25/04/24 06:05:58 INFO ShutdownHookManager: Deleting directory /tmp/temporary-ef85e29e-f0bd-4d21-8c8c-96b8720dae2d
2025-04-24 15:05:58.559 | 25/04/24 06:05:58 INFO ShutdownHookManager: Deleting directory /tmp/temporary-c78571f4-8fa3-463d-a680-ad63781afb6f